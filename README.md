# Knowledge-enriched Text Generation paper reading

ğŸ˜ Awesome list of papers about knowledge-enhanced Question generation with notes.

:white_check_mark: : **already reading carefully**

:fire:: **high citation in recent years**

:hammer_and_wrench:: **available code**

> Content

[TOC]



---

## :grey_question: Question Generation

---

### :mountain_snow: **Textual Question Generating Crosstalk**

**ä¸€ã€åˆ©ç”¨ç­”æ¡ˆå’Œè¯­è¨€ç‰¹å¾**

1. **ä¸¤ç¯‡Ground Breaking Work**

:white_check_mark: :fire: **Neural question generation from text: A preliminary study**, in EMNLP 2017. [[pdf](https://arxiv.org/abs/1704.01792)] 

* åœ¨ç¼–ç æ—¶é¢å¤–è€ƒè™‘äº†ç­”æ¡ˆä½ç½®ä¸è¯­æ³•ä¿¡æ¯ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚(ç°åœ¨æ¥çœ‹éå¸¸**basic**é‡è¦çš„ä¿¡æ¯ï¼)
  * word case åšè®­ç»ƒæ—¶å€™çš„teacher forcing
  * answer position feature
  * lexical features
    * **POS**
    * **NER**

```mermaid
graph LR
en((encoder)) --bi-GRU--> fe((feature-Rich)) --> word-vecotr
fe --> lexcial-feature-embedding-vectors --> POS+NER
fe --> answer-position-embedding --> BIO-tagging

word-vecotr --> åŒå‘çš„éšè—å±‚
POS+NER --> åŒå‘çš„éšè—å±‚
BIO-tagging --> åŒå‘çš„éšè—å±‚

de((decoder)) --å¸¦æ³¨æ„åŠ›æœºåˆ¶,ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›--> maxout-hidden+å…·ä½“éœ€è¦çœ‹referenceè®ºæ–‡
de --> GRU

de --> Copy-Mechanism,ä¸€æ ·ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ› --> è®¡ç®—å‡ºæ¦‚ç‡ä»sourceå¥å­ä¸­ç›´æ¥copyå•è¯
```



:white_check_mark: :fire: :hammer_and_wrench: **Learning to Ask: Neural Question Generation for Reading Comprehension**, in ACL 2017. [[pdf]](https://arxiv.org/abs/1705.00106) [[official code (torch)](https://github.com/xinyadu/nqg)]
* å°†ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œåº”ç”¨äºé—®é¢˜ç”Ÿæˆ
* é‡‡ç”¨seq2seq+attentionæ¨¡å‹æ¶æ„
* æ‘†è„±äº†è½¬æ¢è§„åˆ™ä¸æ¨¡ç‰ˆçš„å±€é™ï¼Œå–å¾—äº†ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„æ€§èƒ½
* åŠ å…¥äº†paragraph-level


```mermaid
graph LR
ä»»åŠ¡éš¾ç‚¹ --æ›´åŠ æ¥è¿‘äºäººç±»--> åŒä¹‰è¯æ›¿æ¢+çŸ¥è¯†å¼•å…¥ --> ç›¸å…³å·¥ä½œ --> è¿‡å»:rule-based 
ç›¸å…³å·¥ä½œ --> å…¶ä»–æ•°æ®æ˜ å°„è‡ªç„¶è¯­è¨€

Seq2Seq --> en((encoder)) --bidirectional--> softè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° --> lstm((LSTM))  --> only-sentence
lstm --> sentence+paragraph --> truncateæˆªæ–­,å½“ç„¶æ›´å¥½çš„æ–¹æ³•æ˜¯åˆ‡ç‰‡

Seq2Seq --> de((decoder)) --word-level-prediction--> LSTM((LSTM)) --> éšè—å±‚åˆå§‹åŒ– --basic-model --> å¥å­encoderçš„æœ€åéšè—å±‚
LSTM --oours--> å¥å­+æ®µè½çš„encoderè¾“å‡º


```

2. **ç­”æ¡ˆç¼–ç **

:white_check_mark: :fire: **Improving Neural Question Generation using Answer Separation**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1809.02393)] 
* å¾ˆå¤šåŸºç¡€æ“ä½œ
* åœ¨ç­”æ¡ˆä¸Šåšäº†ç®€å•é«˜æ•ˆçš„é¢„å¤„ç†
  * Mask åŸæ–‡ä¸­çš„ç­”æ¡ˆ
  * å¯¹ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯åšæŠ½å–ï¼Œè®¡ç®—attention

3. **è¯­è¨€ç‰¹å¾å¼ºåŒ–**

> ä¼ ç»Ÿçš„æœ‰**POS**ï¼ˆè¯æ€§æ ‡æ³¨ï¼‰å’Œ**NER**ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚åç»­è¿˜æœ‰ä¸€äº›æ›´åŠ ç»†å¾®çš„å¤„ç†

:fire: **Learning to Generate Questions by Learning What not to Generate**, in WWW 2019.  [[pdf](https://arxiv.org/pdf/1902.10418.pdf)] 

* clue å’Œ copyçš„æœºåˆ¶
* ![image-20220703112845472](https://s2.loli.net/2022/07/03/hoa1kTVzIpXGeQK.png)
* ![image-20220703113749291](https://s2.loli.net/2022/07/03/bdToKkIADvemV6B.png)
* æ–‡ç« è´¡çŒ®
  * å¸®åŠ©æ¨¡å‹å†³ç­–ä»€ä¹ˆæ—¶å€™ç”Ÿæˆï¼Œä»€ä¹ˆæ—¶å€™copy
  * ç”Ÿæˆå¤šä¸ªé—®é¢˜

4. ç–‘é—®è¯ç±»å‹ï¼ˆquestion typeï¼‰

**Question-type Driven Question Generation**, in EMNLP 2019.  [[pdf](https://arxiv.org/pdf/1909.00140.pdf)]

* å¼•å…¥å¯¹ç–‘é—®è¯çš„é¢„æµ‹æ¨¡å—ï¼Œå¹¶ä¸”åŠ å…¥å¯¹åº”çš„æŸå¤±å‡½æ•°
* ![image-20220703121312077](https://s2.loli.net/2022/07/03/uhcs1erWpQ9UFKL.png)
* ![image-20220703121503229](https://s2.loli.net/2022/07/03/GTO7j5c1AkXNnqY.png)
* ![image-20220703121520488](https://s2.loli.net/2022/07/03/seIfghRSbvpO68a.png)
* [æŸå¤±å‡½æ•°å¼•æ–‡](https://aclanthology.org/P17-1099.pdf)ï¼š <img src="https://s2.loli.net/2022/07/03/UZv17XkMyLRPOgd.png" alt="image-20220703122045296" style="zoom: 50%;" />

**äºŒã€æ®µè½çº§åˆ«ç‰¹å¾**

1. **å¼ºåŒ–æ®µè½çº§åˆ«æ–‡æœ¬çš„ç‰¹å¾**

:fire: :hammer_and_wrench: **Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks**, in EMNLP 2018. [[pdf](https://github.com/seanie12/neural-question-generation)] [[torch](https://github.com/seanie12/neural-question-generation)]

* ä¸»è¦è´¡çŒ®éƒ½åœ¨æ¨¡å‹ä¸Šé¢ï¼ŒåŸºäºseq2seqè®¾è®¡ï¼š![image-20220704150006013](https://s2.loli.net/2022/07/04/trhe5uTRkUqyfsx.png)

  * gate self-attention: ä¸ªäººè§‰å¾—æ˜¯ä¸€å¥—å¾ˆå¸¸ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸‹ï¼Œä¹Ÿéå¸¸ç®€å•

  * Maxout ==**Pointer**== & Decoding **å…¨æ–°çš„å¤„ç† copy æœºåˆ¶**  (æœ‰ç©ºå¯ä»¥è‡ªè¡Œå»çœ‹çœ‹ä»£ç ï¼)

    * ä¹‹å‰copyå¾—åˆ†ï¼š$\operatorname{sc}^{\text {copy }}\left(y_{t}\right)=\left\{\begin{array}{l}\sum_{k, \text { where } x_{k}=y_{t}} r_{t, k}, \quad y_{t} \in \chi \\ -i n f, \text{otherwise}\end{array} \quad\right.$ , é—®é¢˜åœ¨äºè‹¥æ–‡ç« ä¸­æŸä¸ªå•è¯é‡å¤å‡ºç°å¤šæ¬¡ï¼Œåˆ™å¯¹è¯¥å•è¯copyä¹Ÿä¼šå¤šï¼Œå½±å“è¯­å¥é€šé¡ºã€‚

    * ä¸ºæ­¤æ”¹è¿›ä¸ºMaxout Pointerï¼š
      $$
      \operatorname{sc}^{\text {copy }}\left(y_{t}\right)= \begin{cases}\max _{k, \text { where } x_{k}=y_{t}} r_{t, k}, & y_{t} \in \chi \\ -i n f, & \text { otherwise }\end{cases}
      $$

:fire: **Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model**,in ICLR 2020. [[pdf](https://arxiv.org/abs/1908.04942)] [[torch](https://github.com/hugochan/RL-based-Graph2Seq-for-NQG.)]

* å°†passageå’Œanswerçš„è¡¨ç¤ºï¼ˆåŒ…å«bertå‘é‡ï¼Œgloveå‘é‡ï¼Œè¯æ±‡ç‰¹å¾ç­‰ï¼‰è¿›è¡Œå¤šæ¬¡åå¤çš„äº¤äº’è¿›è¡Œç¼–ç ï¼ˆ**éå¸¸ç»†èŠ‚**çš„deep alignment networkï¼‰
* åˆ©ç”¨GNNæ¥ç¼–ç ï¼ˆä½¿ç”¨äº†ä¸¤ç§æ–¹å¼ï¼‰ï¼š
  * å¯¹sentenceåš**dependency parsing**ï¼Œç„¶åç›¸é‚»çš„å¥å­é“¾æ¥å¾—åˆ°passageçš„å›¾
  * é€šè¿‡self attentionçš„æ–¹å¼å¾—åˆ°passage çš„å›¾ï¼ˆæƒå€¼çŸ©é˜µï¼‰

:fire: **Improving Question Generation With to the Point Context**, in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1317.pdf)]

* è”åˆå»ºæ¨¡éç»“æ„åŒ–å¥å­ï¼ˆåŸæ–‡ï¼‰å’Œç»“æ„åŒ–ç­”æ¡ˆç›¸å…³å…³ç³»ï¼ˆ answer-relevant relation é¢„å…ˆä»å¥å­ä¸­æå–ï¼‰ä»¥ç”Ÿæˆé—®é¢˜(**æŠ“å–é‡ç‚¹ä¸Šä¸‹æ–‡**)
* ä½œè€…å‘ç°ä¸Šä¸‹æ–‡ä¸­ï¼Œè·ç¦»ansewræ¯”è¾ƒè¿œçš„è¯å¹¶ä¸ä¸€å®šä¸é‡è¦ï¼Œç›¸å¯¹çš„è·Ÿanswerç´§è´´çš„è¯ä¹Ÿæœ‰å¾ˆå¤šæ— å…³çš„ï¼Œä¸ºäº†æ•æ‰è¿™ç§å…³ç³»ï¼Œä½œè€…ä½¿ç”¨**OpenIE**è¿™ä¸ªå·¥å…·æŠ½å–ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨çš„**å…³ç³»ä¸‰å…ƒç»„**ã€‚

**ä¸‰ã€å¤šä»»åŠ¡è®­ç»ƒ**

**Multi-Task Learning with Language Modeling for Question Generation**, in EMNLP 2019. [[pdf](http://aclanthology.lst.uni-saarland.de/D19-1337.pdf)]

* æŠŠè¯­è¨€æ¨¡å‹ï¼ˆé¢„æµ‹å‰åè¯ï¼‰å’ŒQGä½œä¸ºmulti-taskä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚
* ä¸¤ä¸ªä»»åŠ¡æ˜¯å±‚çº§çš„å…³ç³»ï¼Œå…ˆè¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„æµ‹ï¼Œç„¶åå°†è¯­è¨€æ¨¡å‹çš„hiddenä½œä¸ºç‰¹å¾æä¾›ç»™åé¢seq2seq

:fire: **Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1912.00879)]

* å‡ºå‘ç‚¹æ˜¯æ˜¯**è§£å†³ç”Ÿæˆé”™è¯¯çš„ç–‘é—®è¯**å’Œ**copyåŸæ–‡ä¸­æ— å…³è¯**çš„é—®é¢˜

* ä½œè€…è®¤ä¸ºç”Ÿæˆé”™è¯¯è¯çš„åŸå› æ˜¯æ²¡æœ‰æ­£ç¡®çš„åˆ©ç”¨**answer position**ä¿¡æ¯ï¼Œcopyæ— å…³è¯çš„åŸå› æ˜¯ç¼ºä¹**å±€éƒ¨è¯­ä¹‰ä¿¡æ¯**ã€‚

* ä¸ºäº†åˆ†åˆ«ç¼“è§£è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½œè€…ä¹Ÿæ˜¯è®¾è®¡äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼š

  * è¯­ä¹‰åŒ¹é…åˆ†ç±»ï¼šè¿™ä¸ªä»»åŠ¡çš„è®¾è®¡å‡ºå‘ç‚¹ä¹Ÿæ˜¯SQuADçš„æ•°æ®ç‰¹ç‚¹ï¼Œå¯¹äºä¸€ä¸ªpassageå­˜åœ¨å¤šä¸ªanswer-questionè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å¯¹è¿™æ ·çš„æ•°æ®å®¹æ˜“äº§ç”Ÿä¸€äº›å®½æ³›ä¸å…·ä½“çš„é—®é¢˜ã€‚æ‰€ä»¥ä½œè€…æŠŠpassage-questionä½œä¸ºæ­£æ ·æœ¬ï¼Œpassage-random selected questionï¼Œ random selected passage-questionä½œä¸ºè´Ÿæ ·æœ¬è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚

  ![img](https://pic3.zhimg.com/80/v2-6e137d01fd833693fcc6cf526a39fb8e_720w.jpg)

  * answer-postionä½ç½®é¢„æµ‹ï¼šäº†è®©æ¨¡å‹æ›´å¥½çš„åˆ©ç”¨answerä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹answeråœ¨ä¸Šä¸‹æ–‡ä¸­startå’Œendä½ç½®çš„æ¨¡å‹ï¼ˆpointer networkï¼‰ï¼Œå…¶ä¸­åŸºç¡€çš„ç¼–ç éƒ¨åˆ†é‡‡ç”¨BiDAFçš„æ–¹å¼ã€‚
  * ç„¶å**QGå’Œè¿™ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ä¸€èµ·è®­ç»ƒ**ï¼Œæ•ˆæœå¯ã€‚


****

### :sunrise: Visual QG

:hammer_and_wrench: **[No Visual] Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing**, in AAAI 2021. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17544)] [[torch](https://github.com/VISLANG-Lab/EGSS)]

* Multi-feature Encoder: ä½¿ç”¨äº†POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰+ NERï¼ˆå…³ç³»æŠ½å–ï¼‰

:hammer_and_wrench: **Multiple Objects-Aware Visual Question Generation**, in ACM MM 2021. [[pdf](https://dl.acm.org/doi/abs/10.1145/3474085.3476969)]
* **å†™ä½œä¸Šå†™å¾—å¾ˆå®åœ¨ï¼Œå¾ˆå®¹æ˜“æ‡‚**ï¼Œæœ‰å¾ˆå¤šæ‰¿ä¸Šå¯ä¸‹çš„å¥å­ã€‚
* é¦–æ¬¡å°†**å¯¹è±¡**èå…¥åˆ°é—®é¢˜ç”Ÿæˆä»»åŠ¡å½“ä¸­

![image-20220629230122528](https://s2.loli.net/2022/06/29/6Mn4HPG9ZiCjOqv.png)

:hammer_and_wrench:  **Difficulty-Controllable Visual Question Generation**, in APWeb-WAIM 2021. [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-85896-4_26.pdf)]

* **éš¾åº¦å¯æ§**çš„é—®é¢˜ç”Ÿæˆï¼šé‡‡ç”¨äº†æ•™è‚²å­¦é¢†åŸŸæ”¶é›†å¥½çš„é—®é¢˜éš¾åº¦æ ‡ç­¾(DIF), è¯¦è§[é“¾æ¥](https://www.apims.net/index.php/apims/article/view/9)
* åœ¨VQA2.0æ•°æ®é›†çš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŒºåˆ†ä¸ºå®¹æ˜“å’Œéš¾çš„é—®é¢˜æ•°æ®é›†
  * å¼•å…¥ä¸¤ä¸ªVQAçš„æ¨¡å‹æ¥è¿›è¡Œå›ç­”ï¼Œéƒ½å›ç­”å¯¹çš„ä¸ºå®¹æ˜“ï¼Œéƒ½å›ç­”é”™è¯¯å°±æ˜¯éš¾çš„

* ![image-20220629231350190](https://s2.loli.net/2022/06/29/POftb69si7hnINX.png)
  * å…¶ä¸­Difficulty Variableå°±æ˜¯$\{0, 1\}$




### :video_camera: Video QG

**Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning**, in ICASSP 2020. [[pdf](https://ieeexplore.ieee.org/document/9053476)]

* ä½¿ç”¨äº†**[TVQA](https://paperswithcode.com/dataset/tvqa)**æ•°æ®é›†ï¼Œis based on 6 popular TV shows and consists of **152,545 QA pairs** from **21,793 clips**.
* æ€»ä½“æ²¡ä»€ä¹ˆåˆ›æ–°çš„

**Multi-Turn Video Question Generation via Reinforced Multi-Choice Attention Network**, in T-CSVT 2021.[[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9161024)]

* Multi-Turnï¼ˆM-VQGï¼‰ï¼šç»“åˆå¤šè½®å¯¹è¯+è§†é¢‘ä¿¡æ¯
* ä¼˜ç‚¹ï¼š åˆ©ç”¨åŠ¨æ€åœºæ™¯ä¿¡æ¯ï¼Œé—®é¢˜å¯å›ç­”æ€§ï¼Œå¯¹è¯è®°å½•ä¿¡æ¯æŠ½å–
* æ–¹æ³•ï¼šbaselineæ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆçœ‹ä¸æ‡‚ï¼‰

**End-to-End Video Question-Answer Generation with Generator-Pretester Network**, in T-CSVT 2021. [[pdf](https://arxiv.org/pdf/2101.01447.pdf)]

* å¼•å…¥ä¸€é—®ä¸€ç­”çš„å½¢å¼ï¼Œç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œç„¶åæµ‹è¯•ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
* ç¡¬ä»¶å¹³å°ï¼šNVIDIA DGX-1ï¼ˆ8 * V100ï¼‰

### :sun_with_face: QG examples

:white_check_mark:  :hammer_and_wrench:  **Mixture Content Selection for Diverse Sequence Generation**, in EMNLP 2019.[[pdf](https://arxiv.org/abs/1909.01953)] [[torch](https://github.com/clovaai/FocusSeq2Seq)]

:hammer_and_wrench: **Radial Graph Convolutional Network for Visual Question Generation**, in IEEE Transactions on Neural Networks and Learning Systems 2020. [[pdf](https://ieeexplore.ieee.org/document/9079208)] [[torch](https://github.com/Wangt-CN/VQG-GCN)]

## :bookmark_tabs: Question Answering

---

:fire:  :hammer_and_wrench: **[Question Answering] Commonsense for Generative Multi-Hop Question Answering Tasks**, in EMNLP 2018. [[pdf\]](https://arxiv.org/abs/1809.06309) [[code (tf)\]](https://github.com/yicheng-w/CommonSenseMultiHopQA)

:hammer_and_wrench: **[Dialogue System] Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering**, in AAAI 2020. [[pdf\]](https://arxiv.org/abs/1912.07491) [[code (torch)\]](https://github.com/siat-nlp/TransDG)

**[Question Answering] Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs**, in EMNLP 2019. [[pdf\]](https://arxiv.org/abs/1910.08435)

:fire: :hammer_and_wrench: **[Question Answering] ** **Improving Multi-hop Question Answering over Knowledge Graphs usingKnowledge Base Embeddings**, in ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.412/)] [[torch](https://github.com/malllabiisc/EmbedKGQA)]



## :book: Paraphrase

---



:hammer_and_wrench: **[Sentence Discrimination] Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator**,in COLING 2018. [[pdf](https://aclanthology.org/C18-1230/)] [[torch](https://github.com/badripatro/PQG)]

:hammer_and_wrench: **[Hierarchical Sketch&Paraphrase Generation] Hierarchical Sketch Induction for Paraphrase Generation**, in ACL 2022.[[pdf](https://aclanthology.org/2022.acl-long.178.pdf)] [[torch](https://github.com/tomhosking/hrq-vae)]

### :whale2: Related Big Model

:fire: :hammer_and_wrench: **[Cross-Modal&Contrastive Learning] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning**, in ACL(long paper) 2021. [[pdf](https://aclanthology.org/2021.acl-long.202/)] [[project from Baidu](https://unimo-ptm.github.io/)]

:hammer_and_wrench: **[MultiModal] UniT: Multimodal Multitask Learning with a Unified Transformer**, ICCV 2021. [[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.pdf)] [[project from Fair](https://mmf.sh/)]

## :framed_picture: Image Caption

---



:white_check_mark: :hammer_and_wrench: **[Image Caption] Generating Diverse and Descriptive Image Captions Using Visual Paraphrases**, in ICCV 2019. [[pdf](https://ieeexplore.ieee.org/document/9010984)] [[torch](https://github.com/pkuliu/visual-paraphrases-captioning)]

* è¯¥è®ºæ–‡ç ”ç©¶äº†ç›®å‰å›¾åƒçš„æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å¤è¿°çš„ä¸¤é˜¶æ®µè§£ç çš„æ¨¡å‹ã€‚
  * ç»™å®šå›¾åƒè¾“å…¥ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç”Ÿæˆåˆæ­¥çš„å¥å­ï¼Œå†å°†å…¶æ”¹å†™ä¸ºå†…å®¹æ›´åŠ å¤šæ ·å’Œä¸°å¯Œçš„æè¿°ã€‚åœ¨MS COCOå›¾åƒæè¿°æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ã€‚

  * é‡ç‚¹æ¢ç´¢**visual paraphrases** è§’è‰² + **scoring function**
  
    * ```mermaid
      graph LR
      ä¸äººç±»ç›¸æ¯” --æ–‡ç« ä¸­æœ‰example--> ç¼ºå°‘å¤šæ ·æ€§å’Œå…·ä½“æ€§ --> ä¸¤é˜¶æ®µè§†è§‰å¤è¿°æ–¹æ³• --> MSCOCOæ•°æ®é›†
      ```
  
  
  * æ•…äº‹å±•å¼€:
  
    * ```mermaid
      graph LR
      æ ‡å‡† -->æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> å½¢å®¹è¯
      æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> ç»†èŠ‚,with
      å½¢å®¹è¯ --> Pa((Paraphrase))
      ç»†èŠ‚,with --> Pa
      Pa --> visual-paraphrase
      visual-paraphrase --> sentence_pairs --> ä¸¤é˜¶æ®µç¼–ç 
      ```
  
    * ```mermaid
      graph LR
      ç›¸å…³å·¥ä½œ --caption--> å¤šcaption.vs.å•caption --paraphrases--> æœªå¤„ç†ç‰¹å¾å’Œè§†è§‰ä¿¡æ¯ --ä¸¤é˜¶æ®µç¼–ç --> ä¸­é—´seq.vs.2captions 
      ```
  
  
  * æ¨¡å‹æ–¹æ³•ï¼š
  
    * ```mermaid
      graph LR
      é€‰æ‹©è§†è§‰å¤è¿°captionå¯¹ --> è¯„åˆ†å‡½æ•° --> è®¾è®¡ä¸‰ä¸ªAttentionæ“ä½œ,å­¦ä¹ åˆ°å¤šæ¨¡æ€çŸ¥è¯† --> æœ€åsoftmaxè¾“å‡º
      ```

* æ›´å¤šç»†èŠ‚å¯è§æˆ‘ä¸ªäººçš„[slide](https://kdocs.cn/l/conDzdschwAn)

:white_check_mark: ::fire: :hammer_and_wrench: **[Text Generation & Image Caption] Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions**, in CVPR 2019. [[pdf](https://openaccess.thecvf.com/content_CVPR_2019/html/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.html)] [[torch](https://github.com/aimagelab/show-control-and-tell)]

![](https://s2.loli.net/2022/04/09/COnvomETrl6GRf2.png)

![](https://s2.loli.net/2022/04/09/7ASmXcCazOh9GsU.png)

```mermaid
  graph LR
  å¤–éƒ¨ä¿¡å·æ§åˆ¶ --> å›¾åƒä¸­çš„ä¸€ç»„åŒºåŸŸå— --> core((æ ¸å¿ƒ))
  core --> æ”¹å˜chunkçš„é¡ºåº
  core --> æ”¹å˜å›¾åƒçš„åŒºåŸŸ
  model((æ¨¡å‹)) --åŸºäºåŒºåŸŸçš„ç‰¹å¾ä¸çŠ¶æ€--> LSTM((LanguageModel,ä¸¤å±‚LSTM)) --ç¬¬ä¸€å±‚--> è®¡ç®—attention --æ³¨æ„--> æ‰€æœ‰åŒºåŸŸçš„ç‰¹å¾å‘é‡è¿›è¡Œmean-poolingä½œä¸ºå›¾åƒçš„æ€»ä½“ç‰¹å¾I
  LSTM --ç¬¬äºŒå±‚--> é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯
  model --ä½•æ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå›¾åƒåŒºåŸŸ--> å—è½¬ç§»é—¨ --è®¡ç®—gt--> åŸºäºç¬¬ä¸€å±‚LSTMçš„çŠ¶æ€è®¾ç«‹ä¸€ä¸ªchunk-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sc_t-rtçš„attention
  model --è§†è§‰è¯oræ–‡æœ¬æ¬¡--> AdaptiveAttention --> è®¾ç½®ä¸€ä¸ªvisual-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sv_t-rtçš„attention --> attentionçš„ç»“æœ,å¯ä»¥è®¡ç®—å‡ºå½“å‰æ—¶åˆ»æ¨¡å‹æ­£åœ¨å…³æ³¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ct
  model --æ— åºé›†åˆæ’åº--> æ’åºç½‘ç»œ --> Rä¸­åŒ…å«Nä¸ªåŒºåŸŸé›† --å…¨è¿æ¥å±‚--> æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --Sinkhornç®—å­--> è½¯ç½®æ¢çŸ©é˜µ 
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --> æœ€å°åŒ–è½¯ç½®æ¢ä¸çœŸå®ç»“æœä¹‹é—´çš„å‡æ–¹è¯¯å·®
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --æµ‹è¯•åŒˆç‰™åˆ©ç®—æ³•è¿›è¡ŒåŒ¹é…--> è½¯ç½®æ¢çŸ©é˜µè½¬åŒ–ä¸ºæœ€ç»ˆçš„ç½®æ¢,ä»¥æ­¤æ¥å¯¹Rè¿›è¡Œæ’åº
```

 [è¯¦ç»†è®²è§£](https://zhuanlan.zhihu.com/p/150667499)



:hammer_and_wrench: **Injecting Semantic Concepts into End-to-End Image Captioning**, in CVPR 2022.  [[pdf](https://arxiv.org/abs/2112.05230)]  [[torch](https://github.com/jacobswan1/ViTCAP)]

* ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œdetector-free
* åŠ å…¥Concept
  * é€šè¿‡æŠ½å–captionä¸­çš„åŠ¨åè¯æˆ–è€…é€šè¿‡çŸ¥è¯†è’¸é¦å¾—åˆ°ä¸€äº›å›¾åƒæ ‡ç­¾

## :sunglasses: Video Understanding

### :video_camera: Features Fusion

:white_check_mark: :fire: :hammer_and_wrench: **[TSN] Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**, in ECCV 2016.  [[pdf](https://arxiv.org/abs/1608.00859)]  [[torch](https://github.com/yjxiong/temporal-segment-networks)]

* æŠ½å–æ‰€æœ‰å¸§æ˜¯ä¸ç°å®çš„ï¼ŒTSNå°†å…¶**ç­‰é—´éš”åˆ†**ä¸º$K$ä¸ªç‰‡æ®µï¼ˆi.e., $K=16$ï¼‰,åœ¨æ¯ä¸ªç‰‡æ®µä¸­è°å¯„æŠ½å–ä¸€å¸§ä½œä¸ºè¾“å…¥

* æä¾›äº†éå¸¸å¸¸ç”¨çš„æ•°æ®äº‰å¼ºæ–¹å¼å’Œä¸€äº›è®­ç»ƒæ—¶å€™çš„trickï¼ˆä¸»è¦åŒ…æ‹¬location jittering, horizontal flipping, corner cropping, and scale jitteringï¼‰

* ä»ç„¶åˆ©ç”¨åŒæµçš„æ€è·¯ï¼Œè®©æ¯ä¸ªç‰‡æ®µä¿¡æ¯æœ€åé€šè¿‡ä¸€ä¸ªå…±è¯†ç½‘ç»œå†Fusion

  ![](https://pic4.zhimg.com/80/v2-67b66b3618606af8b81d1f77b1f92a3b_1440w.jpg)

:white_check_mark: :fire: :hammer_and_wrench: **[TRN] Temporal Relation Reasoning in Videos**, in ECCV 2018.  [[pdf]()] [[torch](https://github.com/zhoubolei/TRN-pytorch)]

![img](https://pic4.zhimg.com/80/v2-86fa6c271c9d2dfad07d4603ed457a83_1440w.jpg)

* èåˆå°ºåº¦ç¡®å®š (éœ€è¦å¤šå°‘ä¸ªè§†é¢‘å¸§æ¥èåˆ)ã€å¦‚å›¾æ‰€ç¤ºã€‘æœ‰2ï¼Œ3ï¼Œ4è¿™ä¸‰ç§å°ºåº¦
* æ¯ä¸ªå°ºåº¦ä¸‹éœ€è¦å¤šå°‘ç»„è§†é¢‘å¸§
* åœ¨åº”ç”¨å¤šå°ºåº¦TRNçš„æ—¶å€™ï¼Œä¸€èˆ¬ä¼šé¢å¤–å¢åŠ ä¸€ä¸ªå…¨å¸§çš„å°ºåº¦ï¼Œå³12å¸§ç‰¹å¾å…¨éƒ¨concatåˆ°ä¸€èµ·ï¼Œä»¥å……åˆ†åˆ©ç”¨æœ‰æ•ˆä¿¡æ¯ã€‚
* **å¹³è¡¡**æ•ˆæœå’Œè®¡ç®—é€Ÿåº¦ï¼Œ**ç®€å•å¥½ç”¨**

:white_check_mark: :fire: :hammer_and_wrench: **[TSM] TSM: Temporal Shift Module for Efficient Video Understanding**, in ICCV 2019.  [[pdf](https://arxiv.org/abs/1811.08383)] [[torch](https://github.com/mit-han-lab/temporal-shift-module)]

* å¯¹æŸäº›é€šé“shiftï¼Œå¾—åˆ°å‰ä¸€å¸§æˆ–è€…åä¸€å¸§çš„ç‰¹å¾

  ![image-20220709174802714](https://s2.loli.net/2022/07/09/cb1f8LWpV9JotO3.png)

* ç”±äºshiftæ˜¯æœ‰æŸå¤±çš„ï¼Œä¸ºæ­¤è®¾è®¡æ®‹å·®æ¥è¿›è¡Œå¼¥è¡¥ï¼ˆåŸæ¥çš„ä¸æ®‹å·®çš„å¯¹æ¯”ï¼‰

![image-20220709174842935](https://s2.loli.net/2022/07/09/2dTjLBJwQ5FUber.png)

:white_check_mark: :fire: :hammer_and_wrench: **[LRCN] Long-term Recurrent Convolutional Networks for Visual Recognition and Description**, in CVPR 2015.  [[pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf)] [[torch](https://github.com/garythung/torch-lrcn)]

* CNNæŠ½å‡ºæ¥çš„å¸§ç‰¹å¾å†æ”¾è¿›å»`LSTM`å¾—åˆ°æ¯å¸§çš„æ—¶åºç‰¹å¾

> å…³äºè§†é¢‘ç‰¹å¾æŠ½å–ï¼Œä¸‹é¢è®²ä¸€ä¸‹`netvlad`ç³»åˆ—çš„ç»“æ„,NextVladå°±æ˜¯ä¸“é—¨é’ˆå¯¹è§†é¢‘å¸§èåˆæ¥åšçš„ä¼˜åŒ–ã€‚
>
> [ç›¸å…³åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/385512915)

:fire: :hammer_and_wrench: **[NetVLAD] NetVLAD: CNN architecture for weakly supervised place recognition**, in CVPR 2016.  [[pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)] [[torch (simple)](https://github.com/lyakaap/NetVLAD-pytorch)]

* VLADç®—æ³•ï¼ˆå®é™…ä¸Šå°±æ˜¯Kmeansï¼‰ï¼š
  $$
  V(j, k)=\sum_{i=1}^{N} a_{k}\left(x_{i}\right)\left(x_{i}(j)-c_{k}(j)\right), \quad k \in K, j \in D
  $$

* æœ¬æ–‡ä½¿ç”¨`CNN`æ¨¡æ‹Ÿè¯¥VLADç®—æ³•çš„è¿‡ç¨‹

  * å¹³æ»‘åŒ–$\alpha$ ä½¿å…¶å˜æˆä¸€ä¸ª0-1åˆ†å¸ƒçš„æƒé‡å‚æ•°ï¼Œä½¿ç”¨$1 \times 1$å·ç§¯+softmax è¿›è¡Œè¯¥è¿‡ç¨‹ï¼Œå¹³æ»‘æ¨å¯¼å…¬å¼$\bar{a}_{k}\left(\mathbf{x}_{i}\right)=\frac{e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k}\right\|^{2}}}{\sum_{k^{\prime}} e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k^{\prime}}\right\|^{2}}}$

  

  ![image-20220712104829439](https://s2.loli.net/2022/07/12/aFlJSCw8dvBnzEH.png)

   

:fire: :hammer_and_wrench: **[NextVLAD] NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**, in ECCV workshop 2018.  [[pdf](https://arxiv.org/pdf/1811.05014.pdf)] [[tensorflow](https://github.com/linrongc/youtube-8m)]

> åŒæ—¶ï¼Œè¿˜æœ‰[å…³äºå¤šæ¨¡æ€ï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰Transformeræ¨¡å‹çš„åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/388361095)





### :timer_clock: Temporal Grounding

> æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªååˆ†ç»å…¸çš„ä»»åŠ¡ï¼ˆTemporal Groundingï¼‰æ¥çœ‹çœ‹è§†é¢‘çš„ç‰¹å¾æ˜¯å¦‚ä½•åˆ©ç”¨çš„

:fire::hammer_and_wrench: **[Video-NLP] Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, in AAAI 2020.  [[pdf](https://arxiv.org/pdf/1912.03590.pdf)] [[torch](https://github.com/microsoft/VideoX)]

> extend version: **MS-2D-TAN**, in TPAMI 2021.  [[pdf](https://arxiv.org/pdf/2012.02646.pdf)]  [[torch](https://github.com/microsoft/VideoX)]

* 2D: start time & end time æ„æˆçš„é‚»æ¥çŸ©é˜µ

![image-20220723153302775](https://s2.loli.net/2022/07/23/lUBL6uPHRFCzvxt.png)

* **æ ¸å¿ƒæ€æƒ³**ï¼š

  * ä½¿ç”¨max pooling ï¼ˆæœ¬æ–‡ä½¿ç”¨ï¼‰ æˆ–è€… stack convolutionçš„è·å–moment featureï¼Œå¦‚ä¸Šå›¾`2D Temporal Feature Map Extraction`æ‰€ç¤º
  * ç”±äºè¿™æ ·å­çš„è®¡ç®—å¼€é”€å¤ªå¤§äº†ï¼Œä½¿ç”¨**ç‰¹å®šçš„é‡‡æ ·æ–¹å¼**è¿›è¡Œè°ƒæ•´ï¼Œè¯¦è§è®ºæ–‡ï¼ï¼ˆè·ç¦»è¿‘çš„é‡‡æ ·å¤šä¸€ç‚¹ï¼Œè¿œçš„é‡‡æ ·å°‘ä¸€ç‚¹ï¼‰
  * å¤šæ¨¡æ€èåˆï¼ˆ`Hadamard product`ï¼‰

  $$
  \mathbf{F}=\left\|\left(\mathbf{w}^{S} \cdot \mathbf{f}^{S} \cdot \mathbb{1}^{T}\right) \odot\left(\mathbf{W}^{M} \cdot \mathbf{F}^{M}\right)\right\|_{F}
  $$

  * æŸå¤±è®¡ç®—æ—¶å€™ï¼Œå¯¹`IoU`è¿›è¡Œä¸€ä¸ªscaleå˜æˆç›‘ç£ä¿¡å·

  $$
  y_{i}= \begin{cases}0 & o_{i} \leq t_{\min } \\ \frac{o_{i}-t_{\min }}{t_{\max }-t_{\min }} & t_{\min }<o_{i}<t_{\max } \\ 1 & o_{i} \geq t_{\max }\end{cases}
  $$

  * BCE loss:

  $$
  L o s s=\frac{1}{C} \sum_{i=1}^{C} y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)
  $$

  

:fire: :hammer_and_wrench: **Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2109.04872)] [[torch](https://github.com/MCG-NJU/MMN)] [[blog](https://zhuanlan.zhihu.com/p/446203594)]

* ä¸»å¹²ç½‘ç»œæ˜¯æ²¿ç”¨[TDN](https://arxiv.org/abs/2012.10071)

* ä½¿ç”¨äº†**metric learning**çš„æ–¹æ³•å¹¶ä¸”å¼•å…¥**è´Ÿæ ·æœ¬**æ¥åšTemporal Groundingçš„ä»»åŠ¡

  * è§†é¢‘é—´çš„è´Ÿæ ·æœ¬ï¼ˆ`IoU`æ¥æ ‡å®šç›‘ç£ä¿¡å·`yi`ï¼Œä¸`2D-TAN`ä¸€æ ·å¤„ç†å¾—æ¥çš„ï¼Œè®°å¾—`scale`ä¸€ä¸‹ï¼‰
  * æ–‡æœ¬ä¸­çš„è´Ÿæ ·æœ¬ï¼Œä»å…¶ä»–è§†é¢‘çš„æ–‡æœ¬è¯­å¥å½“ä¸­é€‰å–å‡ºæ¥

* è´¡çŒ®

  * æ„é€ äº†æ–°çš„ç›‘ç£ä¿¡å·ï¼šè§†é¢‘é—´çš„æ­£è´Ÿæ ·æœ¬(`IoU`æ¥é‡‡æ ·)ï¼Œ å¥å­å’Œè§†é¢‘å¯¹åº”çš„æ­£è´Ÿæ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬å¥å­ä»åˆ«çš„è§†é¢‘æŠ½å–è¿‡æ¥ï¼‰
  * ä¸€ä¸ªè§†é¢‘åªéœ€è¦å»ºæ¨¡ä¸€æ¬¡ï¼Œå¤§å¤§èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä»¥å¾€çš„fusionæ–¹æ³•éƒ½æ˜¯è¦æ–‡æœ¬-è§†é¢‘å¸§å¯¹æ¥å»ºæ¨¡

* Trick

  * ä¸ºäº†ç¼–ç å…¬å¹³ï¼Œä½¿ç”¨é¢„è®­ç»ƒå¥½çš„`DistilBERT`æ¥è¿›è¡Œç¼–ç å¥å­

* æŸå¤±å‡½æ•°è®¡ç®—

  * å’Œ`2D-TDN`ä¸€æ ·çš„`BCE_loss`
  * ç±»ä¼¼äº`InfoNCE loss`çš„è®¾è®¡å¯¹æ¯”æŸå¤±

  $$
  \begin{aligned}
  &p\left(i_{s} \mid v\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)}{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)+\sum_{j \neq i}^{N_{s}} \exp \left(\mathbf{f}_{j}^{S T} \mathbf{f}^{V} / \tau_{v}\right)} \\
  &p\left(i_{v} \mid s\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)}{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)+\sum_{j \neq i}^{N_{v}} \exp \left(\mathbf{f}_{j}^{V T} \mathbf{f}^{S} / \tau_{s}\right)} \\
  &L_{m m}=-\left(\sum_{i=1}^{N} \log p\left(i_{v} \mid s_{i}\right)+\sum_{i=1}^{N} \log p\left(i_{s} \mid v_{i}\right)\right)
  \end{aligned}
  $$

  

### :man_student: Video Question Answer

**Invariant Grounding for Video Question Answering**, in CVPR 2022 oral.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)] [[torch](https://github.com/yl3800/IGV)]

> è¿™ç¯‡æ–‡ç« æ„Ÿè§‰æ˜¯ä¸€ç¯‡å¾ˆæ ‡å‡†çš„`CVPR`çš„ä¸­è§„ä¸­çŸ©æ–‡ç« ï¼Œå†™ä½œç”¨è¯ä¸Šéå¸¸å‡ºè‰²çš„

* å…ˆåšäº†Groundingçš„æ£€æµ‹ï¼Œæ£€æµ‹å‡ºé—®é¢˜ç›¸å…³å¸§ï¼ˆæœ‰å› æœå…³ç³»`Casual`ï¼‰è¿˜æœ‰æ— å…³å¸§ï¼ˆè¡¥å¿å¸§`Complement`ï¼‰
* æ„å»ºè´Ÿæ ·æœ¬åˆ°æ— å…³å¸§å½“ä¸­ï¼Œä½¿ç”¨`memory bank`æ¥å­˜å‚¨æ‰€æœ‰æ ·æœ¬ (å› æ­¤è¦æ³¨æ„å­˜å‚¨çš„ç‰¹å¾ç»´åº¦ä¸èƒ½å¤ªå¤§)

### :writing_hand: Video Caption

**[Video Caption] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2101.12059)]

:hammer_and_wrench: :fire: **[Video Caption] Robust Change Captioning**, in ICCV 2019. [[pdf](https://arxiv.org/pdf/1901.02527.pdf)] [[torch](https://github.com/Seth-Park/RobustChangeCaptioning)]

* è¾“å…¥ä¸ºå‰åå›¾åƒå¯¹ï¼Œäº”ç§å˜åŒ–ç±»å‹ï¼ˆcolor/material change,adding/dropping/moving an objectï¼‰
* æå‡ºä¸€ä¸ªæœ‰è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[CLEVR-Change](https://cs.stanford.edu/people/jcjohns/clevr/)ï¼ˆ80Kå›¾ç‰‡å¯¹ï¼‰ï¼Œå¹¶åœ¨æ— è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[Spot-the-Diff](https://github.com/harsh19/spot-the-diff)å–å¾—SOTAæ•ˆæœã€‚
* æ¨¡å‹ï¼šDual æ³¨æ„åŠ›ï¼Œ åˆ†è¾¨**è§†ç‚¹å˜åŒ–**ï¼Œ å…¶å®æ˜¯é€šè¿‡è¾“å…¥ä¸¤å¼ å·®ä¸å¤šçš„å›¾ç‰‡ï¼Œæå‰æ ‡å®šå¥½æ•°æ®é›†è·å¾—çš„ï¼Œæœ‰ç‚¹è¢«å‘çš„æ„æ€![image-20220522213419579](https://s2.loli.net/2022/05/22/fiUArgZIjlzw4p1.png)

:hammer_and_wrench: :fire: **[Video Caption] Semantic Grouping Network for Video Captioning**, in AAAI 2021. [[pdf](https://arxiv.org/pdf/2102.00831.pdf)] [[torch](https://github.com/hobincar/SGN)]

* ![image-20220621204108736](https://s2.loli.net/2022/06/21/DMmzxs7dKwyU6BE.png)

:hammer_and_wrench: :fire: **Hierarchical Context-aware Network for Dense Video Event Captioning**, in ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.156/)] [[torch](https://github.com/KirkGuo/HCN)]

* **å±€éƒ¨ä¿¡æ¯**+**å…¨å±€ä¿¡æ¯**ç»“åˆç”Ÿæˆdense caption ï¼ˆè¾“å…¥åŒ…æ‹¬**video** å’Œ **transcript**ï¼‰
* ä¸ºæ­¤è®¾è®¡äº†ä¸¤å¥—`Attention`æœºåˆ¶
  * falt attention + cross attention

![image-20220820000708776](https://s2.loli.net/2022/08/20/wdO5eWZcxgqsItT.png)

* è®¾è®¡äº†é—¨æœºåˆ¶æ¥decodeï¼ˆä¹‹å‰çš„æ–‡æœ¬ä¿¡æ¯ä¸æœªæ¥çš„æ–‡æœ¬ä¿¡æ¯ï¼‰



```mermaid
  graph LR
  SG(Semantic-Grouping) --å»æ‰å†—ä½™phrase--> ç›¸ä¼¼åº¦è®¡ç®—
  SG --attentionæœºåˆ¶ --> å¯¹å…¶phraseå’Œframe --> åŠ å…¥å¯¹æ¯”æŸå¤±,è®¡ç®—æ²¡æœ‰åŒ…å«negativeçš„æ¦‚ç‡
```

**å¯¹æ¯”æŸå¤±**$\mathcal{L}_{c a}=\sum_{(V, Y) \in \mathcal{D}} \sum_{t} \sum_{i}^{M_{t}}\left(-\log p_{c a}\left(s_{i, t}\right)\right)$, $p_{c a}\left(s_{i, t}\right)=\sum_{j=1}^{N} \alpha_{i, j, t}^{p o s}$    ($\alpha^{pos}$ ä¸ºæ­£æ ·æœ¬æ—¶å€™å¯¹é½æ³¨æ„åŠ›çš„æƒé‡) 







## :abc: Scene Text Recognization

:hammer_and_wrench: **From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network**, in ICCV 2021. [[pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2108.09661)] [[torch](https://link.zhihu.com/?target=https%3A//github.com/wangyuxin87/VisionLAN)]

* è¿‡å»çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«éœ€è¦ï¼šè§†è§‰ç‰¹å¾æŠ½å–å™¨ + è¯­è¨€æ¨¡å‹

* æœ¬æ–‡ç›´æ¥åœ¨è§†è§‰ç©ºé—´è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼ˆç±»ä¼¼äººç±»ï¼Œè¯­è¨€ä¿¡æ¯æ˜¯å¯ä»¥å­¦ä¹ çš„ï¼‰
  * å¯¹å­—ç¬¦çº§åˆ«çš„Maskæ“ä½œ![image-20220701212346925](https://s2.loli.net/2022/07/01/ZLFUIkb41S782GD.png)
    * è®­ç»ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨å¼±ç›‘ç£äº’è¡¥å­¦ä¹ ![image-20220701212430601](https://s2.loli.net/2022/07/01/kc3K7XxAN6SfRut.png)

:hammer_and_wrench: **Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition**, in AAAI 2022.  [[pdf]([https://arxiv.org/abs/2112.12916](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2112.12916))] [[torch]([https://github.com/adeline-cs/GTR](https://link.zhihu.com/?target=https%3A//github.com/adeline-cs/GTR))]
* åŠ å…¥ä¸€ä¸ªGCNå¼ºåŒ–äº†è§†è§‰å­¦ä¹ çš„è¿‡ç¨‹ï¼Œå¹¶ä¸”åšäº†ä¸€ä¸ªfusion



## :label: NER

>  Named Entity Recognition

**å½“å‰ç«èµ›NERä»»åŠ¡çš„baselineï¼š**

- BERT + BILSTM + CRF
- [åšå®¢è¿æ¥ NERé“æ‰“çš„baseline](https://zhuanlan.zhihu.com/p/166496466)

:fire: **Bidirectional LSTM-CRF Models for Sequence Tagging**, in 2015. [[pdf](https://arxiv.org/pdf/1508.01991v1.pdf)] [[code](https://paperswithcode.com/paper/bidirectional-lstm-crf-models-for-sequence)

* ä½¿ç”¨BiLSTM+CRFåšNERçš„å¼€å±±ä¹‹ä½œ
* [ç›¸å…³åšå®¢è¿æ¥](https://zhuanlan.zhihu.com/p/166496466)

:hammer_and_wrench: :fire: **Fast and Accurate Entity Recognition with Iterated Dilated Convolutions**, in EMNLP 2017.  [[pdf](https://arxiv.org/pdf/1702.02098.pdf)] [[tensorflow](https://github.com/iesl/dilated-cnn-ner)]

* Iterated Dilated Convolutions ç©ºæ´å·ç§¯ 


![image-20220825002628063](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825002628063.png)

* æ ¸å¿ƒæ€æƒ³

  * ä¼ ç»Ÿå·ç§¯çš„é—®é¢˜ï¼špoolingå±‚ä¼š**æŸå¤±ä¿¡æ¯**ï¼Œé™ä½ç²¾åº¦ã€‚é‚£ä¹ˆä¸åŠ poolingå±‚ä¼šä½¿**æ„Ÿå—é‡å˜å°**ï¼Œå­¦ä¸åˆ°å…¨å±€çš„ç‰¹å¾ã€‚å¦‚æœå•çº¯çš„å»æ‰poolingå±‚ã€æ‰©å¤§å·ç§¯æ ¸çš„è¯ï¼Œè¿™æ ·çº¯ç²¹çš„æ‰©å¤§å·ç§¯æ ¸åŠ¿å¿…å¯¼è‡´**è®¡ç®—é‡çš„å¢å¤§**ã€‚
  * CNNä¹Ÿå¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜

![image-20220825004251541](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825004251541.png)

* é€Ÿåº¦æ¯”ä»¥å‰çš„`Bi-LSTM-CRF`å¿«äº†éå¸¸å¤šï¼Œè€Œä¸”ç²¾åº¦æ²¡æœ‰ä¸‹æ»‘

:hammer_and_wrench: :fire: **BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision** , in KDD 2020.  [[pdf](https://arxiv.org/pdf/2006.15509.pdf)] [[torch](https://github.com/cliang1453/BOND)]

* è¿œè·ç¦»ç›‘ç£çš„é—®é¢˜

  * å™ªå£°ï¼Œè´Ÿæ ·æœ¬ä¸å¥½ç”Ÿæˆ
  * å®Œæ•´æ€§ä¸è¶³
  * `trade-off` åœ¨æ ‡æ³¨å‡†ç¡®åº¦å’Œè¦†ç›–èŒƒå›´ä¹‹é—´

* æƒ³æ³•

  * ç¬¬ä¸€é˜¶æ®µä½¿ç”¨`RoBERTa`å¾®è°ƒï¼Œé€‚åº”`NER`ä»»åŠ¡

    * ä½¿ç”¨`Early stopping` æ–¹æ³•é˜²æ­¢æ•°æ®è¿‡æ‹Ÿåˆè¿˜æœ‰å¯¹**æœªçŸ¥æ•°æ®å¢å¼ºæ³›åŒ–èƒ½åŠ›**
    * é¦–å…ˆé€šè¿‡`POS`è¯†åˆ«æ½œåœ¨å®ä½“ï¼Œç„¶åé€šè¿‡è¯­æ–™åº“è®¡ç®—æœ€å°æŸå¤±ç¡®å®šå®ä½“

    ![img](https://pic2.zhimg.com/80/v2-0951f0afefa53b2efc269f92136a3ae9_720w.jpg)

  * ç¬¬äºŒé˜¶æ®µè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ ï¼ˆteacher-studentæ¨¡å‹2ï¼‰

    * åº”å¯¹**å˜ˆæ‚å’Œä¸å®Œæ•´æ ‡æ³¨**çš„æŒ‘æˆ˜
    * teacherç”Ÿæˆä¼ªæ ‡ç­¾äº¤ç»™studentå»é¢„æµ‹
    * **é‡æ–°åŠ æƒçš„é«˜ç½®ä¿¡åº¦è½¯æ ‡ç­¾**
    * ==ç¬¬äºŒé˜¶æ®µçš„æ ¸å¿ƒå°±æ˜¯å¢å¼ºæ•°æ®çš„ç½®ä¿¡ç¨‹åº¦ï¼==

![img](https://pic4.zhimg.com/80/v2-8788aa61ea19b041e763b597d844ebcb_720w.jpg)

ä¸¤é˜¶æ®µçš„BONDæ¡†æ¶

* åœ¨é˜¶æ®µIä¸­ï¼Œç»è¿‡é¢„è®­ç»ƒçš„BERTç”¨äºæ—©åœçš„è¿œè·ç¦»NERä»»åŠ¡
* åœ¨é˜¶æ®µIIä¸­ï¼Œé¦–å…ˆä»é˜¶æ®µIä¸­å­¦ä¹ çš„æ¨¡å‹åˆå§‹åŒ–studentæ¨¡å‹å’Œteacheræ¨¡å‹ã€‚ç„¶åä½¿ç”¨teacheræ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹studentæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åŒæ—¶ï¼Œç”±æ—©åœçš„studentè¿­ä»£æ›´æ–°teacheræ¨¡å‹ã€‚



:hammer_and_wrench: :fire: **[A Boundary-aware Neural Model for Nested Named Entity Recognition](https://aclanthology.org/D19-1034.pdf)** , in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1034.pdf)] [[torch](https://github.com/thecharm/boundary-aware-nested-ner)]

* è§£å†³`Nested NER`çš„é—®é¢˜
* ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•
  * é¢„æµ‹è¾¹ç•Œ
  * é¢„æµ‹å®ä½“åˆ†ç±»

:hammer_and_wrench: :fire: **Cross-Domain NER using Cross-Domain Language** , in ACL 2020.  [[pdf](https://aclanthology.org/P19-1236/)] [[torch](https://github.com/jiachenwestlake/Cross-Domain_NER)]

* è§£å†³**è·¨é¢†åŸŸæ— ç›‘ç£**çš„æ ‡æ³¨é—®é¢˜
* æ ¸å¿ƒæ€æƒ³

![img](https://pic4.zhimg.com/80/v2-52cb3242e0a708e48b29ab2f8d81e027_720w.jpg)

æœ€åº•ä¸‹çš„ä¸€å±‚æ˜¯æ•°æ®å±‚ï¼Œæ ‡å‡†æƒ…å†µä¸€ä¸‹æ€»å…±æœ‰å››ä»½è¯­æ–™ï¼Œåˆ†åˆ«å¯¹åº”**ä¸¤ä¸ªdomainä¸‹çš„ä¸¤ä¸ªtaskï¼ˆNERå’Œè¯­è¨€å»ºæ¨¡ï¼‰**ã€‚å…¶ä¸­Source Domainï¼ˆå³ä¿è¯æœ‰æ ‡è®°æ•°æ®ç”¨äºNERçš„domainï¼‰å¯¹åº”ä¹‹å‰æåˆ°çš„News Domainï¼Œå› ä¸ºè®ºæ–‡ä¸­Source Domainä½¿ç”¨çš„æ˜¯æ–°é—»æ•°æ®ã€‚å¦å¤–å¦‚æœæ˜¯æ— ç›‘ç£æŠ½å–Target Domainæ•°æ®åˆ™åªæœ‰ä¸‰ä»½è¯­æ–™ã€‚

ç”±åº•å‘ä¸Šç¬¬äºŒå±‚æ˜¯Word Embeddingå±‚ï¼Œè®ºæ–‡ä¸­çš„Word Embeddingç»“åˆäº†è¯çº§åˆ«å’Œå­—ç¬¦çº§åˆ«çš„å‘é‡è¡¨ç¤ºã€‚å³æŠŠè¯å‘é‡å’Œä¸€ä¸ªè¯çš„å­—ç¬¦åºåˆ—å½¢æˆçš„çŸ©é˜µç»è¿‡CNNå¤„ç†åçš„**å‘é‡concatenateèµ·æ¥**ã€‚

ç¬¬ä¸‰å±‚æ˜¯åŒå‘LSTMï¼Œç”¨äºåºåˆ—å¤„ç†ç¬¬äºŒå±‚çš„æ•°æ®ï¼Œç”Ÿæˆå‰åå‘hidden stateã€‚

ç¬¬å››å±‚LMå’ŒCRFï¼Œå³task modelå±‚ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬å››å±‚æœ‰ä¸‰ä¸ªæ¨¡å—ï¼Œä¸¤ä¸ªæ˜¯ç”¨äºNERçš„CRFæ¨¡å‹ï¼Œåˆ†åˆ«å¯¹åº”Source Domainå’ŒTarget Domainã€‚å¦ä¸€ä¸ªæ˜¯åŸºäºç¬¬ä¸‰å±‚BiLSTMçš„è¯­è¨€æ¨¡å‹ï¼ŒNSSoftmaxæ˜¯æŒ‡è¿™ä¸ªè¯­è¨€æ¨¡å‹åˆ©ç”¨**Negative Sampling Softmaxçš„æ–¹å¼è¿›è¡Œè®­ç»ƒ**ã€‚



* æœ€å…³é”®çš„åœ°æ–¹å°±æ˜¯`Bi-LSTM`çš„å‚æ•°æ˜¯ç”Ÿæˆçš„ï¼Œä¸åŒdomainçš„ä¸åŒtaskéœ€è¦çš„LSTMçš„å‚æ•°å’Œ$I$æœ‰å…³

$$
\begin{equation}
\theta_{\mathrm{LSTM}}^{d, t}=\mathbf{W} \otimes \mathbf{I}_{d}^{D} \otimes \mathbf{I}_{t}^{T},
\end{equation}
$$





**NERçš„æœªæ¥**

æ—¢ç„¶æ¨¡å‹æ‰“ä¸åŠ¨äº†ï¼Œç„¶åæˆ‘æ‰¾äº†æ‰¾ ACL2020åšNERçš„è®ºæ–‡ï¼Œçœ‹çœ‹ç°åœ¨çš„NERè¿˜åœ¨åšå“ªäº›äº‹æƒ…ï¼Œä¸»è¦åˆ†å‡ ä¸ªæ–¹é¢

1. **å¤šç‰¹å¾**ï¼šå®ä½“è¯†åˆ«ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤æ‚çš„ä»»åŠ¡ï¼Œä¸éœ€è¦å¤ªæ·±å…¥çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå°±æ˜¯åŠ ç‰¹å¾ï¼Œç‰¹å¾è¶Šå¤šæ•ˆæœè¶Šå¥½ï¼Œæ‰€ä»¥å­—ç‰¹å¾ã€è¯ç‰¹å¾ã€è¯æ€§ç‰¹å¾ã€å¥æ³•ç‰¹å¾ã€KGè¡¨å¾ç­‰ç­‰çš„å°±ä¸€ä¸ªä¸ªåŠ å§ï¼Œç”šè‡³æœ‰äº›ä¸­æ–‡ NER ä»»åŠ¡é‡Œè¿˜åŠ å…¥äº†æ‹¼éŸ³ç‰¹å¾ã€ç¬”ç”»ç‰¹å¾...... å¿ƒæœ‰å¤šå¤§ï¼Œç‰¹å¾å°±æœ‰å¤šå¤š
2. **å¤šä»»åŠ¡**ï¼šå¾ˆå¤šæ—¶å€™åš NER çš„ç›®çš„å¹¶ä¸ä»…æ˜¯ä¸ºäº† NERï¼Œè€Œæ˜¯æœåŠ¡äºä¸€ä¸ªæ›´å¤§çš„ç›®æ ‡æˆ–ç³»ç»Ÿï¼Œæ¯”å¦‚ä¿¡æ¯æŠ½å–ã€é—®ç­”ç³»ç»Ÿç­‰ç­‰ã€‚å¦‚æœæŠŠæ•´ä¸ªå¤§ä»»åŠ¡åšä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œå°±éœ€è¦åšæˆä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼ŒæŠŠ NER ä½œä¸ºå…¶ä¸­ä¸€ä¸ªå­ä»»åŠ¡ï¼›å¦å¤–ï¼Œå•çº¯çš„ NER ä¹Ÿå¯ä»¥åšæˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚å®ä½“ç±»å‹è¿‡å¤šæ—¶ï¼Œä»…ç”¨ä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡æ¥åŒæ—¶æŠ½å–å®ä½“ä¸åˆ¤æ–­å®ä½“ç±»å‹ï¼Œä¼šæœ‰äº›åŠ›ä¸ä»å¿ƒï¼Œå°±å¯ä»¥æ‹†æˆä¸¤ä¸ªå­ä»»åŠ¡æ¥åš
3. **æ—¶ä»¤å¤§æ‚çƒ©**ï¼šæŠŠå½“ä¸‹æ¯”è¾ƒæµè¡Œçš„æ·±åº¦å­¦ä¹ è¯é¢˜æˆ–æ–¹æ³•è·Ÿ NER ç»“åˆä¸€ä¸‹ï¼Œæ¯”å¦‚ç»“åˆå¼ºåŒ–å­¦ä¹ çš„ NERã€ç»“åˆ **few-shot learning** çš„ NERã€ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯çš„ NERã€ç»“åˆè·¨è¯­ç§å­¦ä¹ çš„ NER ç­‰ç­‰çš„ï¼Œå…·ä½“å°±ä¸æäº† (Few-shot + Cross-domainæ˜¯ä¸ªä¸é”™çš„é€‰é¡¹ï¼)

ä½œè€…ï¼šç‹å²³ç‹é™¢é•¿
é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/166496466
æ¥æºï¼šçŸ¥ä¹
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚
