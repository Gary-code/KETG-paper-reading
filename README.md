

# Knowledge-enriched Text Generation paper reading

ğŸ˜ Awesome list of papers about knowledge-enhanced Question generation with notes.

:white_check_mark: : **already reading carefully**

:fire:: **high citation in recent years**

:hammer_and_wrench:: **available code**

> Content

[TOC]





## :bookmark_tabs: Question Generation & Answering & Reasoning

---

### :mountain_snow: **Textual QG**

**ä¸€ã€åˆ©ç”¨ç­”æ¡ˆå’Œè¯­è¨€ç‰¹å¾**

1. **ä¸¤ç¯‡Ground Breaking Work**

:white_check_mark: :fire: **Neural question generation from text: A preliminary study**, in EMNLP 2017. [[pdf](https://arxiv.org/abs/1704.01792)] 

* åœ¨ç¼–ç æ—¶é¢å¤–è€ƒè™‘äº†ç­”æ¡ˆä½ç½®ä¸è¯­æ³•ä¿¡æ¯ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚(ç°åœ¨æ¥çœ‹éå¸¸**basic**é‡è¦çš„ä¿¡æ¯ï¼)
  * word case åšè®­ç»ƒæ—¶å€™çš„teacher forcing
  * answer position feature
  * lexical features
    * **POS**
    * **NER**

```mermaid
graph LR
en((encoder)) --bi-GRU--> fe((feature-Rich)) --> word-vecotr
fe --> lexcial-feature-embedding-vectors --> POS+NER
fe --> answer-position-embedding --> BIO-tagging

word-vecotr --> åŒå‘çš„éšè—å±‚
POS+NER --> åŒå‘çš„éšè—å±‚
BIO-tagging --> åŒå‘çš„éšè—å±‚

de((decoder)) --å¸¦æ³¨æ„åŠ›æœºåˆ¶,ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›--> maxout-hidden+å…·ä½“éœ€è¦çœ‹referenceè®ºæ–‡
de --> GRU

de --> Copy-Mechanism,ä¸€æ ·ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ› --> è®¡ç®—å‡ºæ¦‚ç‡ä»sourceå¥å­ä¸­ç›´æ¥copyå•è¯
```



:white_check_mark: :fire: :hammer_and_wrench: **Learning to Ask: Neural Question Generation for Reading Comprehension**, in ACL 2017. [[pdf]](https://arxiv.org/abs/1705.00106) [[official code (torch)](https://github.com/xinyadu/nqg)]

* å°†ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œåº”ç”¨äºé—®é¢˜ç”Ÿæˆ
* é‡‡ç”¨seq2seq+attentionæ¨¡å‹æ¶æ„
* æ‘†è„±äº†è½¬æ¢è§„åˆ™ä¸æ¨¡ç‰ˆçš„å±€é™ï¼Œå–å¾—äº†ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„æ€§èƒ½
* åŠ å…¥äº†paragraph-level


```mermaid
graph LR
ä»»åŠ¡éš¾ç‚¹ --æ›´åŠ æ¥è¿‘äºäººç±»--> åŒä¹‰è¯æ›¿æ¢+çŸ¥è¯†å¼•å…¥ --> ç›¸å…³å·¥ä½œ --> è¿‡å»:rule-based 
ç›¸å…³å·¥ä½œ --> å…¶ä»–æ•°æ®æ˜ å°„è‡ªç„¶è¯­è¨€

Seq2Seq --> en((encoder)) --bidirectional--> softè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° --> lstm((LSTM))  --> only-sentence
lstm --> sentence+paragraph --> truncateæˆªæ–­,å½“ç„¶æ›´å¥½çš„æ–¹æ³•æ˜¯åˆ‡ç‰‡

Seq2Seq --> de((decoder)) --word-level-prediction--> LSTM((LSTM)) --> éšè—å±‚åˆå§‹åŒ– --basic-model --> å¥å­encoderçš„æœ€åéšè—å±‚
LSTM --oours--> å¥å­+æ®µè½çš„encoderè¾“å‡º


```

2. **ç­”æ¡ˆç¼–ç **

:white_check_mark: :fire: **Improving Neural Question Generation using Answer Separation**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1809.02393)] 

* å¾ˆå¤šåŸºç¡€æ“ä½œ
* åœ¨ç­”æ¡ˆä¸Šåšäº†ç®€å•é«˜æ•ˆçš„é¢„å¤„ç†
  * Mask åŸæ–‡ä¸­çš„ç­”æ¡ˆ
  * å¯¹ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯åšæŠ½å–ï¼Œè®¡ç®—attention

3. **è¯­è¨€ç‰¹å¾å¼ºåŒ–**

> ä¼ ç»Ÿçš„æœ‰**POS**ï¼ˆè¯æ€§æ ‡æ³¨ï¼‰å’Œ**NER**ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚åç»­è¿˜æœ‰ä¸€äº›æ›´åŠ ç»†å¾®çš„å¤„ç†

:fire: **Learning to Generate Questions by Learning What not to Generate**, in WWW 2019.  [[pdf](https://arxiv.org/pdf/1902.10418.pdf)] 

* clue å’Œ copyçš„æœºåˆ¶
* ![image-20220703112845472](https://s2.loli.net/2022/07/03/hoa1kTVzIpXGeQK.png)
* ![image-20220703113749291](https://s2.loli.net/2022/07/03/bdToKkIADvemV6B.png)
* æ–‡ç« è´¡çŒ®
  * å¸®åŠ©æ¨¡å‹å†³ç­–ä»€ä¹ˆæ—¶å€™ç”Ÿæˆï¼Œä»€ä¹ˆæ—¶å€™copy
  * ç”Ÿæˆå¤šä¸ªé—®é¢˜

4. ç–‘é—®è¯ç±»å‹ï¼ˆquestion typeï¼‰

**Question-type Driven Question Generation**, in EMNLP 2019.  [[pdf](https://arxiv.org/pdf/1909.00140.pdf)]

* å¼•å…¥å¯¹ç–‘é—®è¯çš„é¢„æµ‹æ¨¡å—ï¼Œå¹¶ä¸”åŠ å…¥å¯¹åº”çš„æŸå¤±å‡½æ•°
* ![image-20220703121312077](https://s2.loli.net/2022/07/03/uhcs1erWpQ9UFKL.png)
* ![image-20220703121503229](https://s2.loli.net/2022/07/03/GTO7j5c1AkXNnqY.png)
* ![image-20220703121520488](https://s2.loli.net/2022/07/03/seIfghRSbvpO68a.png)
* [æŸå¤±å‡½æ•°å¼•æ–‡](https://aclanthology.org/P17-1099.pdf)ï¼š <img src="https://s2.loli.net/2022/07/03/UZv17XkMyLRPOgd.png" alt="image-20220703122045296" style="zoom: 50%;" />

**äºŒã€æ®µè½çº§åˆ«ç‰¹å¾**

1. **å¼ºåŒ–æ®µè½çº§åˆ«æ–‡æœ¬çš„ç‰¹å¾**

:fire: :hammer_and_wrench: **Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks**, in EMNLP 2018. [[pdf](https://github.com/seanie12/neural-question-generation)] [[torch](https://github.com/seanie12/neural-question-generation)]

* ä¸»è¦è´¡çŒ®éƒ½åœ¨æ¨¡å‹ä¸Šé¢ï¼ŒåŸºäºseq2seqè®¾è®¡ï¼š![image-20220704150006013](https://s2.loli.net/2022/07/04/trhe5uTRkUqyfsx.png)

  * gate self-attention: ä¸ªäººè§‰å¾—æ˜¯ä¸€å¥—å¾ˆå¸¸ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸‹ï¼Œä¹Ÿéå¸¸ç®€å•

  * Maxout ==**Pointer**== & Decoding **å…¨æ–°çš„å¤„ç† copy æœºåˆ¶**  (æœ‰ç©ºå¯ä»¥è‡ªè¡Œå»çœ‹çœ‹ä»£ç ï¼)

    * ä¹‹å‰copyå¾—åˆ†ï¼š$\operatorname{sc}^{\text {copy }}\left(y_{t}\right)=\left\{\begin{array}{l}\sum_{k, \text { where } x_{k}=y_{t}} r_{t, k}, \quad y_{t} \in \chi \\ -i n f, \text{otherwise}\end{array} \quad\right.$ , é—®é¢˜åœ¨äºè‹¥æ–‡ç« ä¸­æŸä¸ªå•è¯é‡å¤å‡ºç°å¤šæ¬¡ï¼Œåˆ™å¯¹è¯¥å•è¯copyä¹Ÿä¼šå¤šï¼Œå½±å“è¯­å¥é€šé¡ºã€‚

    * ä¸ºæ­¤æ”¹è¿›ä¸ºMaxout Pointerï¼š
      $$
      \operatorname{sc}^{\text {copy }}\left(y_{t}\right)= \begin{cases}\max _{k, \text { where } x_{k}=y_{t}} r_{t, k}, & y_{t} \in \chi \\ -i n f, & \text { otherwise }\end{cases}
      $$

:fire: **Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model**,in ICLR 2020. [[pdf](https://arxiv.org/abs/1908.04942)] [[torch](https://github.com/hugochan/RL-based-Graph2Seq-for-NQG.)]

* å°†passageå’Œanswerçš„è¡¨ç¤ºï¼ˆåŒ…å«bertå‘é‡ï¼Œgloveå‘é‡ï¼Œè¯æ±‡ç‰¹å¾ç­‰ï¼‰è¿›è¡Œå¤šæ¬¡åå¤çš„äº¤äº’è¿›è¡Œç¼–ç ï¼ˆ**éå¸¸ç»†èŠ‚**çš„deep alignment networkï¼‰
* åˆ©ç”¨GNNæ¥ç¼–ç ï¼ˆä½¿ç”¨äº†ä¸¤ç§æ–¹å¼ï¼‰ï¼š
  * å¯¹sentenceåš**dependency parsing**ï¼Œç„¶åç›¸é‚»çš„å¥å­é“¾æ¥å¾—åˆ°passageçš„å›¾
  * é€šè¿‡self attentionçš„æ–¹å¼å¾—åˆ°passage çš„å›¾ï¼ˆæƒå€¼çŸ©é˜µï¼‰

:fire: **Improving Question Generation With to the Point Context**, in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1317.pdf)]

* è”åˆå»ºæ¨¡éç»“æ„åŒ–å¥å­ï¼ˆåŸæ–‡ï¼‰å’Œç»“æ„åŒ–ç­”æ¡ˆç›¸å…³å…³ç³»ï¼ˆ answer-relevant relation é¢„å…ˆä»å¥å­ä¸­æå–ï¼‰ä»¥ç”Ÿæˆé—®é¢˜(**æŠ“å–é‡ç‚¹ä¸Šä¸‹æ–‡**)
* ä½œè€…å‘ç°ä¸Šä¸‹æ–‡ä¸­ï¼Œè·ç¦»ansewræ¯”è¾ƒè¿œçš„è¯å¹¶ä¸ä¸€å®šä¸é‡è¦ï¼Œç›¸å¯¹çš„è·Ÿanswerç´§è´´çš„è¯ä¹Ÿæœ‰å¾ˆå¤šæ— å…³çš„ï¼Œä¸ºäº†æ•æ‰è¿™ç§å…³ç³»ï¼Œä½œè€…ä½¿ç”¨**OpenIE**è¿™ä¸ªå·¥å…·æŠ½å–ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨çš„**å…³ç³»ä¸‰å…ƒç»„**ã€‚

**ä¸‰ã€å¤šä»»åŠ¡è®­ç»ƒ**

**Multi-Task Learning with Language Modeling for Question Generation**, in EMNLP 2019. [[pdf](http://aclanthology.lst.uni-saarland.de/D19-1337.pdf)]

* æŠŠè¯­è¨€æ¨¡å‹ï¼ˆé¢„æµ‹å‰åè¯ï¼‰å’ŒQGä½œä¸ºmulti-taskä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚
* ä¸¤ä¸ªä»»åŠ¡æ˜¯å±‚çº§çš„å…³ç³»ï¼Œå…ˆè¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„æµ‹ï¼Œç„¶åå°†è¯­è¨€æ¨¡å‹çš„hiddenä½œä¸ºç‰¹å¾æä¾›ç»™åé¢seq2seq

:fire: **Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1912.00879)]

* å‡ºå‘ç‚¹æ˜¯æ˜¯**è§£å†³ç”Ÿæˆé”™è¯¯çš„ç–‘é—®è¯**å’Œ**copyåŸæ–‡ä¸­æ— å…³è¯**çš„é—®é¢˜

* ä½œè€…è®¤ä¸ºç”Ÿæˆé”™è¯¯è¯çš„åŸå› æ˜¯æ²¡æœ‰æ­£ç¡®çš„åˆ©ç”¨**answer position**ä¿¡æ¯ï¼Œcopyæ— å…³è¯çš„åŸå› æ˜¯ç¼ºä¹**å±€éƒ¨è¯­ä¹‰ä¿¡æ¯**ã€‚

* ä¸ºäº†åˆ†åˆ«ç¼“è§£è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½œè€…ä¹Ÿæ˜¯è®¾è®¡äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼š

  * è¯­ä¹‰åŒ¹é…åˆ†ç±»ï¼šè¿™ä¸ªä»»åŠ¡çš„è®¾è®¡å‡ºå‘ç‚¹ä¹Ÿæ˜¯SQuADçš„æ•°æ®ç‰¹ç‚¹ï¼Œå¯¹äºä¸€ä¸ªpassageå­˜åœ¨å¤šä¸ªanswer-questionè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å¯¹è¿™æ ·çš„æ•°æ®å®¹æ˜“äº§ç”Ÿä¸€äº›å®½æ³›ä¸å…·ä½“çš„é—®é¢˜ã€‚æ‰€ä»¥ä½œè€…æŠŠpassage-questionä½œä¸ºæ­£æ ·æœ¬ï¼Œpassage-random selected questionï¼Œ random selected passage-questionä½œä¸ºè´Ÿæ ·æœ¬è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚

  ![img](https://pic3.zhimg.com/80/v2-6e137d01fd833693fcc6cf526a39fb8e_720w.jpg)

  * answer-postionä½ç½®é¢„æµ‹ï¼šäº†è®©æ¨¡å‹æ›´å¥½çš„åˆ©ç”¨answerä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹answeråœ¨ä¸Šä¸‹æ–‡ä¸­startå’Œendä½ç½®çš„æ¨¡å‹ï¼ˆpointer networkï¼‰ï¼Œå…¶ä¸­åŸºç¡€çš„ç¼–ç éƒ¨åˆ†é‡‡ç”¨BiDAFçš„æ–¹å¼ã€‚
  * ç„¶å**QGå’Œè¿™ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ä¸€èµ·è®­ç»ƒ**ï¼Œæ•ˆæœå¯ã€‚

:hammer_and_wrench: **Varifocal Question Generation for Fact-checking**, in EMNLP 2022. [[pdf](https://arxiv.org/abs/2210.12400)] [[torch]()]

> ä½¿ç”¨QGè¾…åŠ©åšFact-checkingä»»åŠ¡ï¼ˆå¾ˆæœ‰ç”¨çš„ä¸€ä¸ªåº”ç”¨åœºæ™¯ï¼‰ï¼Œä¹Ÿæœ‰å¯¹åº”çš„æ•°æ®é›†

![image-20221113161641588](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221113161641588.png)





:fire: :hammer_and_wrench: **Generative Language Models for Paragraph-Level Question Generation**, in EMNLP 2022 oral. [[pdf](https://arxiv.org/abs/2210.03992)] [[demo](https://autoqg.net/#/)]

> å¤§æ¨¡å‹åšQGçš„Benchmark
>
> * éå¸¸solidçš„ä¸€é¡¹å·¥ä½œ

* åŠ¨æœº
  * ä¹‹å‰éƒ½æ˜¯åŸºäºè¯„ä»·æŒ‡æ ‡æ¥è¿›è¡Œè®¡ç®—çš„ï¼Œå¦‚`BLEU`ç­‰
  * å­˜åœ¨å¾ˆå¤šæ–¹é¢çš„çº¦æŸï¼Œæ¯”å¦‚å¯å›ç­”æ€§ï¼Œè¾“å…¥ä¸ä¸€æ ·ç­‰
* è´¡çŒ®
  * ç»Ÿä¸€åˆ°ä¸€ä¸ªbenchmarkå½“ä¸­
    * ç‰¹å®šé¢†åŸŸçš„dataset
    * å¤šç§è¯­è¨€ï¼ˆ8ç§ï¼‰
  * åœ¨LMä¸­è¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¸Šä¸€ç‚¹æåˆ°çš„æ•°æ®ä¸­è¿›è¡ŒéªŒè¯
  * å¤šæ–¹é¢éªŒè¯
    * è‡ªåŠ¨è¯„ä»·æŒ‡æ ‡
    * äººå·¥è¯„ä»·æŒ‡æ ‡



:hammer_and_wrench: **Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-Centric Summarization**, in ACL 2022. [[pdf](https://arxiv.org/abs/2203.14187)] [[torch](https://github.com/zhaozj89/Educational-Question-Generation)]

* åŠ¨æœº

  * ä»ç«¥è¯æ•…äº‹å½“ä¸­ç”Ÿæˆ`hugh-cognitive-demand`çš„é—®é¢˜å¾ˆæœ‰æ„ä¹‰
  * è¿‡å»éƒ½æ˜¯`low-dognitive-demand (LCD)` é—®é¢˜æè¿°ï¼Œæ¯”å¦‚è¯´ ä»–æ˜¯è°ï¼Ÿ
  * æ•…äº‹çš„äº‹ä»¶ä¹‹é—´æ˜¯æœ‰è”ç³»çš„ï¼Œéœ€è¦é—®å‡ºactionï¼Œcausal relationshipçš„é—®é¢˜

* æ–¹æ³•

  ![image-20221114170953173](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221114170953173.png)



:hammer_and_wrench: **CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation**, in ACL 2022.  [[pdf](https://aclanthology.org/2022.acl-long.475.pdf)] [[torch](https://github.com/sion-zcfei/CQG)]

* åŠ¨æœº

  * è¿‡å»çš„å¤šæŒ‘QGæ–¹æ³•æ— æ³•ä¿è¯é—®é¢˜çš„**å¤æ‚ç¨‹åº¦**ï¼ˆcomplexityï¼‰
  * ä¸¤å¤§æŒ‘æˆ˜
    * å»ºç«‹ä¸åŒæ–‡æ¡£ä¿¡æ¯çš„è”ç³»
    * complex chains of entities

  ![image-20221115112431373](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221115112431373.png)

* æ–¹æ³•

  * å…ˆè¯•ç”¨GATæŠ½å–å¤šæ–‡æ¡£ä¹‹é—´è”ç³»ï¼Œ`Standford corenlp toolkit`æ¥å»ºç«‹å®ä½“å›¾

    * é‡ç‚¹åœ¨äºæ‰¾åˆ°å…³é”®çš„å®ä½“ï¼Œå’Œgtè¿›è¡Œlossçš„è®¡ç®—

  * è®¾è®¡flag tagæ¥çº¦æŸchain of entitiesï¼Œä¿è¯é—®é¢˜ç”Ÿæˆçš„å¤æ‚ç¨‹åº¦ï¼ˆæ³¨æ„ï¼šå¹¶ä¸æ˜¯`teacher forcig`ï¼‰
    $$
    \operatorname{flag}_i^t= \begin{cases}0 & x_i \text { is not a constrain } \\ 1 & x_i \text { does not appear in } y_{1: t} \\ 2 & x_i \text { appear in } y_{1: t}\end{cases}
    $$

![image-20221115112909494](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221115112909494.png)

  * æ¨¡å‹å›¾

  ![image-20221115155002446](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221115155002446.png)




:hammer_and_wrench: **[Meta Learning] DSM: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner**, in EMNLP 2022. [[pdf](https://xiaojingzi.github.io/publications/EMNLP22-Guo-et-al-DSM.pdf)] [[torch](https://github.com/RUCKBReasoning/DSM)]

> COLING 2022 ä¹Ÿæœ‰ä¸€ç¯‡[CQG](https://aclanthology.org/2022.acl-long.475/)ä½¿ç”¨**Meta Learning**åšQGçš„ï¼Œå’Œè¿™ç¯‡å¾ˆåƒ

* åŠ¨æœº

  * ä¸åŒçš„é—®é¢˜å®é™…ä¸Šæœ‰**ä¸åŒçš„çŸ¥è¯†è¯­ä¹‰ç»“æ„**
  * è¿‡å»çš„æ¨¡å‹å¾€å¾€éƒ½æ˜¯ä½¿ç”¨**ä¸€ä¸ªæ–¹æ³•æ¥æ‹Ÿåˆè¿™äº›ä¸åŒè¯­ä¹‰çš„é—®é¢˜**ï¼Œä½†æ˜¯è¿™æ ·å­ä¼šå¯¼è‡´æ¨¡å‹è®­ç»ƒå›°éš¾ï¼ˆå®é™…ä¸Šè¿˜å¯ä»¥è§£å†³æ•°æ®imbalanceçš„é—®é¢˜ï¼ŒCOLING2022 CQGä¸­çš„åŠ¨æœºå°±æ˜¯è¿™ä¸ªï¼‰

  ![image-20230218215343124](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230218215343124.png)

* æ–¹æ³•æ¨¡å‹ï¼Œç±»ä¼¼**MAMLçš„å…ƒå­¦ä¹ å™¨**ï¼Œ [MAMLç®€å•è§£æ](https://blog.csdn.net/weixin_42392454/article/details/109891791)

![image-20230218215656301](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230218215656301.png)



:hammer_and_wrench: **Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.03088)] [[torch](https://github.com/dxlong2000/SG-CQG)]

> æœ‰ç‚¹æ•™è‚²å­¦ + AIçš„æ„æ€ï¼Œä½†å®é™…ä¸Šè¿˜æ˜¯æ¯”è¾ƒç®€å•çš„åšæ³•

* åŠ¨æœºï¼šWhat-to-ask and How-to-ask ï¼ˆ**æ•™è‚²å­¦**ï¼‰
* æ–¹æ³•ï¼ˆRoBERTa$_{large}$ ä½œä¸º**QT classifier**ï¼Œ é¢„è®­ç»ƒå¥½çš„T5$_{base}$ä½œä¸ºä¸»å¹²ç½‘ç»œå‚æ•°ï¼‰

![image-20230510163004578](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230510163004578.png)



:hammer_and_wrench: **Synthetic Question Value Estimation for Domain Adaptation of Question Answering**, in ACL 2023. [[pdf](https://aclanthology.org/2022.acl-long.95.pdf)] [[torch](https://github. com/xiangyue9607/QVE)]

* åŠ¨æœº
  * ç›®æ ‡é¢†åŸŸçš„QAï¼Œéœ€è¦ç”Ÿæˆç›®æ ‡é¢†åŸŸçš„QGï¼ˆè®¾ç½®æœ‰ç‚¹å¤æ‚ï¼‰

![image-20230718215810631](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230718215810631.png)

* æ¨¡å‹æ–¹æ³•

![image-20230718215838581](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230718215838581.png)


****

### :sunrise: Visual QG

:fire: **Generating Natural Questions About an Image**, in ACL 2022.

> ç¬¬ä¸€ç¯‡VQGçš„å·¥ä½œ

* åŠ¨æœº

  * ä¸captionç›¸æ¯”ï¼ŒVQGæ›´å¸Œæœ›ç¨³ä¸€äº›è¶…å‡ºè¯­è¨€è¡¨è¾¾æœ¬èº«ï¼Œå…·æœ‰é€»è¾‘æ¨ç†ï¼Œ**å¸¸è¯†çŸ¥è¯†**çš„ä¸€äº›é—®é¢˜

  ![image-20221111221831899](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221111221831899.png)

  ![image-20221111221858832](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221111221858832.png)

* æ–¹æ³•

  * å¾ˆç®€å•çš„æ–¹æ³•ï¼Œå°±æ˜¯ä¸€äº›ç®€å•çš„baselineå°è¯•

:hammer_and_wrench: **[No Visual] Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing**, in AAAI 2021. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17544)] [[torch](https://github.com/VISLANG-Lab/EGSS)]

* Multi-feature Encoder: ä½¿ç”¨äº†POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰+ NERï¼ˆå…³ç³»æŠ½å–ï¼‰



:white_check_mark:  :hammer_and_wrench:  **Mixture Content Selection for Diverse Sequence Generation**, in EMNLP 2019.[[pdf](https://arxiv.org/abs/1909.01953)] [[torch](https://github.com/clovaai/FocusSeq2Seq)]

:hammer_and_wrench: **Radial Graph Convolutional Network for Visual Question Generation**, in IEEE Transactions on Neural Networks and Learning Systems 2020. [[pdf](https://ieeexplore.ieee.org/document/9079208)] [[torch](https://github.com/Wangt-CN/VQG-GCN)]

### :sunflower: VQG

> åœ¨2022å¹´çš„ä»Šå¤©ï¼ŒVQAä»»åŠ¡ä¸å¤ªå¯èƒ½ä»åˆ·åˆ†çš„è§’åº¦æ¥å…¥æ‰‹äº† [[Blogé“¾æ¥](https://www.zhihu.com/question/419828408/answer/1595386400)]
>
> - VQAä»»åŠ¡æ˜¯ä»€ä¹ˆ
>
> - ä»‹ç»ä¹‹å‰çš„æ¨¡å‹å’Œæ–¹æ³•
>
> - æ¬¢è¿æ¥åˆ°Transformerçš„æ—¶ä»£
>
> - - 2019ï¼šå°è¯•å¤šæ¨¡æ€è¡¨å¾
>   - 2020ï¼šæ‹¥æŠ±å¤šæ¨¡æ€è¡¨å¾
>   - 2021ï¼šç»Ÿä¸€æ„æ¶çš„æ¢ç´¢

machine reading comprehension (**MRC**)å’Œquestion answering (QA)çš„å…³ç³»å…¶å®æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚Pure VQAä»»åŠ¡ä¸€èˆ¬æ˜¯æ²¡æœ‰å¼•å…¥é¢å¤–çš„**æ–‡æœ¬å†…å®¹**ï¼Œåªæ˜¯å•çº¯çš„æœ‰$\{å›¾ï¼Œ é—®å¥ï¼Œ å›ç­”\}$ã€‚è€ŒMultimodal MRCä»»åŠ¡ï¼Œå®é™…ä¸Šå°±åªæ˜¯å¼•å…¥äº†**é¢å¤–çš„context**ä½œä¸ºVQAä»»åŠ¡çš„çŸ¥è¯†ï¼Œå¹¶ä¸”æ›´åŠ æ³¨é‡äºè‡ªç„¶è¯­è¨€çš„ç†è§£ã€‚MRCçš„ä¸»è¦**ä»»åŠ¡ç±»å‹**ä¸€å…±æœ‰å››ç§ï¼Œåˆ†åˆ«ä¸º:

* å®Œå½¢å¡«ç©ºï¼ˆCloze Styleï¼‰
* å¤šé¡¹é€‰æ‹©ï¼ˆMultiple Choiceï¼‰
* ç‰‡æ®µæŠ½å–ï¼ˆSpan Predictionï¼‰
* è‡ªç”±ä½œç­”ï¼ˆFree-form Answerï¼‰

**[éæ·±åº¦å­¦ä¹ æ–¹æ³•] Answer-Type Prediction for Visual Question Answering**ï¼Œin CVPR 2016. [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780907)]

* é¢„æµ‹é—®é¢˜ç±»åˆ«ï¼ˆäººä¸ºæ ‡å®šï¼‰çš„æ¦‚ç‡å†å›ç­”é—®é¢˜
* åˆ©ç”¨**è´å¶æ–¯ç®—æ³•**å¯¹ç›®æ ‡çš„ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè®¡ç®—å‡ºæ¯ä¸ªç­”æ¡ˆçš„æ¦‚ç‡
* å…¶æœ‰æ•ˆæ€§ä¸å¦‚ç®€å•çš„åŸºçº¿æ¨¡å‹ï¼›éƒ¨åˆ†åŸå› åœ¨äºå…¶**ä¾èµ–è¯­ä¹‰åˆ†å‰²çš„ç»“æœ**



**Differential Attention for Visual Question Answering**, in CVPR 2018. [[pdf](https://arxiv.org/pdf/1804.00298.pdf)]

* è§£å†³ä¸ºäº†è®©æ¨¡å‹æ›´åŠ å…³æ³¨åˆ°**äººç±»æ‰€å…³æ³¨**çš„åŒºåŸŸ

![image-20220910151132747](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220910151132747.png)



:hammer_and_wrench: **Multiple Objects-Aware Visual Question Generation**, in ACM MM 2021. [[pdf](https://dl.acm.org/doi/abs/10.1145/3474085.3476969)]

* **å†™ä½œä¸Šå†™å¾—å¾ˆå®åœ¨ï¼Œå¾ˆå®¹æ˜“æ‡‚**ï¼Œæœ‰å¾ˆå¤šæ‰¿ä¸Šå¯ä¸‹çš„å¥å­ã€‚
* é¦–æ¬¡å°†**å¯¹è±¡**èå…¥åˆ°é—®é¢˜ç”Ÿæˆä»»åŠ¡å½“ä¸­

![image-20220629230122528](https://s2.loli.net/2022/06/29/6Mn4HPG9ZiCjOqv.png)

:hammer_and_wrench:  **Difficulty-Controllable Visual Question Generation**, in APWeb-WAIM 2021. [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-85896-4_26.pdf)]

* **éš¾åº¦å¯æ§**çš„é—®é¢˜ç”Ÿæˆï¼šé‡‡ç”¨äº†æ•™è‚²å­¦é¢†åŸŸæ”¶é›†å¥½çš„é—®é¢˜éš¾åº¦æ ‡ç­¾(DIF), è¯¦è§[é“¾æ¥](https://www.apims.net/index.php/apims/article/view/9)
* åœ¨VQA2.0æ•°æ®é›†çš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŒºåˆ†ä¸ºå®¹æ˜“å’Œéš¾çš„é—®é¢˜æ•°æ®é›†
  * å¼•å…¥ä¸¤ä¸ªVQAçš„æ¨¡å‹æ¥è¿›è¡Œå›ç­”ï¼Œéƒ½å›ç­”å¯¹çš„ä¸ºå®¹æ˜“ï¼Œéƒ½å›ç­”é”™è¯¯å°±æ˜¯éš¾çš„

* ![image-20220629231350190](https://s2.loli.net/2022/06/29/POftb69si7hnINX.png)
  * å…¶ä¸­Difficulty Variableå°±æ˜¯$\{0, 1\}$



:hammer_and_wrench: **Learning to Caption Images Through a Lifetime by Asking Questions**, in ICCV 2019.  [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9009050)] [[torch](https://github.com/fidler-lab/Caption-Lifetime-by-Asking-Questions)]

* å°†Caption å’Œ VQG ä¸€èµ·æ¥åšï¼Œæå‡ç”Ÿæˆçš„æ€§èƒ½

:hammer_and_wrench: **Inferential Visual Question Generation**, in MM 2022.  [[pdf](https://dl.acm.org/doi/10.1145/3503161.3548055)] [[dataset & code](https://github.com/bcxbg/InVQG)]

> æ— éœ€æ¨¡å‹è®­ç»ƒçš„ï¼Œå°±æ˜¯äººä¸º**è§„å®šå‡½æ•°æ¨¡ç‰ˆ**è§„åˆ™çš„æ–¹æ³•ç”Ÿæˆçš„ã€‚

* åŠ¨æœº
  * è¿‡å»VQGçš„æ–¹æ³•ç”Ÿæˆé—®é¢˜ä¸å¤Ÿchallenge æˆ–è€…ä¾èµ–äºäººå·¥æ ‡è®°
* è¿‡å»çš„VQG
  * åªæ˜¯ç®€å•çš„åè½¬ä¸€ä¸‹VQA
  * æ•°æ®é›†å½“ä¸­æœ‰å¾ˆå¤šlong tailå’Œè¯­ä¹‰è”ç³»å¤ªå¼±äº†
  * ç”Ÿæˆé—®é¢˜å¤ªè¿‡æ™®é€šï¼Œè€Œä¸”ç¼ºå°‘å…³é”®ä¿¡æ¯
  * ä¸Captionä¸åŒçš„æ˜¯ï¼ŒVQGéœ€è¦ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜
* æ–¹æ³•

![image-20221015215029327](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221015215029327.png)

* ä¾‹å­
  * å¢åŠ å¯äº¤äº’æ€§

![image-20221015215356890](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221015215356890.png)

![æˆªå±2022-10-15 21.54.35](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-15%2021.54.35.png)

:hammer_and_wrench: **Multi-VQG: Generating Engaging Questions for Multiple Images**, in EMNLP 2022. [[pdf](https://arxiv.org/abs/2211.07441)]] [[dataset](https://github.com/AcademiaSinicaNLPLab/MVQG-Dataset-of-Generating-Engaging-Questions-for-Multiple-Images)]

> Baselineå°±æ˜¯`VL-T5`ï¼Œè¯¦è§è®ºæ–‡

* åŠ¨æœº
  * è¿‡å»çš„VQGä¸»è¦æ˜¯é’ˆå¯¹ä¸€å¼ å›¾ç‰‡è¿›è¡Œæé—®çš„ï¼Œå¾€å¾€å›ç­”çš„éƒ½æ˜¯**å¾ˆç®€å•çš„äº‹å®**ã€‚è€Œäººç±»é€šå¸¸åœ¨æ„é€ é—®é¢˜ä¹‹å‰ä¼š**æ„é€ ä¸€ä¸ªæ•…äº‹**ï¼Œç±»ä¼¼äºå‘æ¨ç‰¹
  * è¿‡å»çš„VQGå‡å°‘äº†äº†å‚ä¸è€…çš„**å›ç­”æ„æ„¿**
  * **å•ä¸ªå›¾ç‰‡**ç¼ºä¹**äº‹ä»¶ä¿¡æ¯**ï¼Œç¼ºä¹å¯¹äº‹ä»¶ä¿¡æ¯**æ—¶åº**çš„ç†è§£

<img src="https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230127235522552.png" alt="image-20230127235522552" />

* datasetçš„ç‰¹ç‚¹
  * æ•…äº‹èƒŒååŒ…å«**æ•…äº‹ä¿¡æ¯**
  * äº”å¼ å›¾ç‰‡ + ä¸€ä¸ªæ•…äº‹æè¿°
  * æ•°æ®é›†æ ‡æ³¨æ–¹æ³•
    * å†™å‡ºtop5å¯¹è±¡å’Œäº‹ä»¶
    * ç»™å‡ºä¸€ä¸ªæè¿°
    * æƒ³æƒ³å¦‚æœæƒ³å°†**è¿™äº”å¼ å›¾ç‰‡å‘ä¸Šå»æ¨ç‰¹**ä¼šé—®ä»€ä¹ˆ
* æ›´å¤šCases

![image-20230128205258419](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230128205258419.png)



:hammer_and_wrench: **Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA**, in ACL 2023. [[pdf](https://arxiv.org/abs/2211.07516)] [[dataset&torch](https://github.com/esteng/ambiguous_vqa)]

* åŠ¨æœº
  * æ ‡æ³¨ä¸€ä¸ªæ–°çš„VQAæ•°æ®é›†ï¼ˆåªæœ‰éªŒè¯å’Œæµ‹è¯•æ—¶å€™çš„ï¼‰ï¼Œå› ä¸ºè¿™ä¸ªå·¥ä½œæ˜¯æƒ³ç”¨æ¥è¯„ä¼°å½“ä¸‹VQGæ¨¡å‹çš„æ€§èƒ½
  * ç¼“è§£ä¸€ä¸‹VQGä»»åŠ¡å½“ä¸­ä¸€äº›å¾ˆambiguityçš„é—®é¢˜
    * è§†è§‰ä¿¡æ¯æ²¡å‡ºç°
    * é—®æ³•ä¸æ˜ç¡®

| ![image-20230621151706413](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230621151706413.png) | ![image-20230621151725513](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230621151725513.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |



* æ–¹æ³•æ¨¡å‹
  * BaselineéªŒè¯ä¸€ä¸‹è€Œå·²
  * æœªæ¥å¯ä»¥ç”¨æ¥åš**VQGæ”¹å†™**çš„ä»»åŠ¡



:fire: :hammer_and_wrench: **Location-Aware Visual Question Generation with Lightweight Models**, in EMNLP 2023. [[pdf](https://arxiv.org/pdf/2310.15129v1.pdf)] [[torch](https://github.com/academiasinicanlplab/locavqg)]

> å¾ˆæçš„ä¸€ä¸ªåŠ¨æœº

* åŠ¨æœº
  * é¿å…å¸æœºçš„ç–²åŠ³é©¾é©¶ï¼Œè®©ç§»åŠ¨è®¾å¤‡æ ¹æ®å½“å‰é©¾é©¶ç¯å¢ƒç»™å¸æœºæé—®

![image-20231105103551827](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231105103551827.png)

* æ•°æ®é›†æ„å»ºè¿‡ç¨‹ï¼ˆåˆ©ç”¨GPT-4ï¼‰

![image-20231105103601176](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231105103601176.png)

---



### :video_camera: Video QG

**Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning**, in ICASSP 2020. [[pdf](https://ieeexplore.ieee.org/document/9053476)]

* ä½¿ç”¨äº†**[TVQA](https://paperswithcode.com/dataset/tvqa)**æ•°æ®é›†ï¼Œis based on 6 popular TV shows and consists of **152,545 QA pairs** from **21,793 clips**.
* æ€»ä½“æ²¡ä»€ä¹ˆåˆ›æ–°çš„

**Multi-Turn Video Question Generation via Reinforced Multi-Choice Attention Network**, in T-CSVT 2021.[[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9161024)]

* Multi-Turnï¼ˆM-VQGï¼‰ï¼šç»“åˆå¤šè½®å¯¹è¯+è§†é¢‘ä¿¡æ¯
* ä¼˜ç‚¹ï¼š åˆ©ç”¨åŠ¨æ€åœºæ™¯ä¿¡æ¯ï¼Œé—®é¢˜å¯å›ç­”æ€§ï¼Œå¯¹è¯è®°å½•ä¿¡æ¯æŠ½å–
* æ–¹æ³•ï¼šbaselineæ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆçœ‹ä¸æ‡‚ï¼‰

**End-to-End Video Question-Answer Generation with Generator-Pretester Network**, in T-CSVT 2021. [[pdf](https://arxiv.org/pdf/2101.01447.pdf)]

* å¼•å…¥ä¸€é—®ä¸€ç­”çš„å½¢å¼ï¼Œç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œç„¶åæµ‹è¯•ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
* ç¡¬ä»¶å¹³å°ï¼šNVIDIA DGX-1ï¼ˆ8 * V100ï¼‰

:hammer_and_wrench: **[2023.3æœªå¼€æº] Learning to Retrieve Videos by Asking Questions**, in MM 2022. [[pdf](https://arxiv.org/abs/2205.05739)] [[torch](https://github.com/avinashsai/ViRED)]

* åŠ¨æœº

  * ç¼©å°æ£€ç´¢çš„èŒƒå›´ï¼Œæ›´åŠ interactive

  ![image-20230317100606225](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230317100606225.png)



* å’Œè¿‡å»æ–¹æ³•çš„ä¸åŒ

![image-20230317100636723](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230317100636723.png)

* æ¨¡å‹æ–¹æ³•

![image-20230317100707179](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230317100707179.png)





### :city_sunset: VQA

:hammer_and_wrench: **MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering**, in CVPR 2022.  [[pdf](https://arxiv.org/pdf/2203.09138.pdf)] [[torch](https://github.com/AndersonStra/MuKEA.)]

* åŠ¨æœº

  * è¿‡å»åŸºäºçŸ¥è¯†çš„ï¼Œéƒ½åªæ˜¯è€ƒè™‘äº†æ–‡æœ¬ä¸Šçš„çŸ¥è¯†ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€çŸ¥è¯†çš„ç†è§£

  ![image-20220901165323764](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901165323764.png)

* ä¸»è¦è´¡çŒ®

  * ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€çŸ¥è¯†è¡¨ç¤º $(Entity, relation, answer)$
  * **pre-training and fine-tuning** strategy to accumulate both **out-domain and in-domain** knowledge

![image-20220901165520112](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901165520112.png)

* ç»†èŠ‚

  * ä¸‰ä¸ª**æŸå¤±å‡½æ•°**çš„è®¾è®¡

    * `Triplet TransE Loss`: ä¿æŒembeddingçš„ç»“æ„ï¼ˆé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼‰

    $$
    \mathcal{L}_{\text {TransE }}=\sum_{t^{+} \in \mathcal{A}^{+}} \sum_{t^{-} \in \mathcal{A}^{-}}\left[\gamma+\mathrm{d}\left(h+\boldsymbol{h}, \boldsymbol{t}^{+}\right)-\mathrm{d}\left(\boldsymbol{h}+\boldsymbol{r}, \boldsymbol{t}^{-}\right)\right]_{+}
    $$

    * `Triplet Consistency Loss`ï¼š ä¿è¯ä¸¥æ ¼çš„**æ‹“æ‰‘å…³ç³»**

    $$
    \mathcal{L}_{\mathrm{Tri}}=\operatorname{MSE}\left(h+r, t^{+}\right)
    $$

    * `Semantic Consistency Loss`: ä¿æŒåœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨è¾¾ä¸€è‡´æ€§

    $$
    {P\left(t^{+}\right)=\operatorname{softmax}\left((T)^{T}(h+r)\right)} \\{\mathcal{L}_{\mathrm{Sem}}=-\log \left(P\left(t^{+}\right)\right)}
    $$

    

  * é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥

    * å…ˆåœ¨`VQA 2.0`æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¥æ”¶é›†è§†è§‰ä¸»å¯¼çš„çŸ¥è¯†
    * åœ¨`KB-VQA`æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ

  * å…³äºå°¾éƒ¨`Entity`

    * è®­ç»ƒçš„æ—¶å€™ç›´æ¥åš`teacher-forcing`
    * æ¨ç†çš„æ—¶å€™è®¡ç®—$\mathbf{h}_{inf}+\mathbf{r}_{inf}$ ä¸ `look up` table $\mathbf{T}$çš„æœ€å°è·ç¦»

    $$
    \boldsymbol{t}_{\inf f}=\underset{\boldsymbol{t}_i \in T}{\arg \min } \mathrm{d}\left(\boldsymbol{h}_{\text {inf } f}+\boldsymbol{r}_{\text {inf } f}, \boldsymbol{t}_{\mathrm{i}}\right)
    $$

    

![image-20220901170039718](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901170039718.png)

**[VCR] Explicit Cross-Modal Representation Learning for Visual Commonsense Reasoning**, in TMM 2022. [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9465732)

* åŠ¨æœºï¼šä¸ºäº†åŠ å¼º`VCR`ä»»åŠ¡çš„**reasoning**è¿‡ç¨‹ï¼Œä¸å†é‚£ä¹ˆéšå¼
* æ–¹æ³•

![image-20220923223408003](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220923223408003.png)

* ä¾‹å­

![image-20220923223502035](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220923223502035.png)



:hammer_and_wrench:  **Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations**, in ICML 2022. [[pdf](https://arxiv.org/abs/2106.13876)] [[code (not released in 2022/11/20)](https://github.com/majumderb/rexc)]

> æ³¨æ„è¿™ç¯‡è®ºæ–‡æ—©äºğŸ‘‡ä¸‹é¢çš„ä¸¤ç¯‡è®ºæ–‡ï¼Œæ‰€ä»¥ç»“æœä¸Šå’Œä¸‹é¢ä¸¤ç¯‡è®ºæ–‡æœ‰æ˜æ˜¾çš„å·®è·

* åŠ¨æœº

  * å€ŸåŠ©**å…³é”®ä¿¡æ¯æŠ½å–**è¿˜æœ‰**è§£é‡Šç”Ÿæˆ**çš„æ–¹æ³•æ¥é¢„æµ‹ç­”æ¡ˆä¼šæ›´å¥½

* æ–¹æ³•ï¼ˆè¿™ç¯‡è®ºæ–‡æ–¹æ³•ä¸Šæ–‡ç« æ²¡æœ‰åšå¾ˆè¯¦ç»†çš„ï¼Œæè¿°ï¼Œå…·ä½“ä»£ç ä¹Ÿæ²¡æœ‰å…¬å¼€ï¼Œæ‰€ä»¥æ— æ³•å¾ˆè¯¦ç»†çš„è§£æï¼‰

  ![image-20221120100005850](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221120100005850.png)

  * æŠ½å–å…³é”®å…ƒç´ 
    * ä½¿ç”¨`HardKuma`å©å’çš„æ–¹æ³•æ¥å­¦ä¹ latent selectors é€‰æ‹©åˆé€‚çš„å…ƒç´ 
  * çŸ¥è¯†æŠ½å–
  * çŸ¥è¯†é€‰æ‹©ï¼ˆæ–¹æ³•å’Œç¬¬ä¸€ç‚¹ä¸€è‡´ï¼‰
  * ç”Ÿæˆè§£é‡Š+é¢„æµ‹ç­”æ¡ˆï¼ˆ**å…ˆ**ç”Ÿæˆè§£é‡Š**å**é¢„æµ‹ç­”æ¡ˆï¼‰

  ![image-20221120100330138](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221120100330138.png)

  



:hammer_and_wrench: **NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks**, in CVPR 2022. [[pdf](https://arxiv.org/abs/2203.05081)] [[pdf](https://github.com/fawazsammani/nlxgpt)]

* åŠ¨æœº
  * ä¹‹å‰æ–¹æ³•åˆ†ä¸ºå…ˆç”Ÿæˆ`A`å†ç”Ÿæˆ`E`ä¸¤æ­¥èµ°, ç¼ºä¹å¯¹reasoningè¿‡ç¨‹çš„è€ƒè™‘
  * ç¼ºå°‘ä¸€äº›å¯¹Eå’ŒAç›¸å…³æ€§è¿˜æœ‰å¯¹biasç¨‹åº¦ä¼°è®¡çš„è¯„ä»·æŒ‡æ ‡
  * è¿‡å»å’Œæ–‡ç« æ–¹æ³•ä¸Šçš„å¯¹æ¯”

![image-20221001182330655](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182330655.png)

* æ–¹æ³•ï¼ˆåŠå…¶ç®€å•ï¼‰
  * GPT-2æ¨¡å‹æ˜¯åœ¨å¤§è§„æ¨¡çš„image-captionæ•°æ®é›†ä¸­è’¸é¦å‡ºæ¥çš„
  * Vision Encoder ä½¿ç”¨äº†`CLIP`

![image-20221001182426390](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182426390.png)

* æ–°çš„è‡ªåŠ¨åŒ–è¯„ä»·æŒ‡æ ‡

  * è¯„ä¼°`E`å’Œ`A`çš„ç›¸å…³æ€§

  ![image-20221001182614962](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182614962.png)

  * è¯„ä¼°æ¨¡å‹Biasç¨‹åº¦

  ![image-20221001182654034](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182654034.png)

  çº¢è‰²ä¸ºå°äº0çš„å€¼ï¼Œéƒ½ç½®ä¸º0ï¼Œç„¶åè®¡ç®—distanceçš„å¹³å‡å€¼ï¼Œ**å€¼è¶Šå°ï¼Œæ¨¡å‹çš„Biasè¶Šå°**

:hammer_and_wrench: **[Viisual Explanation]  Chunk-aware Alignment and Lexical Constraint for VisualEntailment with Natural Language Explanations**, in MM 2022. [[pdf](https://arxiv.org/abs/2207.11401)] [[Talk](https://www.youtube.com/watch?v=nAHIZOQSiXg)] [[torch](https://github.com/HITsz-TMG/ExplainableVisualEntailment)]

* æ˜¯ä¸Šé¢ä¸€ç¯‡å·¥ä½œçš„è¿›ä¸€æ­¥ç ”ç©¶
* ä»»åŠ¡ä»‹ç»
  * ç»™å®šä¸€ä¸ªå›¾ç‰‡æ–‡æœ¬å¯¹ ----> å…³ç³»ï¼ˆ**entailmentæˆ–è€…ç­”æ¡ˆ**ï¼‰+ è§£é‡Š

![image-20220930221347803](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220930221347803.png)

* åŠ¨æœº

  * ç¼ºå°‘å¯¹Chunkçº§åˆ«çš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯çš„ç†è§£
  * æ²¡æœ‰å†³ç­–çš„è¿‡ç¨‹ï¼Œåªæ˜¯ç®€å•çš„èåˆç‰¹å¾

* æ–¹æ³•ï¼ˆæ³¨æ„ï¼šä½¿ç”¨äº†**Oscar**è¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼‰

  ![image-20220930221438196](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220930221438196.png)

:hammer_and_wrench: **SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering**, in CVPR 2022. [[pdf](https://arxiv.org/abs/2204.02285)] [[torch](https://github.com/vipulgupta1011/swapmix/)]

* åŠ¨æœº

  * ä½œè€…ä»ä¸€ä¸ªæ–°çš„è§’åº¦æ¥ç ”ç©¶ VQA æ¨¡å‹çš„é²æ£’æ€§ï¼švisual contextã€‚å¹¶è¡¨ç¤ºï¼Œè¿™äº›æ¨¡å‹è¿‡åº¦ä¾èµ–visual contextï¼Œå³å›¾åƒä¸­ä¸ç›¸å…³çš„ç‰©ä½“ï¼Œæ¥è¿›è¡Œé¢„æµ‹ã€‚

  ![img](https://pic2.zhimg.com/80/v2-184656fad6e2bfbecb20726736d1283d_720w.webp)

* å‘ç°

  * è™½ç„¶ä¹‹å‰çš„å·¥ä½œä»**è¯­è¨€ä¸Šä¸‹æ–‡**çš„è§’åº¦ç ”ç©¶äº†VQAé²æ£’æ€§ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å¦ä¸€ä¸ªè§’åº¦ç ”ç©¶äº†VQAæ¨¡å‹çš„é²æ£’æ€§ï¼š**è§†è§‰ä¸Šä¸‹æ–‡**ã€‚
  * é€šè¿‡**æ‰°ä¹±ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡**ï¼Œè¶…è¿‡45%çš„é¢„æµ‹çš„æ­£ç¡®ç­”æ¡ˆéƒ½ä¼šè¢«æ”¹å˜ã€‚è¡¨æ˜VQAæ¨¡å‹é«˜åº¦ä¾èµ–å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ï¼Œå› æ­¤å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡å¹²æ‰°çš„å½±å“ã€‚
  * è¿‡åº¦ä¾èµ–ä¸Šä¸‹æ–‡åœ¨**å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè§†è§‰è¡¨å¾**çš„è´¨é‡
    * ä¸ªå®Œç¾çš„è§†è§‰æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ä¾èµ–è¦å°å¾—å¤šã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨**groundtruthå¯¹è±¡å’Œå±æ€§ç¼–ç **æ›¿æ¢è§†è§‰è¡¨å¾æ¥å®ç°è¿™ä¸€ç‚¹

* è´¡çŒ®

  * æ˜¯ç¬¬ä¸€ä¸ªä»è§†è§‰ä¸Šä¸‹æ–‡çš„è§’åº¦ç ”ç©¶VQAç¨³å¥æ€§çš„äººï¼Œé€šè¿‡ç®€å•çš„**ä¸Šä¸‹æ–‡æ‰°åŠ¨ç­–ç•¥SwapMix**ï¼Œå¯¹ä¸¤ä¸ªæœ‰ä»£è¡¨æ€§çš„VQAæ¨¡å‹çš„å¥å£®æ€§è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘ç°å®ƒä»¬è¿‡åº¦ä¾èµ–è§†è§‰ä¸Šä¸‹æ–‡ã€‚
  * å‘ç°ä¸€ä¸ªå®Œç¾çš„è§†è§‰æ¨¡å‹å¯¹è§†è§‰ä¸Šä¸‹æ–‡çš„ä¾èµ–è¦å°å¾—å¤šï¼Œä¸ºæ¨¡å‹æä¾›**å®Œç¾çš„è§†è§‰ç¼–ç **ï¼Œå¹¶è§‚å¯Ÿæ¨¡å‹ç¨³å¥æ€§çš„æ”¹å–„ã€‚
  * å®šä¹‰äº†**ä¸¤ä¸ªæŒ‡æ ‡**ï¼Œ**ä¸Šä¸‹æ–‡ä¾èµ–æ€§å’Œæœ‰æ•ˆå‡†ç¡®æ€§**ï¼Œå¹¶é€šè¿‡ä½¿ç”¨SwapMixä½œä¸ºæ•°æ®å¢å¼ºæŠ€æœ¯æ˜¾ç¤ºäº†æ”¹è¿›ã€‚

* æ–¹æ³•

  ![image-20221103155743460](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221103155743460.png)

  

**Weakly Supervised Relative Spatial Reasoning for Visual Question Answering**, in ICCV 2021. [[pdf](https://arxiv.org/abs/2109.01934)]

> [åšå®¢é“¾æ¥](https://www.cnblogs.com/lhiker/articles/15630482.html)

* åŠ¨æœº

  * è§†è§‰æ¨ç†çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯**ç©ºé—´ç†è§£**ï¼Œå®ƒæ¶‰åŠåˆ°ç†è§£å¯¹è±¡çš„ç›¸å¯¹ä½ç½®ï¼Œå³éšå¼åœ°å­¦ä¹ åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ã€‚
  * è¿‡å»çš„é¢„è®­ç»ƒå¤§æ¨¡å‹åœ¨æ¨ç†æ—¶å€™ï¼Œéƒ½æ— æ³•ç†è§£2Då›¾åƒå½“ä¸­çš„**ç©ºé—´ä¿¡æ¯**ã€‚
  * ä¸ºæ­¤è®¾è®¡äº†ä¸¤ä¸ªç›®æ ‡ä½œä¸ºç©ºé—´æ¨ç†ï¼ˆSRï¼‰çš„ä»£ç†
    * å¯¹è±¡è´¨å¿ƒä¼°è®¡
    * ç›¸å¯¹ä½ç½®ä¼°è®¡

  ![image-20221106145708208](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221106145708208.png)

* è´¡çŒ®

  * æå‡ºä¸¤ä¸ªå­ä»»åŠ¡ï¼Œç†è§£2Då›¾åƒå½“ä¸­çš„å‡ ä½•ä¿¡æ¯
  * å±•ç°äº†å¼ºå¤§çš„`zero-shot`èƒ½åŠ›ï¼Œåªéœ€è¦10%çš„è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ
  * OODï¼ˆOut of Distributionï¼‰çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¾ˆå¼º

* æ–¹æ³•

  * é¢„å¤„ç†å·¥ä½œ

    * æŠ½å–å›¾åƒçš„æ·±åº¦ï¼ˆé¢„è®­ç»ƒå¥½çš„`AdaBins`æ¨¡å‹ï¼‰

      * è´¨å¿ƒè¡¨ç¤º$(x_c, y_c, z_c),$ $z_cä»£è¡¨æ·±åº¦$
      * ç›¸å¯¹ä½ç½®è¡¨ç¤º: è´¨å¿ƒä¹‹é—´å‘é‡çš„å‡æ³•ï¼ŒåŒæ—¶$dist(Aï¼ŒB)=âˆ’dist(Bï¼ŒA)$

    * åœ¨ä¸Šè¿°ä¸¤ä¸ªä»»åŠ¡ä¸­ï¼Œé¢„æµ‹éƒ½æ˜¯**å®å€¼å‘é‡**ã€‚è¯„ä¼°äº†è¿™äº›ä»»åŠ¡çš„ä¸¤ä¸ªå˜ä½“ï¼š

      * ä¸€ä¸ª**å›å½’ä»»åŠ¡**ï¼Œå…¶ä¸­æ¨¡å‹é¢„æµ‹$\mathbb{R}^{3}_{[-1, 1]}$ä¸­çš„å®å€¼å‘é‡
      * **binåˆ†ç±»**ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ä¸‰ä¸ªç»´åº¦çš„å®å€¼èŒƒå›´åˆ’åˆ†ä¸ºCä¸ªlog-scale binsã€‚ç¬¬cä¸ªbinçš„binå®½ç”±ä¸‹å¼ï¼ˆä½¿ç”¨è¶…å‚æ•°$\lambda=1.5$)ç»™å‡ºï¼š

      $$
      b_c=\frac{1}{\lambda^{C-\left|c-\frac{C}{2}\right|+1}}-\frac{1}{\lambda^{C-\left|c-\frac{C}{2}\right|+2}} \forall c \in\{0 . . C-1\}
      $$

      * å¯¹æ•°å°ºåº¦çš„binså¯¹æ›´è¿‘çš„è·ç¦»æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡ï¼ˆæ›´å¤šçš„binsï¼‰ï¼Œå¯¹æ›´è¿œçš„è·ç¦»æœ‰æ›´ä½çš„åˆ†è¾¨ç‡ï¼ˆæ›´å°‘çš„binsï¼‰
      * æœ€ç®€å•çš„binåˆ†ç±»å½¢å¼æ˜¯ä¸€ä¸ªå…·æœ‰biné—´éš”çš„ä¸‰ç±»åˆ†ç±»ä»»åŠ¡$[âˆ’1,0)ã€[0]ã€(0,1]$

  * å¼±ç›‘ç£ä»£ç†ä»»åŠ¡ $SR$

    * è´¨å¿ƒé¢„æµ‹: $\mathcal{L}_{S R-r e g}=\mathcal{L}_{M S E}\left(f_{r e g}(v), y_{r e g}\right)$
      * å°†è§†è§‰ä¿¡æ¯å‹ç¼©ä¸º$36 \times 3$æ¥å’Œ`gt`(é¢„å¤„ç†å¾—åˆ°çš„) è¿›è¡Œè®¡ç®—
    * ç›¸å¯¹ä½ç½®è¯„ä¼°: $\mathcal{L}_{S R \text {-bin }}=\mathcal{L}_{C E}\left(f_{\text {bin }}(V), y_{b i n}\right)$
      * è®­ç»ƒä¸€ä¸ªä¸¤å±‚å‰é¦ˆç½‘ç»œ$f_bin$æ¥é¢„æµ‹æ¯ä¸ªç»´åº¦ä¸Šæ¯ä¸ªå¯¹è±¡çš„$36\times C \times D$ä¸ª$bin$ç±»ï¼Œå…¶ä¸­$C$æ˜¯ç±»çš„æ•°é‡ï¼Œ$D$ä¸º3

  * Patchesè§†è§‰ä¿¡æ¯

    * èƒ½æ›´å¥½åˆ©ç”¨ç©ºé—´ä¿¡æ¯(è¿™é‡Œè¯´çš„æ˜¯å¹³é¢ç©ºé—´ä¿¡æ¯ï¼Œæœ‰ä½ç½®ç¼–ç )

    ![image-20221106152759382](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221106152759382.png)

:fire: :hammer_and_wrench: **An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2109.05014)] [[torch](https://github.com/microsoft/PICa)] [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/433110834)]

![image-20221108163040036](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221108163040036.png)

* æ¢ç´¢`PLM`å¯¹çŸ¥è¯†çš„ç†è§£èƒ½åŠ›åšVQAï¼Œä¸éœ€è¦æ ¹æ®çŸ¥è¯†åº“è¿›è¡Œæ£€ç´¢
* `few-shot`çš„å½¢å¼



:fire: **CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment**, in ACL 2022. [[pdf](https://arxiv.org/abs/2203.07190)]

> å®éªŒæ€§çš„æ–‡ç« 

* åŠ¨æœº

  * ä¸ºäº†éªŒè¯CLIPåšVQAä»»åŠ¡è¿˜æœ‰Visual Entailmentä»»åŠ¡çš„zero-shotæ€§èƒ½
  * æå‡ºVQAä¸­few-shotä¸‹çš„å¾®è°ƒç­–ç•¥

* æ–¹æ³•

  * Zero-shot VQA

  ![image-20221204155946020](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221204155946020.png)

  * Zero-shot Visual Entailment

  ![image-20221204160041611](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221204160041611.png)

  * **Few-shot VQA**
    * åˆ†æˆ`question type` $\times$ `answer type` è·¯ï¼ˆå¦‚æœæŒ‰ç…§è¯æ±‡è¡¨æ¥åˆ†å°±å¤ªå¤šäº†ï¼‰, $65 \times 3 = 195$ ways
    * å¾®è°ƒç»†èŠ‚è¯¦è§è®ºæ–‡ï¼ˆå¾®è°ƒ`CLIP`æ¨¡å‹çš„ä¸€éƒ¨åˆ†å‚æ•°ï¼‰

:fire: :hammer_and_wrench: **A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge**, in ECCV 2022. [[home page](https://allenai.org/project/a-okvqa/home)]

> OK-VQAçš„å‡çº§ç‰ˆæœ¬

* åŠ¨æœº

  * è¿‡å¾€çš„æ•°æ®é›†é¡¶å¤šå°±æ˜¯æ£€ç´¢æ•°æ®åº“é‡Œé¢çš„çŸ¥è¯†ï¼Œæ²¡æœ‰åšåˆ°å¯¹å›¾ç‰‡å¸¸è¯†çš„æ¨ç†ï¼ˆæ›´åŠ ä¸°å¯Œçš„çŸ¥è¯†ï¼‰
  * æˆ‘ä»¬å®é™…ä¸Šéœ€è¦ï¼Œè¯†åˆ«å›¾ç‰‡ï¼Œç†è§£é—®é¢˜ï¼Œæ‰¾åˆ°çŸ¥è¯†ï¼Œ**æ¨ç†**å‡ºå¯¹åº”çš„ç­”æ¡ˆ
  * è¿‡å»çš„æ•°æ®é›†
    * FVQAï¼šç¼ºä¹æ¨ç†çš„è¿‡ç¨‹ï¼Œè€Œä¸”å’Œå›¾ç‰‡ä¸æ€ä¹ˆç›¸å…³
    * KVQAï¼šé€šå¸¸æ˜¯å®ä½“çš„çŸ¥è¯†ï¼Œåœ¨ç»´åŸºç™¾ç§‘ä¸Šé¢çš„çŸ¥è¯†ï¼Œè€Œä¸”ä¸»è¦æ˜¯é—®ä»»åŠ¡æ–¹é¢çš„ï¼Œæ²¡æœ‰å¸¸è¯†çš„çŸ¥è¯†
    * OK- VQAï¼šæœ‰biasï¼ŒçŸ¥è¯†å¤ªè¿‡ç®€å•ï¼ˆe.g., What is the capital of this country?ï¼‰, è€Œä¸”**ç¼ºä¹æ¨ç†**
    * VCRï¼šé‡ç‚¹æ˜¯ç”µå½±åœºæ™¯ä¸­äººçš„æ„å›¾

* æ•°æ®é›†çŸ¥è¯†ç±»å‹

  ![image-20221110165948489](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221110165948489.png)

  * **Commonsense** - learn from social behavior, æœ‰ç‚¹**æ¨ç†**çš„æ„æ€ (e.g., many donuts being made in a cart implies they are for sale rather than for personal consumption).
  * **Visual** - Knowledge of concepts represented visuallyï¼Œ å’Œ**å›¾ç‰‡ç›¸å…³** (e.g., muted color pallets are associated with the 1950s).
  * **Knowledge bases** - **æ£€ç´¢çŸ¥è¯†åº“çš„çŸ¥è¯†**, Knowledge obtained from textbooks, Wikipedia and other textual sources (e.g., hot dogs were invented in Austria).
  * **Physical** - **æ—¥å¸¸ç”Ÿæ´»çš„çŸ¥è¯†ï¼ŒFact**, Knowledge about the physics of the world (e.g., shaded areas have a lower temperature than other areas)

:hammer_and_wrench: **[VCR] Heterogeneous Graph Learning for Visual Commonsense Reasoning**, in NIPS 2019. [[pdf](https://arxiv.org/abs/1910.11475)] [[torch](https://github.com/yuweijiang/HGL-pytorch)]

* ä¸ä¼ ç»Ÿçš„`VQA`ä¸å¤ªä¸€æ ·ï¼ŒR: è§£é‡Šï¼ˆReasonï¼‰
  * ä¸‰ä¸ªå­ä»»åŠ¡åˆ†åˆ«æ˜¯: $Q \rightarrow A$, $QA \rightarrow R$, $Q \rightarrow AR$
* æ–¹æ³•ï¼šæ„å»ºå¼‚æ„å›¾

![image-20220921151513456](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921151513456.png)

![image-20220921151544188](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921151544188.png)



:hammer_and_wrench: **[VCR] Connective Cognition Network for Directional Visual Commonsense Reasoning**ï¼Œin NIPS 2019.  [[pdf](https://proceedings.neurips.cc/paper/2019/file/8a56257ea05c74018291954fc56fc448-Paper.pdf)] [[torch](https://github.com/AmingWu/CCN)]

* ä¸ä¸Šä¸€ç¯‡è®ºæ–‡æ€æƒ³æ¯”è¾ƒç±»ä¼¼ï¼Œå‚è€ƒç¥ç»ç§‘å­¦å½“ä¸­å°†ç¥ç»å…ƒæ•´åˆèµ·æ¥çš„æ€æƒ³

* åšæ³•

![image-20220921171105320](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171105320.png)

* æ¨¡å‹ç›¸å…³ç»†èŠ‚

![image-20220921171210400](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171210400.png)

* ç¬¬ä¸€partä¸­**è¿æ¥**çš„æ„å»º

![image-20220921171342266](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171342266.png)



:fire: **[Knowledge-Based] KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2012.11014)] [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/392431083)]

* åŠ¨æœº

  * éªŒè¯åˆ©ç”¨**å¤–éƒ¨çŸ¥è¯†+éšå¼çŸ¥è¯†**ç»“åˆçš„åš`QA`çš„èƒ½åŠ›

  * **éšå¼çŸ¥è¯†**å¯ä»¥ä»åŸºäºå¤§è§„æ¨¡è¯­æ–™é¢„è®­ç»ƒçš„æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ã€‚

  * è€Œ**æ˜¾ç¤ºçš„çŸ¥è¯†**å¯ä»¥ä»çŸ¥è¯†åº“ä¸­çš„æ˜ç¡®çš„ã€ç¬¦å·åŒ–çš„çŸ¥è¯†ä¸­å­¦ä¹ ã€‚

  * å°†ä¸¤ç§æ¨¡å‹è¿›è¡Œé›†æˆï¼Œå³å¯åŒæ—¶ç»“åˆéšå¼çŸ¥è¯†ä¸æ˜¾å¼çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚

    ![image-20221112091608964](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221112091608964.png)

* æ–¹æ³•

  * æ„å»ºå¤–éƒ¨çŸ¥è¯†åº“ï¼Œç”±äºåŸæ¥çš„çŸ¥è¯†å¤ªå¤šäº†ï¼Œæ ¹æ®è§„åˆ™å¯¹çŸ¥è¯†è¿›è¡Œä¸€å®šçš„ç­›é€‰

    * DBPedia, ConceptNet , VisualGenome and hasPart KB ï¼Œè¿›è¡ŒèŠ‚ç‚¹è¿‡æ»¤ï¼Œåªä¿ç•™åŒ…å«å›¾åƒç›®æ ‡æ£€æµ‹labelçš„èŠ‚ç‚¹ï¼Œæœ€åçš„çŸ¥è¯†å›¾åŒ…å«8000èŠ‚ç‚¹å’Œ36000æ¡è¾¹ï¼Œé‡‡ç”¨RGCNä½œä¸ºå·ç§¯æ¨¡å‹

      ![img](https://pic4.zhimg.com/80/v2-4299f57ff66a828d0cf5e5eb869e5f17_1440w.webp)

  * æ¨¡å‹è®¾è®¡ï¼ˆå¾ˆç®€å•ï¼‰

  ![æˆªå±2022-11-12 09.18.44](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-11-12%2009.18.44.png)



:hammer_and_wrench: :fire: **[å¤šæ¨¡æ€ç‰ˆæœ¬COT]** **Learn to Explain: Multimodal Reasoning via *Thought Chains for Science Question Answering**, in NIPS 2022. [[pdf](https://arxiv.org/abs/2209.09513)] [[åšå®¢](https://blog.csdn.net/EasyAIForum/article/details/127627611)] [[data&code](https://scienceqa.github.io.)]

> æ‰‹é€Ÿå®åœ¨å¤ªå¿«äº†

* åŠ¨æœº

  * AIåšé—®é¢˜å›ç­”çš„æ—¶å€™åº”è¯¥å…·æœ‰ç±»ä¼¼äººçš„æ€è€ƒæ–¹å¼ï¼Œå½“ä¸‹çš„å¤§è¯­è¨€æ¨¡å‹æ›´å¤šæ˜¯é»‘ç›’
    * é“¾å¼æ€ç»´æ–¹å¼
    * few-shotçš„è¿ç§»èƒ½åŠ›

* è´¡çŒ®

  * æå‡ºä¸€ä¸ªå¤šæ¨¡æ€çš„åŒ…å«å¤šæ­¥è§£é‡Šçš„æ•°æ®é›†ï¼ˆç§‘å­¦é—®é¢˜ï¼Œ21,208ä¸ªä¾‹å­ï¼‰
  * COTçš„æ–¹å¼ç”Ÿæˆè§£é‡Š
  * ç»“åˆè¯­è¨€æ¨¡å‹æ¢ç´¢GPT+COTçš„ä¸Šé™

  ![image-20221122112213974](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221122112213974.png)

* æ•°æ®é›†ç‰¹ç‚¹

![image-20221122112301059](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221122112301059.png)

![image-20221122112322813](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221122112322813.png)

* æ–¹æ³•

  * å›¾åƒå¤„ç†å°±ç®€å•çš„è½¬æˆCaptionï¼Œä½œä¸ºvisual context

  * ä½¿ç”¨COTæ”¹è¿›UnifiedQA (å¾®è°ƒ)

    * åŸæ¥ï¼š->A
    * æ”¹è¿›å: -> ALE (Answer, Lecture, Explanation) 

  * ä½¿ç”¨GPT-3+COT

    * è®¾è®¡Promptï¼ˆfew-shotçš„æ–¹å¼ï¼‰

    ![image-20221122112544202](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221122112544202.png)

    

* å®éªŒç»“æœï¼ˆå…·ä½“ç»“æœå¯ä»¥çœ‹è®ºæ–‡æˆ–è€…åšå®¢ï¼‰

  * è§£é‡Šç”Ÿæˆçš„æ•ˆæœï¼ˆæ˜¯ä¸æ˜¯å†™é”™äº†ï¼Ÿï¼Ÿï¼Ÿï¼‰

  ![image-20221122112645359](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221122112645359.png)

  * æ¨¡å‹ä¸Šé™æ¢ç´¢+ä¸åŒçš„ALEä½ç½®æ¢ç´¢

  ![image-20221122112715645](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221122112715645.png)



:hammer_and_wrench: **A Unified End-to-End Retriever-Reader Framework for Knowledge-based VQA**, in MM 2022. [[pdf]()] [[torch](https://github.com/guoyang9/UnifER)]

> è¿™ç¯‡è®ºæ–‡å®éªŒå‘³é“æ¯”è¾ƒé‡ï¼ŒæŒº**ä¸¥è°¨**çš„ä¸€ä¸ªå·¥ä½œ

* åŠ¨æœº
  * è¿‡å»çš„VQAæ¨¡å‹ä¸»è¦æ˜¯é›†ä¸­åœ¨åˆ©ç”¨**æ˜¾ç¤ºçŸ¥è¯†**è¿˜æœ‰**éšå¼çŸ¥è¯†**ä¸Šé¢
    * è¿‡å»çš„æ˜¾ç¤ºçŸ¥è¯†å¾€å¾€å­˜åœ¨noisyï¼Œå› ä¸ºæ²¡æœ‰å¯¹åº”çš„æ ‡ç­¾æ¥çº¦æŸ
    * éšå¼çŸ¥è¯†åˆå¤§éƒ¨åˆ†æ˜¯åˆ©ç”¨äº†é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œè€Œæ²¡æœ‰äººæ¢ç©¶**å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹**æ˜¯å¦ä¹Ÿæœ‰éšå¼çŸ¥è¯†
  * æ¨¡å‹ä¸Šï¼Œä¼ ç»Ÿçš„éƒ½æ˜¯æ£€ç´¢çŸ¥è¯†ï¼Œå†åšpredict answerã€‚æ˜¯ä¸¤ä¸ªå•ç‹¬çš„æ¨¡å—
    * æœ¬æ–‡å°†ä¸¤ä¸ªæ¨¡å—è”ç³»èµ·æ¥ï¼Œç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å—è§£å†³

* æ–¹æ³•

![image-20230128223057226](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230128223057226.png)

![image-20230128223221039](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230128223221039.png)

* **æ¨ç†**æ—¶å€™
  * ç›´æ¥**æ£€ç´¢å‡ºæ¥æœ€å¥½çš„é‚£ä¸ªçŸ¥è¯†**ï¼Œç„¶åå»å›ç­”å³å¯

* ç»“è®º
  * é¢„è®­ç»ƒæ¨¡å‹å½“ä¸­çš„**éšå¼çŸ¥è¯†æ›´åŠ é‡è¦**
  * å…·ä½“è§**è®ºæ–‡çš„å®éªŒ** 

:hammer_and_wrench: **A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.04530)] [[torch](https://github.com/YunxinLi/Multimodal-Context-Reasoning)]

* åŠ¨æœº

  * è¿‡å»è¿™ä¸ªå¤šæ¨¡æ€æ¨ç†çš„ä»»åŠ¡éƒ½æ˜¯ç”¨VLMçš„æ–¹å¼ï¼Œå¾ˆå°‘è€ƒè™‘åˆ°multimodal contextæ¨ç†çš„èƒ½åŠ›ï¼ˆ**ç¼ºå°‘è¿™ç§pre-contextçš„æœºåˆ¶æ¥æ¿€æ´»æ¨¡å‹**ï¼‰
  * PMRæ•°æ®é›†ä¸­çš„ä¾‹å­

  ![image-20230510161851173](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230510161851173.png)

* è´¡çŒ®
  * å¤šæ¨¡æ€çš„in-contextèƒ½åŠ›
  * æ¿€å‘P LMåšå¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›
* æ–¹æ³•

![image-20230510162012719](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230510162012719.png)

:fire: :hammer_and_wrench: **Combo of Thinking and Observing for Outside-Knowledge VQA**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.06407)] [[torch](https://github.com/PhoebusSi/Thinking-while-Observing)]

* åŠ¨æœº
  * è¿‡å»çš„çŸ¥è¯†æ€§VQAè¦ä¹ˆå¿½è§†äº†textual knowledge in natural-language space
  * è¦ä¹ˆå®Œå…¨æ”¾å¼ƒäº†è§†è§‰ä¿¡æ¯
  * è¿™é‡Œå¸Œæœ›ä¸¤è€…ç»“åˆèµ·æ¥ï¼Œçº¦æŸå¤šæ¨¡æ€ç©ºé—´intoè¯­è¨€ç©ºé—´å½“ä¸­

![image-20230516111041254](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516111041254.png)

* è¿‡å»å·¥ä½œçš„æ±‡æ€»

![image-20230516111123423](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516111123423.png)

* æ–¹æ³•ï¼šè¶…çº§æ— æ•Œå¤§æ‚çƒ©

![image-20230516111152375](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516111152375.png)



:hammer_and_wrench: **[I-T Retrieval] CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not**, in CVPR 2023. [[pdf](https://arxiv.org/abs/2303.13440)] [[torch](https://aneeshan95.github.io/Sketch_LVM/)]

* åŠ¨æœº
  * ç¬¬ä¸€æ¬¡ZSçš„ Sketch-based çš„å›¾ç‰‡æ£€ç´¢
  * æ–¹æ³•ä¸Šè®¾è®¡äº†ä¸€äº›ç‹¬ç‰¹çš„lossï¼ˆå¾ˆä½èµ„æºçš„æ˜¾å¡éƒ½å¯ä»¥è·‘èµ·æ¥ï¼‰

![image-20230724101958886](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230724101958886.png)

* æ–¹æ³•
  * æ•´ä½“æ¡†æ¶

![image-20230724102027823](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230724102027823.png)

* é¢†åŸŸè¿ç§»çš„loss

![image-20230724102052645](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230724102052645.png)



:hammer_and_wrench: **[T-I Retrieval] Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval**, in CVPR 2023. [[pdf](https://arxiv.org/abs/2302.03084)] [[torch](https://github.com/google-research/composed_image_retrieval)]

* åŠ¨æœº
  * Zero-shotçš„å¼€æ”¾ä¸–ç•Œå›¾æ–‡æ£€ç´¢

![image-20230724103100073](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230724103100073.png)

* æ–¹æ³•

![image-20230724103119337](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230724103119337.png)



:hammer_and_wrench: **Symbolic Replay: Scene Graph as Prompt for Continual Learning on VQA Task**, in AAAI 2023. [[pdf](https://arxiv.org/abs/2208.12037)] [[Dataset&torch](https://github.com/showlab/CLVQA)]

* åŠ¨æœº
  * å’ŒCVPR 2023çš„é‚£ä¸€ç¯‡ä¸€æ ·ï¼Œéƒ½æ˜¯VQAè¿ç»­å­¦ä¹ çš„
  * åšæ³•ä¸Šä¸ºäº†ä¿è¯æ•°æ®éšç§ï¼Œmemoryè¦ç”¨ç”Ÿæˆçš„å†…å®¹
  * æå‡ºbenchmarkä¸»è¦åˆ†ä¸ºåœºæ™¯å›¾ï¼ˆ6ç±»åœºæ™¯ï¼‰ + åŠŸèƒ½ï¼ˆ6ç±»åŠŸèƒ½ï¼‰

![image-20230725103249913](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725103249913.png)

![image-20230725103403996](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725103403996.png)

* æ–¹æ³•ï¼ˆä¸ªäººè¿˜æ˜¯è§‰å¾—CVPR 2023çš„é‚£ä¸ªåšæ³•å¥½ä¸€äº›ï¼‰

![image-20230725103420284](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725103420284.png)

![image-20230725103514264](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725103514264.png)



* å®éªŒï¼ˆä¸åŒçš„Task Orderï¼‰

![image-20230725103637538](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725103637538.png)

:hammer_and_wrench: :fire: **PROMPTCAP: Prompt-Guided Task-Aware Image Captioning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2211.09699)] [[torch](https://yushi-hu.github.io/promptcap_demo/)]

> è¿™ç¯‡è®ºæ–‡æˆ‘å…³æ³¨å¾ˆä¹…äº†ï¼Œä¼°è®¡æ˜¯æŠ•CVPRæ²¡ä¸­ï¼Œä¸­äº†ICCV

* åŠ¨æœº
  * å¦‚ä½•ç”¨ChatGPTè¾…åŠ©çŸ¥è¯†æ€§è§†è§‰é—®ç­”

![image-20230829205901778](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829205901778.png)

* è®­ç»ƒæ–¹æ³•

![image-20230829210040620](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829210040620.png)

* Promptçš„è®¾è®¡

![image-20230829210305089](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829210305089.png)

* æ¨¡å‹æ¨ç†è¿‡ç¨‹

![image-20230829210335689](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829210335689.png)

* å®éªŒç»“æœ

![image-20230829210409210](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829210409210.png)



:hammer_and_wrench: **SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images**, in AAAI 2023. [[pdf](https://arxiv.org/abs/2301.04883)] [[torch](https://github. com/nttmdlab-nlp/SlideVQA)]

* åŠ¨æœº
  * PPT VQAæ•°æ®é›†ï¼šå¾ˆå¥½çš„è§£å†³äº†ä¸ºä»€è¦è¿›è¡Œå¤šå›¾åƒçš„QAæˆ–è€…QGçš„åŠ¨æœº
  * è¿‡å»çš„æ–‡æ¡£VQAå¾€å¾€ä¹Ÿåªæ˜¯å…³æ³¨ä¸€å¼ å›¾ç‰‡
  * ç›®å‰æ¨¡å‹çš„ç»“æœå’Œäººç±»çš„ç»“æœç›¸æ¯”ç›¸å·®è¿˜éå¸¸å¤§
* æ•°æ®é›†æ ·ä¾‹
  * å•è·³ï¼Œå¤šè·³ï¼Œæ•°æ®æ¨åˆ°çš„é—®é¢˜

![image-20230912151704417](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230912151704417.png)

* æ¨¡å‹æ–¹æ³•

![image-20230912151818638](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230912151818638.png)



:hammer_and_wrench: :fire: **Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2306.09224)] [[dataset](https://github.com/google-research/google-research/tree/master/encyclopedic_vqa)]

* åŠ¨æœºï¼šæå‡ºä¸€ä¸ªå¼€æ”¾ä¸–ç•Œè¶…çº§éš¾çš„VQAæ•°æ®é›†

![image-20231018210801188](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018210801188.png)

:hammer_and_wrench: :fire: **Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts**, in MM 2023. [[pdf](https://dl.acm.org/doi/10.1145/3581783.3612389)] [[torch](https://github.com/ECNU-DASE-NLP/RQP)]

* åŠ¨æœº
  * LLMåšVQAï¼Œæå‡ºåº”è¯¥æŠŠé—®é¢˜åˆ†å¼€æ¥

![image-20231114211254215](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114211254215.png)

* æ–¹æ³•

![image-20231114211314688](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114211314688.png)



### :sunny: Textual-QA

:fire:  :hammer_and_wrench: **[Question Answering] Commonsense for Generative Multi-Hop Question Answering Tasks**, in EMNLP 2018. [[pdf]](https://arxiv.org/abs/1809.06309) [[tensorflow]](https://github.com/yicheng-w/CommonSenseMultiHopQA)

:hammer_and_wrench: **[Dialogue System] Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering**, in AAAI 2020. [[pdf]](https://arxiv.org/abs/1912.07491) [[torch]](https://github.com/siat-nlp/TransDG)

**[Question Answering] Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs**, in EMNLP 2019. [[pdf\]](https://arxiv.org/abs/1910.08435)

:fire: :hammer_and_wrench: **[Question Answering] ** **Improving Multi-hop Question Answering over Knowledge Graphs usingKnowledge Base Embeddings**, in ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.412/)] [[torch](https://github.com/malllabiisc/EmbedKGQA)]

:hammer_and_wrench: **Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Urooj_Found_a_Reason_for_me_Weakly-supervised_Grounded_Visual_Question_Answering_CVPR_2021_paper.pdf)] [[torch](https://github.com/aurooj/ WeakGroundedVQA_Capsules.git)]

* ä¸ç”¨faster-rcnn
* è®­ç»ƒè¾“å…¥æ˜¯é—®é¢˜å’Œç­”æ¡ˆï¼Œè¾“å‡ºæ˜¯é¢„æµ‹ç­”æ¡ˆå¯¹åº”çš„**grouding area**ã€‚

**KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.422.pdf)] [[project](https://github.com/shijx12/ KQAPro_Baselines)]

* æ›´åŠ å¤æ‚çš„æ•°æ®é‡æ›´å¤§çš„å¼•å…¥çŸ¥è¯†çš„æ•°æ®é›†
  * å¹¶ä¸”ç»™å‡ºäº†ä¸¤ç§reasoningçš„è¿‡ç¨‹
  * å¯ä»¥åšQAå’Œ**è¯­ä¹‰è§£æ**æœåŠ¡
  * åˆ©ç”¨æ›´åŠ å¤æ‚çš„æ¨¡ç‰ˆå’ŒçŸ¥è¯†ç”Ÿæˆé—®é¢˜

![image-20220912160811327](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912160811327.png)

* [è¯¦ç»†ä»‹ç»çš„blog](https://blog.csdn.net/weixin_47903246/article/details/124649493)

**[è‡ªç„¶è¯­è¨€æ¨ç†] Generated Knowledge Prompting for Commonsense Reasoning**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.225.pdf)] [[torch](https://github.com/liujch1998/GKP)

> è¿™ç¯‡è®ºæ–‡çš„æ€æƒ³å’Œ`few-shot` COTå¾ˆåƒ 

* åŠ¨æœº

  * æ¢ç´¢LMå¯¹çŸ¥è¯†çš„ç†è§£èƒ½åŠ›æ¥å›ç­”é€»è¾‘æ¨ç†çš„é—®é¢˜
  * **ä¸å†éœ€è¦**å¤–éƒ¨çŸ¥è¯†åº“è¿›è¡Œæ£€ç´¢ï¼Œæˆ–è€…æ ¹æ®ç‰¹å®šä»»åŠ¡çš„ç›‘ç£ä¿¡å·è¿›è¡ŒçŸ¥è¯†çš„èåˆ

* æ–¹æ³•

  * è®©LMåœ¨few-shotæƒ…å†µä¸‹æ¥ç”Ÿæˆè§£é‡Šçš„`prompt`

    * é¦–å…ˆæ ¹æ®ä»»åŠ¡è§„å®šä¸€ä¸‹promptæ¨¡ç‰ˆï¼Œæ¯ä¸ªä»»åŠ¡äº”ä¸ªæ¨¡æ¿ï¼ˆfew-shotï¼‰

    ![image-20221106225100256](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221106225100256.png)

    * ç»§ç»­è¾“å…¥é—®é¢˜åˆ°é‚£ä¸ªå ä½ç¬¦å½“ä¸­ï¼ŒLMè‡ªåŠ¨ç”ŸæˆçŸ¥è¯†ï¼Œæ‰”å›å»ä½œä¸ºæ ·æœ¬é›†åˆ$K_q=\{k_1, k2, ..., k_M\}$

  * çŸ¥è¯†èåˆ

    * é—®é¢˜å’Œæ¯ä¸ªæ ·æœ¬é›†åˆä¸­çš„çŸ¥è¯†$k$è¿›è¡Œèåˆ, `concat`æ“ä½œ

    $$
    q_0=q, q_1=\left[k_1 \| q\right], \ldots, q_M=\left[k_M \| q\right]
    $$

  * æœ€åé€‰æ‹©æœ€åˆé€‚çš„å¯¹ï¼Œæ‰”è¿›å»ä¸‹ä¸€ä¸ªLMè¿›è¡Œæ¨ç†ï¼Œæœ€åç”Ÿæˆç­”æ¡ˆ




:hammer_and_wrench: **GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models**, in EMNLP Oral. [[pdf](https://arxiv.org/abs/2205.12247)] [[benchmark](https://github.com/WadeYin9712/GeoMLAMA)]

> [Probeä»‹ç»åšå®¢](https://zhuanlan.zhihu.com/p/362385123)
>
> * Probeæ¢ç©¶äº†ç¥ç»ç½‘ç»œçš„**å†…éƒ¨æœºåˆ¶**å¦‚ä½•å¯¹auxiliary linguistic tasks (or probe tasks, or ancillary tasks)è¿›è¡Œ**åˆ†ç±»**
>
> å…·ä½“åœ°ï¼Œä»¥BERTä¸¾ä¾‹ï¼Œå¯¹äºä¸€ä¸ªåœ¨è®­ç»ƒåœ¨ä¸»ä»»åŠ¡ä¸Šçš„å¤§å‹ç¥ç»ç½‘ç»œï¼ŒProbeæ˜¯ä¸€ä¸ªæ’å…¥åœ¨å…¶ä¸­é—´å±‚çš„æµ…å±‚ç¥ç»ç½‘ç»œï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨å±‚ã€‚Probeæœ‰åŠ©äºæ¢æŸ¥ä¸åŒå±‚æ•è·çš„ä¿¡æ¯ã€‚**ä½¿ç”¨è¾…åŠ©ä»»åŠ¡å¯¹Probeè¿›è¡Œè®­ç»ƒå’ŒéªŒè¯**ï¼Œ**ä»¥å‘ç°æ˜¯å¦æ•è·äº†æ­¤ç±»è¾…åŠ©ä¿¡æ¯**ã€‚åŸæ–‡ä½œè€…ç»™äº†ä¸€ä¸ªå›¾ç‰‡ç¤ºä¾‹ï¼š
>
> ![img](https://pic2.zhimg.com/80/v2-726af6b149c175f13a931cc48cdbeb75_720w.webp)

* åŠ¨æœºï¼š**å¸¸è¯†çŸ¥è¯†åœ¨ä¸åŒçš„åœ°ç†ä½ç½®**ä¸­ï¼Œä¼šæœ‰æ‰€ä¸ä¸€æ ·

![image-20230203111838400](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230203111838400.png)

* æ•°æ®é›†æ ‡æ³¨æ–¹æ³•

![image-20230203111924641](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230203111924641.png)





:hammer_and_wrench: :fire: **Automatic Chain of Thought Prompting in Large Language Models**, in ICLR 2023. [[pdf](https://arxiv.org/abs/2210.03493)] [[torch](https://github.com/amazon-science/auto-cot)] [[rebuttal](https://openreview.net/forum?id=5NTt8GFjUHkr)]

* åŠ¨æœº
  * è¿‡å»`COT`ä¸»è¦æ˜¯ä¸¤ç§
    * å®Œå…¨`Zero-shot COT`ï¼Œå°±ç®€å•è¯´ä¸€å¥â€œLetâ€™s think step by stepâ€ å°±è®©GPT-3ç›´æ¥ç”Ÿæˆ
    * `Manual COT` äººä¸ºæ‰‹åŠ¨æ ‡å®šä¸€äº›Q-Cçš„`demonstration` åš in-context learning
  * `Zero-shot COT` æ€§èƒ½ä¸å¤Ÿå¥½ï¼Œ`Manual COT` éœ€è¦æ‰‹å·¥æ ‡æ³¨ï¼Œå¹¶ä¸”å¯¹demonstrationçš„é¢†åŸŸå¾ˆæ•æ„Ÿï¼ˆä»¥å¾€éƒ½æ˜¯æ ¹æ®ç‰¹å®šé¢†åŸŸè¿›è¡Œæ ‡æ³¨ï¼‰

![image-20230210172030892](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230210172030892.png)

* ä½œè€…å¸Œæœ›è¿™äº›demonstrationç›´æ¥è®©`Zero-shot COT`ç”Ÿæˆï¼Œä½†æ˜¯å®éªŒè¿‡ç¨‹ä¸­å‘ç°ï¼Œdemonstrationå¯¹**å¤šæ ·æ€§çš„è¦æ±‚å¾ˆé«˜**ï¼Œæ‰€ä»¥è€ƒè™‘ä½¿ç”¨**é—®é¢˜èšç±»**çš„æ–¹å¼æ¥å…ˆåˆ†ç±»
  * **å¤šæ ·æ€§é«˜çš„demonstration**æ‰ä¼šæ•ˆæœå¥½ï¼Œè¯¦è§è®ºæ–‡ã€‚åšäº†è¯¦ç»†çš„å®éªŒå’Œè¯´æ˜
  * å› ä¸ºå¦‚æœå¤šæ ·æ€§ä¸é«˜ï¼Œdemonstrationç”Ÿæˆçš„chainçš„ç­”æ¡ˆå°±æ˜¯æœ‰é”™çš„ï¼Œ**å¦‚æœéƒ½æ˜¯åŒä¸€ç±»çš„è¯ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯demonstrationç­”æ¡ˆå…¨æ˜¯é”™çš„**ï¼Œæ›´åˆ«è¯´åé¢ç”Ÿæˆç›®æ ‡é—®é¢˜çš„ç­”æ¡ˆäº†

![image-20230210172719091](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230210172719091.png)

* æ–¹æ³•æ¨¡å‹
  * âš ï¸æ³¨æ„äººå·¥ç­›é€‰æ˜¯æŒ‡ç”¨ä¸€äº›è§„åˆ™è¿›è¡Œç­›é€‰ï¼Œè¯¦è§è®ºæ–‡

![image-20230210172754187](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230210172754187.png)





**Iteratively Prompt Pre-trained Language Models for Chain of Thought**, in EMNLP 2022. [[pdf](https://arxiv.org/abs/2203.08383)] [[torch](https://github.com/sunlab-osu/IterPrompt)]

* åŠ¨æœº

  * `CoT`çš„å…¶ä¸­ä¸€ç§å®ç°æ–¹å¼

  ![image-20230323172041557](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230323172041557.png)

* æ–¹æ³•

![image-20230323172136781](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230323172136781.png)



**Inductive Relation Prediction with Logical Reasoning Using Contrastive Representations**, in EMNLP 2022. [[pdf](https://aclanthology.org/2022.emnlp-main.286/)

* åŠ¨æœº

![image-20230327110331795](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230327110331795.png)



* æ–¹æ³•

![image-20230327110354474](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230327110354474.png)



:hammer_and_wrench: **Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.04091)] [[code](https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting[)]

* è¿‡å»çš„`COT`å­˜åœ¨çš„é—®é¢˜
  * calculation errors
  * missing-step error
  * semantic misunderstanding errors

![image-20230510163540908](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230510163540908.png)

* æˆ‘ä»¬çš„æ–¹æ³•å’Œè¿‡å»å¯¹æ¯”

![image-20230510163605207](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230510163605207.png)



:fire: **Think Twice: Measuring the Effificiency of Eliminating Prediction Shortcuts of Question Answering Models**, in ACL 2023. [[pdf](https://arxiv.org/pdf/2305.06841.pdf)]

* QAæ•°æ®é›†ä¸­ï¼ŒLLM **biaså’Œdebiasæ–¹æ³•çš„ç ”ç©¶**
* debiasçš„ä¸€äº›æ–¹æ³•
  * Resampling
  * **LearnedMixin**
    *  uses a biased model as a complement of the trained debiased model in a weighted composition.
  * **Confifidence Regularization**
    * aims to reduce the modelâ€™s confifidence, i.e. **the predicted score over samples marked as biased.**
    * æœ¬æ–‡å®éªŒä¸­ï¼šuse BERT-BASE for both the teacher and bias model.



## :icecream: Vision-Language Text Generation

### :book: Paraphrase

:hammer_and_wrench: **[Sentence Discrimination] Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator**,in COLING 2018. [[pdf](https://aclanthology.org/C18-1230/)] [[torch](https://github.com/badripatro/PQG)]

:hammer_and_wrench: **[Hierarchical Sketch&Paraphrase Generation] Hierarchical Sketch Induction for Paraphrase Generation**, in ACL 2022.[[pdf](https://aclanthology.org/2022.acl-long.178.pdf)] [[torch](https://github.com/tomhosking/hrq-vae)]

---



### :framed_picture: Image Caption

:white_check_mark: :hammer_and_wrench: **[Image Caption] Generating Diverse and Descriptive Image Captions Using Visual Paraphrases**, in ICCV 2019. [[pdf](https://ieeexplore.ieee.org/document/9010984)] [[torch](https://github.com/pkuliu/visual-paraphrases-captioning)]

* è¯¥è®ºæ–‡ç ”ç©¶äº†ç›®å‰å›¾åƒçš„æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å¤è¿°çš„ä¸¤é˜¶æ®µè§£ç çš„æ¨¡å‹ã€‚
  * ç»™å®šå›¾åƒè¾“å…¥ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç”Ÿæˆåˆæ­¥çš„å¥å­ï¼Œå†å°†å…¶æ”¹å†™ä¸ºå†…å®¹æ›´åŠ å¤šæ ·å’Œä¸°å¯Œçš„æè¿°ã€‚åœ¨MS COCOå›¾åƒæè¿°æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ã€‚

  * é‡ç‚¹æ¢ç´¢**visual paraphrases** è§’è‰² + **scoring function**
  
    * ```mermaid
      graph LR
      ä¸äººç±»ç›¸æ¯” --æ–‡ç« ä¸­æœ‰example--> ç¼ºå°‘å¤šæ ·æ€§å’Œå…·ä½“æ€§ --> ä¸¤é˜¶æ®µè§†è§‰å¤è¿°æ–¹æ³• --> MSCOCOæ•°æ®é›†
      ```
  
  
  * æ•…äº‹å±•å¼€:
  
    * ```mermaid
      graph LR
      æ ‡å‡† -->æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> å½¢å®¹è¯
      æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> ç»†èŠ‚,with
      å½¢å®¹è¯ --> Pa((Paraphrase))
      ç»†èŠ‚,with --> Pa
      Pa --> visual-paraphrase
      visual-paraphrase --> sentence_pairs --> ä¸¤é˜¶æ®µç¼–ç 
      ```
  
    * ```mermaid
      graph LR
      ç›¸å…³å·¥ä½œ --caption--> å¤šcaption.vs.å•caption --paraphrases--> æœªå¤„ç†ç‰¹å¾å’Œè§†è§‰ä¿¡æ¯ --ä¸¤é˜¶æ®µç¼–ç --> ä¸­é—´seq.vs.2captions 
      ```
  
  
  * æ¨¡å‹æ–¹æ³•ï¼š
  
    * ```mermaid
      graph LR
      é€‰æ‹©è§†è§‰å¤è¿°captionå¯¹ --> è¯„åˆ†å‡½æ•° --> è®¾è®¡ä¸‰ä¸ªAttentionæ“ä½œ,å­¦ä¹ åˆ°å¤šæ¨¡æ€çŸ¥è¯† --> æœ€åsoftmaxè¾“å‡º
      ```

* æ›´å¤šç»†èŠ‚å¯è§æˆ‘ä¸ªäººçš„[slide](https://kdocs.cn/l/conDzdschwAn)

:white_check_mark: ::fire: :hammer_and_wrench: **[Text Generation & Image Caption] Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions**, in CVPR 2019. [[pdf](https://openaccess.thecvf.com/content_CVPR_2019/html/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.html)] [[torch](https://github.com/aimagelab/show-control-and-tell)]

![](https://s2.loli.net/2022/04/09/COnvomETrl6GRf2.png)

![](https://s2.loli.net/2022/04/09/7ASmXcCazOh9GsU.png)

```mermaid
  graph LR
  å¤–éƒ¨ä¿¡å·æ§åˆ¶ --> å›¾åƒä¸­çš„ä¸€ç»„åŒºåŸŸå— --> core((æ ¸å¿ƒ))
  core --> æ”¹å˜chunkçš„é¡ºåº
  core --> æ”¹å˜å›¾åƒçš„åŒºåŸŸ
  model((æ¨¡å‹)) --åŸºäºåŒºåŸŸçš„ç‰¹å¾ä¸çŠ¶æ€--> LSTM((LanguageModel,ä¸¤å±‚LSTM)) --ç¬¬ä¸€å±‚--> è®¡ç®—attention --æ³¨æ„--> æ‰€æœ‰åŒºåŸŸçš„ç‰¹å¾å‘é‡è¿›è¡Œmean-poolingä½œä¸ºå›¾åƒçš„æ€»ä½“ç‰¹å¾I
  LSTM --ç¬¬äºŒå±‚--> é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯
  model --ä½•æ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå›¾åƒåŒºåŸŸ--> å—è½¬ç§»é—¨ --è®¡ç®—gt--> åŸºäºç¬¬ä¸€å±‚LSTMçš„çŠ¶æ€è®¾ç«‹ä¸€ä¸ªchunk-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sc_t-rtçš„attention
  model --è§†è§‰è¯oræ–‡æœ¬æ¬¡--> AdaptiveAttention --> è®¾ç½®ä¸€ä¸ªvisual-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sv_t-rtçš„attention --> attentionçš„ç»“æœ,å¯ä»¥è®¡ç®—å‡ºå½“å‰æ—¶åˆ»æ¨¡å‹æ­£åœ¨å…³æ³¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ct
  model --æ— åºé›†åˆæ’åº--> æ’åºç½‘ç»œ --> Rä¸­åŒ…å«Nä¸ªåŒºåŸŸé›† --å…¨è¿æ¥å±‚--> æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --Sinkhornç®—å­--> è½¯ç½®æ¢çŸ©é˜µ 
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --> æœ€å°åŒ–è½¯ç½®æ¢ä¸çœŸå®ç»“æœä¹‹é—´çš„å‡æ–¹è¯¯å·®
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --æµ‹è¯•åŒˆç‰™åˆ©ç®—æ³•è¿›è¡ŒåŒ¹é…--> è½¯ç½®æ¢çŸ©é˜µè½¬åŒ–ä¸ºæœ€ç»ˆçš„ç½®æ¢,ä»¥æ­¤æ¥å¯¹Rè¿›è¡Œæ’åº
```

 [è¯¦ç»†è®²è§£](https://zhuanlan.zhihu.com/p/150667499)

:hammer_and_wrench: **Length-Controllable Image Captioning**, in ECCV 2020 by [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q) and  Mingkui Tan.  [[pdf](https://arxiv.org/abs/2007.09580)] [[torch](https://github.com/bearcatt/LaBERT)]

* åŠ¨æœº
  * ä¸ºäº†è®©å¥å­æ›´åŠ ç²—ç•¥æˆ–è€…ç»†èŠ‚ï¼Œæå‡º**é•¿åº¦å¯æ§**çš„captionç”Ÿæˆ
  * è¿‡å»ç”±äºæ–¹æ³•æ˜¯è‡ªå›å½’çš„ï¼Œæ‰€ä»¥è®¡ç®—å¤æ‚åº¦ä¼šéšç€å¥å­é•¿åº¦ä¸Šå‡è€Œä¸Šå‡ã€‚ï¼ˆæ¨¡å‹ä¸Šçš„åˆ›æ–°ï¼‰

![image-20220831210727673](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831210727673.png)

ä¹‹å‰çš„SOTAæ–¹æ³•å¯èƒ½ä¼šé—æ¼ä¸€äº›å…³é”®çš„ä¿¡æ¯ï¼Œå¦‚æœæˆ‘æƒ³è¦æ›´åŠ ç»†èŠ‚ç‚¹çš„æè¿°ï¼Œä»–ä»¬æ— æ³•ç”Ÿæˆã€‚

* æ–¹æ³•

  > è¿‡å»ç”±äºæ–¹æ³•æ˜¯è‡ªå›å½’çš„ï¼Œæ‰€ä»¥è®¡ç®—å¤æ‚åº¦ä¼šéšç€å¥å­é•¿åº¦ä¸Šå‡è€Œä¸Šå‡ã€‚åœ¨è¿™é‡Œæå‡ºäº† non-autoregressiveçš„æ–¹æ³•ã€‚

  * è·å–å¥å­é•¿åº¦ä¿¡æ¯ï¼ˆlevel -> $[L_{low}, L_{high}]$ï¼‰åšembedding

  * æå‡ºDecode é˜¶æ®µ (non-autoregressive) **LaBERT**

    * ä½¿ç”¨ä½ç½®ä¿¡æ¯æ¥é¢„æµ‹mask

    * ä½¿ç”¨é•¿åº¦ä¿¡æ¯æ¥é¢„æµ‹unmask

    * æ¨ç†çš„æ—¶å€™é¼“åŠ±ç”Ÿæˆ**æ›´é•¿çš„å¥å­**

      * exponentially decay: $p_i\left(s_i=[\mathrm{EOS}]\right) \leftarrow \gamma^{L_{\text {high }}-i} p_i\left(s_i=[\mathrm{EOS}]\right), \forall i \in\left[L_{\text {low }}, L_{\text {high }}\right]$

        ![image-20220831212334860](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831212334860.png)

      * æ¯ä¸€æ­¥éƒ½ä¼šå¯¹æœ€ä½ç½®ä¿¡åº¦çš„å•è¯è¿›è¡Œmask

:hammer_and_wrench: **Human-like Controllable Image Captioning with Verb-specific Semantic Roles**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Human-Like_Controllable_Image_Captioning_With_Verb-Specific_Semantic_Roles_CVPR_2021_paper.pdf)] [[torch](https://github.com/mad-red/VSR-guided-CIC)]

* ä¸ä¸Šé¢ä¸¤ç¯‡å·¥ä½œå¯æ§æ€§çš„å¯¹æ¯”

![image-20220831213129731](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831213129731.png)

* åŠ¨æœº

  * äº‹ä»¶å…¼å®¹æ€§ï¼Œä¸¤ä¸ªä¸å…¼å®¹çš„äº‹ä»¶ä¸åº”è¯¥åˆåœ¨ä¸€èµ·

  * é‡‡æ ·çš„å…¼å®¹æ€§ï¼Œä¸åˆç†çš„é‡‡æ ·ä¸åº”è¯¥å‡ºç°åœ¨å¥å­å½“ä¸­

  * å¯¹äºä¸Šé¢çš„caseï¼š 

    ```python
    verb=sit, Arg1="thing sitting", Arg2="sitting position" 
    verb=read, Arg0="reader", Arg1="thing read"
    ```

* æ–¹æ³•ä¸Šæ˜¯å…ˆæŠ½å–å‡ºæ¥çº¦æŸçš„æ ‡ç­¾ï¼Œå†decoder

![image-20220831213747447](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831213747447.png)

:star: **MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning**, in AAAI 2022.  [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/20243)]

* åŠ¨æœº
  * ä¸ºäº†captionçš„ç”Ÿæˆæ›´åŠ ä¸°å¯Œå¤šæ ·ï¼Œå¹¶ä¸”**æ— éœ€è¿‡å¤šçš„æ ‡æ³¨æ•°æ®**ï¼
  * captionç›´æ¥åšåˆ°**åœºæ™¯æ–‡æœ¬**çº§åˆ«
* æ–¹æ³•
  * Unpired Captioningçš„æ–¹æ³•ï¼ˆå…¶å®å°±æ˜¯`GAN`çš„æ€æƒ³ï¼‰
  * å­¦åˆ°äº†æ¨¡æ€å†…éƒ¨ï¼Œè·¨æ¨¡æ€ä¹‹é—´çš„å…³ç³»
  * Unpaired å­¦ä¹ çš„èŒƒå¼ï¼Œæ— éœ€è¿‡å¤šç›‘ç£ä¿¡å·

![image-20220904163643769](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220904163643769.png)





:hammer_and_wrench: :fire: **Show, Edit and Tell: A Framework for Editing Image Captions**, in CVPR 2020.  [[pdf](https://arxiv.org/abs/2003.03107)] [[torch](https://github.com/fawazsammani/show-edit-tell)]

* ç›´æ¥å¯¹ç”Ÿæˆçš„captionè¿›è¡Œç¼–è¾‘ä¿®æ”¹

:hammer_and_wrench: **Towards Accurate Text-based Image Captioning with Content Diversity Exploration**, in CVPR 2021. [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Towards_Accurate_Text-Based_Image_Captioning_With_Content_Diversity_Exploration_CVPR_2021_paper.pdf)]  [[torch](https://github.com/guanghuixu/AnchorCaptioner)]

* åŠ¨æœº
  * Captionç”Ÿæˆçš„å¤šæ ·æ€§
  * æŒ‘æˆ˜
    * ä¸çŸ¥é“åº”è¯¥å¦‚ä½•é€‰æ‹©æ–‡æœ¬ä¿¡æ¯
    * æ–‡æœ¬å’Œå›¾ç‰‡ä¹‹é—´çš„å…³ç³»
    * å¤šæ ·æ€§captionçš„ç”Ÿæˆ

![image-20220831221056898](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831221056898.png)



* æ¨¡å‹æ–¹æ³•

![image-20220831221253743](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831221253743.png)



**Improving OCR-based Image Captioning by Incorporating Geometrical Relationship**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf)]

* åŠ¨æœº
  * æ— æ³•å»ºç«‹OCRæŠ½å‡ºæ¥ä¸œè¥¿ä¹‹é—´çš„å…³ç³»
* æ–¹æ³•
  * é€šè¿‡é«˜åº¦ï¼Œå®½åº¦ã€è·ç¦»ã€IoUå’Œæ–¹å‘æ„å»ºç›¸åº”çš„OCR

![image-20220831223252058](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831223252058.png)



 :hammer_and_wrench: **Towards Unique and Informative Captioning of Images**, in ECCV 2020.  [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-58571-6_37.pdf)] [[torch](https://github.com/princetonvisualai/SPICE-U)]

* ç›®å‰é—®é¢˜ï¼š

![image-20220901103845451](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901103845451.png)

* å…³é”®è´¡çŒ®ï¼Œåšäº†ä¸€ä¸ª**æ–°çš„è¯„ä»·æŒ‡æ ‡**

:hammer_and_wrench: **Comprehensive Image Captioning via Scene Graph Decomposition**, in ECCV 2020.  [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-58568-6_13.pdf)] [[torch](https://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html)]

* åœºæ™¯å›¾åˆ†è§£æ¥å®ç°å¤šæ ·æ€§

![image-20220901104357419](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104357419.png)

![image-20220901104406842](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104406842.png)

:hammer_and_wrench: **In Defense of Scene Graphs for Image Captioning**, in ICCV 2021.  [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9710596)]  [[torch](https://github.com/ Kien085/SG2Caps)]

* åŠ¨æœº
  * å¼¥è¡¥æ–‡æœ¬åœºæ™¯å›¾è¿˜æœ‰è§†è§‰åœºæ™¯å›¾ç›´æ¥çš„Gap
  * ä»¥å¾€çš„å·¥ä½œåœ¨è®­ç»ƒcaptioneræ—¶ï¼Œå¾€å¾€ç”¨**TSGä½œä¸ºè¾“å…¥**ï¼Œæµ‹è¯•æ—¶å†æ¢æˆVSG
  * VGæ•°æ®é›†ä¸Šå­¦å¾—çš„åœºæ™¯å›¾ä¸­relationshipå¤šæ˜¯has, onè¿™ç±»**æ— æ„ä¹‰çš„å…³ç³»**
  * VSGä¸TSGå¹¶ä¸å…¼å®¹  ï¼ˆä¸¤ä¸ªåœºæ™¯å›¾ä¹‹é—´ï¼‰

![image-20220831224945278](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831224945278.png)

* åŸºæœ¬æ€æƒ³
  *  close the **semantic gap** between the two scene graphs
  * ä½¿ç”¨**HOIä¿¡æ¯å¢å¼ºVSG**ï¼Œå¹¶å¼•å…¥object locationä¿¡æ¯æå‡VSGçš„è¡¨è¾¾èƒ½åŠ›

![image-20220831224458911](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831224458911.png)

* å…·ä½“æ–¹æ³•

  * VSGæ„å»º
    * VGæ•°æ®é›†è®­ç»ƒä¸€ä¸ªVSG generatorï¼ŒåŒä»¥å¾€å·¥ä½œä¸€æ ·å¯¹MSCOCOä¸­çš„å›¾ç‰‡ç”ŸæˆVSGã€‚ä¸æ­¤åŒæ—¶ï¼Œä½œè€…åˆåœ¨MSCOCOä¸Šè®­ç»ƒäº†ä¸€ä¸ªobject detectorï¼Œå¯¹å›¾ç‰‡æ£€æµ‹å‡ºä¸€ç³»åˆ—çš„ç‰©ä½“ã€‚
  * VSGç¼–ç 
    * éšåä½¿ç”¨HOI inferenceå¯¹ä¸äººç›¸å…³çš„ç‰©ä½“è¿›è¡Œå…³ç³»åŠå±æ€§çš„é¢„æµ‹ã€‚æœ€åå–åŸå§‹VSGä¸HOI (æ£€æµ‹åˆ°çš„ç‰©ä½“) graphçš„å¹¶é›†ä½œä¸ºæœ€ç»ˆVSGã€‚
    * ä½¿ç”¨å¤šä¸ªGCNå¯¹å…¶è¿›è¡Œç¼–ç ï¼Œä¸åŒç±»å‹çš„èŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„GCNå‚æ•°ã€‚
  * decodeé˜¶æ®µ (Up-down)
    * ä»…ä»…ä½¿ç”¨scene graphï¼Œä¸ä½¿ç”¨ä»»ä½•è§†è§‰ç‰¹å¾ï¼ŒSG2Capsæ¨¡å‹ä¾¿å¯ä»¥å–å¾—æœ‰ç«äº‰åŠ›çš„æè¿°ç”Ÿæˆç»“æœã€‚
  * caseå±•ç¤º

  ![image-20220831230157943](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831230157943.png)



:hammer_and_wrench: **Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning**, in CVPR 2022. [[pdf]()] [[torch](https://github.com/GT-RIPL/Xmodal-Ctx)]

* å…³æ³¨åˆ°æ›´å¤šçº§åˆ«çš„ä¿¡æ¯

![image-20220901105828069](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901105828069.png)

* æ–¹æ³•ä¸Šä¸»è¦åŠ å…¥äº†Crop

![image-20220901105902348](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901105902348.png)

:hammer_and_wrench: **Comprehending and Ordering Semantics for Image Captioning**, in CVPR. [[pdf](https://arxiv.org/pdf/2206.06930.pdf)] [[torch](https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet)]

* **è¯­ä¹‰çš„è¯­è¨€æ’åº**ï¼ˆä¸å•å•æ˜¯å¯¹è±¡ï¼‰åŒæ ·å¾ˆé‡è¦

![image-20220901111757395](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111757395.png)

* æ–¹æ³•

![image-20220901111817536](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111817536.png)

:hammer_and_wrench: **DIFNet: Boosting Visual Information Flow for Image Captioning**, in CVPR 2022.  [[pdf](DIFNet: Boosting Visual Information Flow for Image Captioning)] [[torch](https://github.com/mrwu-mac/DIFNet)]

* è€ƒè™‘äº†ä¿¡æ¯æµçš„ä¿¡æ¯

![image-20220901111002940](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111002940.png)

![image-20220901111014355](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111014355.png)

:hammer_and_wrench: **Injecting Semantic Concepts into End-to-End Image Captioning**, in CVPR 2022.  [[pdf](https://arxiv.org/abs/2112.05230)]  [[torch](https://github.com/jacobswan1/ViTCAP)]

* ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œdetector-free å’ŒåŠ å…¥è¯­ä¹‰concept
* è¿‡å»çš„å·¥ä½œ

![image-20220901104826507](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104826507.png)

* åŠ å…¥Concept
  * é€šè¿‡æŠ½å–captionä¸­çš„åŠ¨åè¯æˆ–è€…é€šè¿‡çŸ¥è¯†è’¸é¦å¾—åˆ°ä¸€äº›conceptä½œä¸º**ä¼ªæ ‡ç­¾**åšåˆ†ç±»

![image-20220901104847149](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104847149.png)



:hammer_and_wrench: **Show, Deconfound and Tell: Image Captioning with Causal Inference**, in CVPR 2022.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Show_Deconfound_and_Tell_Image_Captioning_With_Causal_Inference_CVPR_2022_paper.pdf)] [[torch](https: //github.com/CUMTGG/CIIC)]

* è§£å†³æ•°æ®é›†ä¸­å¤§é‡å‡ºç°äº†ï¼Œæ¨¡å‹**short-cut path** çš„é—®é¢˜

![image-20220901110327977](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901110327977.png)

* ä¸»è¦ä¸ºäº†è§£å†³ä¸¤ä¸ªCaptionå­˜åœ¨çš„é—®é¢˜
  * è¯†åˆ«**å¯¹è±¡é”™è¯¯**ï¼ˆé•¿å¤´å‘çš„ç”·äººè¯†åˆ«æˆäº†å¥³äººï¼‰
  * æè¿°å¾—**ä¸å¤Ÿå…³é”®å’Œè¯¦ç»†**

* Encoderé˜¶æ®µï¼ˆè§£å†³åˆ†ç±»å‡†ç¡®æ€§çš„é—®é¢˜ï¼‰
  * åŸºäºFaster-RCNNå¾—åˆ°æ— åçš„ç‰©ä½“åˆ†ç±»

![image-20220901110456322](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901110456322.png)

![image-20220915113730411](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915113730411.png)

![image-20220915113654808](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915113654808.png)

* decoderé˜¶æ®µè€ƒè™‘ç”Ÿæˆå•è¯çš„bias

![image-20220915114452498](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915114452498.png)



![image-20220915114520789](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915114520789.png)



**[å› æœå…³ç³» + å¼ºåŒ–å­¦ä¹ ] Dependent Multi-Task Learning with Causal Intervention for Image Captioning**, in IJCAI 2021.  [[pdf](https://www.ijcai.org/proceedings/2021/0312.pdf)] 

> è¯´å®è¯è¿™ç¯‡è®ºæ–‡å†™ä½œ**æœ‰ç‚¹å¤ªå¤æ‚äº†ï¼Œå¾ˆéš¾çœ‹æ‡‚**

* è§£å†³captionç”Ÿæˆ**åäº‹å®**ä¸**ä¸å¤Ÿè¯¦ç»†**çš„é—®é¢˜

![image-20220914112625963](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220914112625963.png)

* **å› æœå¹²é¢„åˆ†æè¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œè¯¦è§è®ºæ–‡**

![image-20220914112637246](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220914112637246.png)

**NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge**, in CVPR 2022. [[pdf](https://arxiv.org/pdf/2203.14499.pdf)]

* ä»»åŠ¡æè¿°

  * æ–°ç‰©ä½“æè¿°(novel object captioning)ï¼Œå³è®©æ¨¡å‹æè¿°å‡ºè®­ç»ƒ**æè¿°è¯­æ–™ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„ç‰©ä½“**ã€‚
  * å¦‚å›¾ï¼Œä¼ ç»Ÿçš„captionè®­ç»ƒå¦‚å·¦ä¸‹æ¡†æ‰€ç¤ºï¼Œè®­ç»ƒè¯­æ–™é‡Œæ²¡æœ‰rampè¿™ä¸ªè¯ï¼Œæµ‹è¯•æ—¶è‡ªç„¶ä¹Ÿä¸ä¼šç”Ÿæˆå«æœ‰**ramp**çš„å¥å­ã€‚

  ![img](https://raw.githubusercontent.com/Gary-code/pic/main/img/v2-f66020b9a4dad5f7a4ef1108c57468f5_720w.webp)

  * åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¾€å¾€ä¼šè§åˆ°åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡è§è¿‡çš„æ–°ç‰©ä½“ï¼Œæ­¤æ—¶ä¼ ç»Ÿçš„æ–¹æ³•ä¸èƒ½åšåˆ°å¯¹æ–°ç‰©ä½“ç”Ÿæˆæè¿°ï¼Œä¸ç¬¦åˆæˆ‘ä»¬å¯¹æ¨¡å‹åº”ç”¨çš„æœŸå¾…ã€‚

* å½“å‰çš„æ–¹æ³•

  * **é‡‡é›†æ›´å¤šçš„æ•°æ®**ï¼Œè®©è®­ç»ƒè¯­æ–™åŒ…å«å°½å¯èƒ½å¤šçš„ç‰©ä½“å¹¶é‡æ–°è®­ç»ƒã€‚ç„¶è€Œæ•°æ®çš„é‡‡é›†å’Œæ ‡æ³¨æ˜¯éº»çƒ¦ä¸”æ˜‚è´µçš„ï¼Œå¹¶ä¸”æ¯æ¬¡åŠ å…¥æ–°ç‰©ä½“ï¼Œæ¨¡å‹éœ€éƒ½éœ€è¦**é‡æ–°è®­ç»ƒ**ï¼Œå¾ˆä¸ä¼˜é›…ã€‚
  * å¦‚å…ˆå¯¹å›¾ç‰‡åšç‰©ä½“æ£€æµ‹ï¼Œè·å¾—ROIå’Œobject tagsï¼Œå†ç»“åˆå…¶è®­ç»ƒcaptionæ¨¡å‹ã€‚ï¼ˆå½“å‰**ä¸»æµæ–¹æ³•**ï¼‰
  * ç„¶è€Œï¼Œæ‰€ç”¨ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„èƒ½åŠ›ä¹Ÿæ˜¯æœ‰é™çš„ï¼Œè§åˆ°çš„**æ–°ç‰©ä½“ææœ‰å¯èƒ½è¶…å‡ºæ£€æµ‹æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´**ã€‚

* åŠ¨æœº

  * äººç±»è®¤çŸ¥ç‰©ä½“æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€æ˜¯é **å¤–è§‚çš„åŒ¹é…**ï¼Œè§è¿‡å³è®¤è¯†ï¼›äºŒæ˜¯é å®šä¹‰ï¼Œå“ªæ€•æ²¡è§è¿‡ä¸€ä¸ªç‰©ä½“ï¼Œ**å‡­å€Ÿç‰©ä½“çš„å®šä¹‰**ï¼Œæˆ‘ä»¬å¾€å¾€ä¹Ÿèƒ½ç†è§£è¯†åˆ«ã€‚
  * æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªè¯è¯­æ£€ç´¢æ¨¡å—ï¼Œå°†**==æ–°ç‰©ä½“å®šä¹‰ä½œä¸ºå¤–éƒ¨çŸ¥è¯†==**å¼•å…¥captionæ¨¡å‹ï¼Œä¸captionæ¨¡å—ä¸€èµ·ç«¯åˆ°ç«¯åœ°è®­ç»ƒã€‚

* æ–¹æ³•

  ![image-20221005102539630](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221005102539630.png)

  * åŒ¹é…`loss`çš„è®¡ç®—

    * ä¸ºäº†é¼“åŠ±æ¨¡å‹å¼•å…¥æ–°çš„ç±»ï¼Œæˆ‘ä»¬å°†paddingï¼ˆå’ŒRegion featureæ•°é‡ä¸€è‡´ï¼‰çš„ç©ºç±»çš„15%éšæœºæ›¿æ¢æˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­éšæœºæŒ‘é€‰çš„è¯ä½œä¸ºGT

    * è®¡ç®—$Hungarian$ loss (åŒˆç‰™åˆ©æŸå¤±)
      $$
      \mathcal{L}_{\mathrm{H}}(\mathcal{Y}, \mathcal{V})=\sum_{i=1}^K-\log \operatorname{sim}\left(\mathbf{y}_i, \hat{\mathbf{v}}_{\hat{\sigma}(i)}\right)
      $$
      

**Image Captioning with Novel Topics Guidance and Retrieval-based Topics Re-weighting**, in TMM 2022. [[pdf](https://ieeexplore.ieee.org/document/9869686)]

* åˆ©ç”¨ä¸»é¢˜(**Topic**)æ¨¡å‹æ¥åšimage caption

  * æ¢ç´¢å›¾ç‰‡å¯¹è±¡å’ŒTopicä¹‹é—´çš„å…³ç³»

* ä»€ä¹ˆæ˜¯ä¸»é¢˜ [é“¾æ¥](https://zhuanlan.zhihu.com/p/41683009)

  * ä¸‰ä¸ªlistï¼šã€é¸¡èƒ¸è‚‰ï¼Œè›‹ç™½ç²‰ï¼Œé¥®é£Ÿæ§åˆ¶ã€‘ã€ã€è·‘æ­¥æœºï¼Œæ¤­åœ†æœºï¼Œé¾™é—¨æ¶ã€‘ã€ã€å‡è‚¥ï¼Œä½“é‡ä¸‹é™ï¼Œç²¾ç¥çŠ¶æ€ã€‘ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬ä¹Ÿèƒ½å¤§æ¦‚çŸ¥é“æ¯ä¸ªè¯èƒŒåçš„ä¸»é¢˜æ˜¯ä»€ä¹ˆäº†ã€‚
  * æˆ‘ä»¬å¯ä»¥å°†Topic Model çœ‹å¾…ä¸ºä¸€ä¸ª**Clusteré—®é¢˜**ã€‚è€Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å°†ä¸€äº›ç‰¹å¾æ˜æ˜¾çš„è¯æŠ“å–å‡ºæ¥ã€‚

* æ–¹æ³•

  ![æˆªå±2022-10-08 19.14.43](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-08%2019.14.43.png)

  * Topicæ˜¯é€šè¿‡`NMF`(NLTKä¸­æœ‰)æ¥ä»captionä¸­æå‰è·å–çš„ï¼Œæ•°é‡ä¸º200ã€‚
  * RTRæ˜¯æ¨ç†æ—¶å€™æ‰ä½¿ç”¨çš„ï¼Œä¸ºäº†è®©**ä¸»é¢˜å’Œå›¾åƒä¿¡æ¯æ›´åŠ ç›¸å…³**
  * ETPå­¦ä¹ å¯¹è±¡å’Œä¸»é¢˜ä¹‹é—´çš„ç›¸ä¼¼åº¦
  * STPæ¨¡å—åµŒå…¥åœ¨æ¯ä¸ªLSTMCellä¸Šé¢ï¼Œä¸ºäº†åœ¨å¯¹åº”çš„æ—¶é—´æ­¥é€‰æ‹©åˆé€‚çš„ä¸»é¢˜è¿›è¡Œcaption



**[ä¸»é¢˜ + Image Caption] Show, Rethink, And Tell: Image Caption Generation With Hierarchical Topic Cues**, in ICME 2021. [[pdf](https://ieeexplore.ieee.org/document/9428353)]

* åŠ¨æœºï¼ˆcaptionæœ¬èº«å°±å…·å¤‡å¤šä¸ªæˆ–è€…ä¸€ä¸ªä¸»é¢˜çš„ç‰¹å¾ï¼‰ã€ç›®å‰è§‰å¾—topicçš„ä½œç”¨å°±æ˜¯ç”Ÿæˆæ›´åŠ å¤šè¯­ä¹‰ä¿¡æ¯+ç”Ÿæˆæ›´åŠ ç²¾ç¡®ã€‘

  * ä¹‹å‰æ–¹æ³•æ²¡æœ‰å»ºç«‹ä¸åŒç±»å‹å›¾ç‰‡ç‰¹å¾çš„è”ç³»ï¼Œcaptionä¸å¤Ÿå‡†ç¡®
  * decoderæ—¶å€™æ³¨æ„åŠ›æœºåˆ¶å¯¹äºæ¯ä¸ªå•è¯åªä¼šæ“ä½œä¸€æ¬¡

* æ–¹æ³•

  * æ ¹æ®Faster-RCNNç‰¹å¾æ„å»ºå‡ºä¸»é¢˜->å…³é”®è¯çº§åˆ«çš„ç‰¹å¾ï¼ˆæ¯ä¸ªæ£€æµ‹åŒºåŸŸéƒ½ä¼šæ£€ç´¢å‡ºå¯¹åº”çš„ä¸»é¢˜ï¼‰
  * decoderæ—¶å€™åŠ å…¥å¯¹ä¸»é¢˜çš„attentionæ“ä½œï¼ˆå¯ä»¥ç”Ÿæˆä¸€äº›ä¸å¸¸ç”¨çš„æè¿°ï¼Œå¦‚little boyä¸­çš„littleï¼‰

  ![æˆªå±2022-10-17 16.08.26](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-17%2016.08.26.png)

:hammer_and_wrench: **Explicit Image Caption Editing**, in ECCV 2022. [[pdf](https://arxiv.org/pdf/2207.09625.pdf)] [[torch](https://github.com/baaaad/ECE)]

* ä»»åŠ¡ï¼šImage Captionç¼–è¾‘ï¼Œä¿®æ”¹å¾—æ›´åŠ åˆç†

  * è¿‡å»çš„æ–¹æ³•éƒ½æ˜¯éšå¼çš„ä¿®æ”¹ï¼Œç¼ºå°‘å¯è§£é‡Šæ€§ï¼Œæ›´åƒæ˜¯re-writingçš„ä»»åŠ¡ï¼Œå¥å­çš„ç»“æ„ä¹Ÿå®¹æ˜“è¢«ç ´å

  ![image-20221012105504301](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012105504301.png)

* æ•°æ®é›†æ„å»º

  * Ref-Capå’ŒGT-Capéƒ½éœ€è¦æ˜¯äººå·¥æ ‡è®°çš„ï¼ˆä»¥å‰çš„æ–¹æ³•Ref-Capæ˜¯æ¨¡å‹ç”Ÿæˆçš„ï¼‰
  * COCOæ•°æ®é›†
    * æ¯å¼ å›¾çš„5ä¸ªcapéƒ½ä½œä¸ºGTï¼Œç„¶åæ ¹æ®é€šè¿‡å›¾åƒå’Œcapè®¡ç®—å’Œå…¶ä»–ï¼ˆä¸æ˜¯è¿™å¼ å›¾ï¼‰captionç›¸ä¼¼åº¦è®¡ç®—ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„
  * e-SNLI-VEï¼ˆFlicker30Kï¼‰æ•°æ®é›†
    * contradictionå’Œentailmentåˆ†åˆ«ä½œä¸ºRefå’ŒGTå³å¯

* æ–¹æ³• (æ¨¡å‹æ˜¯åŸºäº**BERT**)

  * å¼•å…¥ç¼–è¾‘ç®—å­`<ADD>, <DELETE>, <KEEP>`

    ![image-20221012110107268](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012110107268.png)

    ![](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012110107268.png)

* Case Study

![image-20221012110327880](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012110327880.png)

:hammer_and_wrench: **Object-Centric Unsupervised Image Captioning**, in ECCV 2022.  [[pdf](https://arxiv.org/abs/2112.00969)] [[torch](https://github.com/zihangm/obj-centric-unsup-caption)]

* ä¸»è¦åŠ¨æœº
  * äººå·¥æ ‡æ³¨çš„image caption annotationå¤ªæµªè´¹æ—¶é—´äº†

* è´¡çŒ®
  * **æ— ç›‘ç£**çš„Image Caption + **å¤šè¯­è¨€**

* å¦‚ä½•æ— ç›‘ç£

  * ä½¿ç”¨ç½‘ç»œçˆ¬ä¸‹æ¥çš„è¯­æ–™åº“ï¼Œè¿˜æœ‰æ•°æ®é›†ä¸­å›ºæœ‰çš„å›¾åƒä¿¡æ¯æ¥åš
  * ä½†`BLEU4`å¾ˆä½ï¼Œåªæœ‰6å·¦å³

  ![image-20221105204258603](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221105204258603.png)

:hammer_and_wrench: **Human-Centric Image Captioning**, in PR 2022. [[pdf](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320322X00034/1-s2.0-S0031320322000267/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGoaCXVzLWVhc3QtMSJIMEYCIQDGI9Pe2Vg29Uf127g8bgoLH2in5NivB%2FYNbpJFB7t6pwIhAIJI6BJ3FO8IVZf7anhzx1JlrQnoA%2BvNPnTnFXynNyuLKswECEMQBRoMMDU5MDAzNTQ2ODY1IgyuR0sHCTJ7ZSggWKQqqQRnRT7Gv%2F7BVa9ChvOv%2Bv5L7LGF8GiKDTAVH%2FlK624uvf7X%2B8Rb0sn9tn7aGPgMwDLI8wG3Xu0uZKtLTtWkTE8zzNxzxDCTkNqLc7ePo3gd5kKdrjOCgXRyPF3K%2FKu%2FHiy5b%2FkKhvaXoSIv49WpU4Mw3jduHux0yg74CmV3I087VDVR6y0Z%2Fn1MGartEpeGdbwBoy%2F0Z59JHQF7rjquT4cG5isYwRK0nnr%2B79L%2BtWkL6uIaFyYk%2Bi%2B9GtknfOyGpuOQ4aF%2FVoFmdAzCsqwJAGjqS30nAaK7SiiiFtIoYpRPT5woODRU8SJ68UI4nHt0Es%2FEQJWTF59qx%2Fpl8SlN2sI%2FDIfN4aBDgwtF7ZPC3vgpwmcxBg67LjXS5Lwsha0Ui1PlDfo4EyrzzBHnE2wimBvn%2BU5L4xokqI246LjOALNUEUH8fPRm3ZnN9b2IWKqDFkJWwQ76T7FN2hnGPu%2F3JYf%2FX6mOJXaSVsumevPlqTn4oE1gQHZ%2FmP8uNc2feI36D4y17pFycY3HBZp5COJ3XlrSmGIBTFQFmXwt7JxQ14peTqI1GFR%2FTNZqTP9Uv6UkfQCdhYkK7dkemhKJpKtALjJz%2F0J1RIwaetYxcSMo%2F4kQDowiRJpQY7Y6OsRqgz7aqxCxPp9PNVIR64T9p%2Bj4VJdzsLs7B6%2BDWHoq0Sz99UQhVWupnVDXjH%2F8HLIaWQq5l6soAeVRmG8Z8t28pXM1pzgaoPzY%2Fdvl36r0MNqIv5oGOqgB8Mx5pBXQYDZWHBBnVJkdllGOlXSy8W7DgEygt929CiDsF%2F%2Fz4aCPqCmaN8WERin7vVZgIAwGD8dyHFgfCKJ0wY8%2BxnqhtHVeRq4FHknBpQJlBc50wlk9Jd4H4SIl%2FqASs5J7ssdt7UqhLhim%2FEosHKxTPOhVlHkd7HlgjOYebUvlAnxsOuqyy9LoMtdnpRwnCaCzqdOahqTFZ0bY1KoGgjLvfX306cS0&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221019T105552Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYV2BRT6MM%2F20221019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=cdb598601cea787f991b427a00715b9977c06eb5ca33f744d087db73f1e3f9d3&hash=4a2e7e1603daf30647f492f3181b0a0e2260dc043d979436c01ef5d645e16465&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320322000267&tid=spdf-176b5542-edc3-49a9-b2fd-3d5bf73304c5&sid=19abb8f59650524d941be66636a7de94bc37gxrqb&type=client&ua=51570d5e09030508575a&rr=75c8fbdf599b8b41)] [[code & dataset](https://github.com/JohnDreamer/HCCM/)]

* åŠ¨æœºã€åŠ¨æœºä¸æ˜¯å¾ˆå¼ºï¼Œè¯´å¾—ä¸å¤Ÿæœ‰é“ç†ã€‘

  * å½“å‰æ–¹æ³•ç¼ºä¹å¯¹äººç±»è¡Œä¸ºåŠ¨ä½œçš„æè¿°ï¼Œåªæ˜¯ç®€å•çš„æ¢ç´¢å‰æ™¯ç‰©ä½“
  * æ— è§†äº†è¿™ä¸ªå…³é”®ç‰©ä½“çš„ä¸€äº›ç»†èŠ‚è¿˜æœ‰å’Œå…¶ä»–ç‰©ä½“ä¹‹é—´çš„å…³ç³»

* æ–¹æ³•

  * æ ‡æ³¨**æ•°æ®é›†**ï¼Œå·²å…¬å¼€

    * äººå·¥æ ‡æ³¨äº†äººçš„èº«ä½“éƒ¨ä½çš„boxes

      ![æˆªå±2022-10-20 16.08.38](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-20%2016.08.38.png)

    * èƒŒæ™¯ç‰©ä½“å’Œæ´»åŠ¨ç‰©ä½“çš„ç‰¹å¾åŒºåˆ†æ˜¯é€šè¿‡ç®—å’Œäººä¹‹é—´çš„similarityæ¥å¾—åˆ°çš„

:hammer_and_wrench: **Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning**, in IJCV 2022. [[pdf](https://arxiv.org/abs/2105.04143)] [[torch](https://github.com/dandanguo1993/vtcm-based-image-paragraph-caption)]

> å¼•å…¥Topicæ¥åšparagraph Captionçš„ç”Ÿæˆ

* åŠ¨æœº

  * é™ä½ç”ŸæˆCaptionçš„éš¾åº¦
  * ä»¥å¾€çš„Topicæ¨¡å‹åšCaptionå­˜åœ¨å†—ä½™ä¿¡æ¯
  * åªæœ‰å•å±‚Topicä¸å¤Ÿä¸°å¯Œ
  * Captionçš„å¤šæ ·æ€§ï¼Œå¤šå±‚Topicçš„Keywordæ˜¯ç”¨æˆ·æƒ³è¦å¼ºè°ƒçš„ä¸œè¥¿

* æ–¹æ³•

  * ä½¿ç”¨**`end-to-end`çš„æ–¹æ³•**ï¼Œå³è®­ç»ƒä¸»é¢˜æ¨¡å‹ï¼Œåˆç”ŸæˆCaptionï¼Œå’Œä»¥å¾€çš„Two-stageä¸ä¸€æ ·ã€‚
    * ä»¥å¾€éƒ½æ˜¯å•å±‚çš„Topicæ¨¡å‹
    * ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆç”¨å¯¹è±¡åœ¨é¢„è®­ç»ƒå¥½çš„Topicæ¨¡å‹é‡Œé¢æ£€ç´¢å‡ºæ¥Topicï¼Œç¼ºä¹å¯¹å›¾åƒä¿¡æ¯çš„è€ƒè™‘æ¥ä¸¢å¼ƒTopicä¸­çš„å†—ä½™ä¿¡æ¯ã€‚
  * å­¦ä¹ åˆ°äº†**å±‚æ¬¡åŒ–**çš„Topicè¡¨ç¤º
  * å¯æ’æ‹”çš„

  ![image-20221023213041474](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221023213041474.png)

* Case Study

![image-20221023213130865](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221023213130865.png)



:hammer_and_wrench: :fire: **ClipCap: CLIP Prefix for Image Captioning**, on arXiv (2021). [[pdf](https://arxiv.org/abs/2111.09734)] [[torch](https://github.com/rmokady/CLIP_prefix_caption)]

> æ¨¡å‹éå¸¸ç®€å•ï¼Œè·‘èµ·æ¥ä¹Ÿå¾ˆç®€å•ã€‚ä¸€å°1080 Ti å°±å¯ä»¥è®­ç»ƒäº†ï¼

* åŠ¨æœº

  * è¿‡å»decodeå¥å­çš„æ—¶å€™éƒ½æ˜¯ä»¥å‰é¢ç”Ÿæˆçš„å•è¯ä½œä¸ºconditionï¼ŒåŒæ ·çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å›¾ç‰‡ä¿¡æ¯æ¥ä½œä¸ºcondition
  * å¯¹äºimages & captions $\{x^i, c^i\}^N_{i=1}$

  $$
  \max _\theta \sum_{i=1}^N \sum_{j=1}^{\ell} \log p_\theta\left(c_j^i \mid x^i, c_1^i, \ldots, c_{j-1}^i\right)
  $$

* æ¨¡å‹æ¶æ„

![image-20221225095940586](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221225095940586.png)



**Controllable Image Captioning via Prompting**, in AAAI 2023. [[pdf](https://arxiv.org/abs/2212.01803)]

* åŠ¨æœº

  * é€šè¿‡å­¦ä¹ **ä¸åŒçš„åŠ¨æ€prompt**ï¼Œç”Ÿæˆä¸åŒé£æ ¼çš„å›¾åƒå­—å¹•

  ![image-20230305214259376](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230305214259376.png)

* æ–¹æ³•æå…¶ç®€å•

  * é¢„è®­ç»ƒ+å¾®è°ƒç»“æ„
  * **é¢„è®­ç»ƒéƒ½æ˜¯BLIPçš„é‚£ä¸€å¥—**

![image-20230305214515377](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230305214515377.png)



:hammer_and_wrench: **Learning Distinct and Representative Modes for Image Captioning**, in NIPS 2022. [[pdf](https://arxiv.org/abs/2209.08231)] [[torch](https://github.com/bladewaltz1/ModeCap)]

* åŠ¨æœº
  * Captionçš„å¯æ§æ€§ + å¤šæ ·æ€§

![image-20230319202536999](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230319202536999.png)

* æ–¹æ³•

![image-20230319202651561](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230319202651561.png)



**Incorporating Unlikely Negative Cues for Distinctive Image Captioning**, in IJCAI 2023. [[pdf](https://www.ijcai.org/proceedings/2023/0083.pdf)]

* åŠ¨æœº
  * ä¸è¦ç”Ÿæˆé‚£ä¹ˆgenericï¼ˆé€šç”¨ï¼‰çš„Caption
  * ä½¿ç”¨unlikely Trainingçš„æ–¹æ³•ï¼ˆå°±æ˜¯é¿å…è®©æ¨¡å‹å­¦ä¹ åˆ°è´Ÿæ ·æœ¬çš„è¡Œä¸ºï¼‰

![image-20230829203433135](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829203433135.png)

* æ¨¡å‹æ–¹æ³•

![image-20230829203508036](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829203508036.png)



:hammer_and_wrench: **Transferable Decoding with Visual Entities for Zero-Shot Image Captioning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2307.16525)]] [[torch](https://github.com/FeiElysia/ViECap)]

* åŠ¨æœº
  * å‡å°‘æ²¡è§è¿‡ç‰©ä½“çš„**å¯¹è±¡å¹»è§‰**ç°è±¡å’Œbiasé—®é¢˜
    * OODé—®é¢˜
  * è¦è®©æ¨¡å‹åœ¨in-domainå’Œcross-domainéƒ½è¡¨ç°è‰¯å¥½çš„æ€§èƒ½
  * **åªç”¨çº¯æ–‡æœ¬æ¥è¿›è¡Œè®­ç»ƒ**

![image-20230829204925597](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829204925597.png)



* æ¨¡å‹æ–¹æ³•ï¼ˆçº¯æ–‡æœ¬è¿›è¡Œè®­ç»ƒ + zero-shot inferenceï¼‰
  * åŠ ä¸€ä¸ªCLIP entity clsssifieræ¥è¯†åˆ«å®ä½“

![image-20230829205102812](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829205102812.png)

* ä¸»è¦å¯¹æ¯”çš„æ˜¯ä¹‹å‰ä¸¤ç¯‡åŒæ ·æ˜¯çº¯æ–‡æœ¬è®­ç»ƒçš„Captionè®ºæ–‡ï¼ˆEMNLP Findings 2022 å’Œ ICLR 2022ï¼‰

![image-20230829205237122](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829205237122.png)



:hammer_and_wrench: **With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2308.12383)] [[torch](https://github.com/aimagelab/PMA-Net)]

* åŠ¨æœº
  * è¿‡å»çš„ä¼ ç»Ÿattentionä¸ä¼šå­¦ä¹ åˆ°åˆ«çš„æ ·æœ¬å½“ä¸­çš„è¯­ä¹‰ä¿¡æ¯
  * æå‡ºåŸå‹ç½‘ç»œæ¥åšImage Caption

![image-20230829215053304](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829215053304.png)

* æ¨¡å‹æ–¹æ³•

![image-20230829215139441](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230829215139441.png)



:hammer_and_wrench: **[å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘] CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation**, in ICCV 2023. [[pdf](https://vcg.seas.harvard.edu/publications/cliptrans-transferring-visual-knowledge-with-pre-trained-models-for-multimodal-machine-translation/paper)] [[torch](https://github.com/devaansh100/CLIPTrans)]

* åŠ¨æœº
  * å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘æ¨ç†çš„æ—¶å€™ä¸éœ€è¦ä½¿ç”¨åˆ°å›¾ç‰‡
  * è¿‡å»çš„æ–¹æ³•åœ¨ä½èµ„æºçš„è¯­è¨€ä¸Šè¡¨ç°ä¸å¥½ï¼ˆæœ¬èº«æ ‡æ³¨å°±å°‘ï¼‰
  * ç©¶å…¶åŸå› å¯èƒ½æ˜¯æ–‡æœ¬å’Œå›¾åƒçš„embeddingæ²¡æœ‰å¾ˆå……åˆ†çš„å¯¹é½

![image-20230905111542477](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230905111542477.png)

* æ¨¡å‹æ–¹æ³•
  * ä¸¤é˜¶æ®µè®­ç»ƒè¿ç§»å­¦ä¹ ï¼ˆ**caption -> ç¿»è¯‘**ï¼‰

![image-20230905111625639](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230905111625639.png)

* ä¿®æ”¹äº†Decoder

![image-20230905111848743](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230905111848743.png)

:hammer_and_wrench: **MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.664/)] [[torch](https://github.com/yangbang18/MultiCapCLIP)]

* åŠ¨æœº
  * å¤šè¯­è¨€çš„Image Caption
  * å¤šè¯­è¨€çš„Captionæ ‡æ³¨å›°éš¾
* æ–¹æ³•
  * åœ¨è®­ç»ƒçš„æ—¶å€™åªæ˜¯ä½¿ç”¨æ–‡æœ¬æ•°æ®ï¼Œæ¨ç†æ‰ä½¿ç”¨å›¾åƒæ¨ç†

![image-20230907213754699](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230907213754699.png)

* ä¾‹å­å±•ç¤º

![image-20230907215730610](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230907215730610.png)



:hammer_and_wrench: **[è§†è§‰ä¿¡æ¯å¢å¼ºLMæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›] Learning to Imagine: Visually-Augmented Natural Language Generation**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.526/)] [[torch](https://github.com/RUCAIBox/LIVE)]

> âš ï¸**äº‹å…ˆå£°æ˜ï¼šè¿™ç¯‡è®ºæ–‡å®é™…ä¸Šæ˜¯åšçº¯æ–‡æœ¬çš„ï¼Œè¾“å…¥çš„æ˜¯æ–‡æœ¬ï¼Œè¾“å‡ºçš„ä¹Ÿæ˜¯æ–‡æœ¬ï¼Œåªä¸è¿‡æ˜¯æƒ³è”æƒ³ç”Ÿæˆä¸€äº›å›¾ç‰‡æ¥å¢å¼ºæ¨¡å‹æ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›**

* åŠ¨æœº
  * äººç±»åœ¨å†™ä½œçš„æ—¶å€™ä¼šè”æƒ³ä¸€äº›ç”»é¢æ¥è¾…åŠ©å†™ä½œï¼Œæ¯”å¦‚è¯´æ»‘é›ªï¼Œå°±ä¼šæƒ³åˆ°ä¸€äº›å¯¹åº”çš„åœºæ™¯
  * åˆ©ç”¨å›¾ç‰‡å¢å¼ºLMæ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›
* æ¨¡å‹ï¼ˆ**Stable Diffusionæ¥ç”Ÿæˆå›¾ç‰‡**ï¼‰

![image-20230918095120803](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918095120803.png)



:hammer_and_wrench: **[å¤šæ¨¡æ€æ‘˜è¦ç”Ÿæˆ] CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization**, in ACL 2023. [[pdf]()] [[torch](https://github.com/xiaomin418/CFSum)]

* åŠ¨æœº
  * è¿‡å»çš„æ¨¡å‹æ²¡æœ‰å¾ˆå¥½æˆ–è€…æœ‰é€‰æ‹©æ€§çš„åˆ©ç”¨å›¾åƒä¿¡æ¯
  * æ— æ³•å…³æ³¨åˆ°æ–‡æœ¬ä¸­çš„å®ä½“å’Œå›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»

![image-20230918113933715](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918113933715.png)

* æ¨¡å‹æ–¹æ³•

![image-20230918114129452](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918114129452.png)



:fire: :hammer_and_wrench: **OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2307.11636)] [[project](https://torrvision.com/tvghic/)]

* åŠ¨æœº
  * åˆ›å»ºäº†ä¸€ä¸ªå¹½é»˜çš„Captionæ•°æ®é›†
  * å‘ç°æ˜¯ç¬¦åˆå¹½é»˜çš„è‰¯æ€§å†²çªç†è®ºçš„ [[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/400646267)]
  * è€Œä¸”æœ¬æ–‡ä¸æ˜¯ç”¨ä¼ ç»Ÿçš„BLEUç­‰æŒ‡æ ‡è¿›è¡Œæµ‹è¯„çš„ï¼ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20231013113213641](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231013113213641.png)

![image-20231013113317492](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231013113317492.png)

* æ–¹æ³•ï¼šå¢åŠ é€‰æ‹©çš„å¤šæ ·æ€§ï¼Œå› ä¸ºæœ¬æ¥æ•°æ®å°±æ˜¯å¾ˆå¤šæ ·çš„ 

![image-20231013113230477](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231013113230477.png)

:hammer_and_wrench: **ALIP: Adaptive Language-Image Pre-training with Synthetic Caption**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2308.08428)]] [[torch](https://github.com/deepglint/ALIP)]

* åŠ¨æœº
  * è¿‡å»çš„Captionåœ¨æµ·é‡æ•°æ®ç”Ÿæˆæ—¶å€™ï¼Œä¼šå­˜åœ¨I-T noiseçš„æƒ…å†µ

![image-20231021160149360](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231021160149360.png)

* æ–¹æ³•ï¼šç”¨OFAç”Ÿæˆåˆç†çš„Captionï¼Œæ§åˆ¶weightè®¾è®¡lossæ¥è®­ç»ƒï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20231021160251481](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231021160251481.png)



:hammer_and_wrench: **Noise-aware Learning from Web-crawled Image-Text Data for Image Captioning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2212.13563)] [[torch](https://github.com/kakaobrain/noc)]

* åŠ¨æœº
  * è¿‡å»ç½‘ä¸Šçš„æ•°æ®å­˜åœ¨noiseçš„é—®é¢˜ï¼Œå›¾æ–‡ä¸åŒ¹é…
  * è¿‡å»çš„æ–¹æ³•éœ€è¦filterå†è®­ç»ƒï¼ˆç±»ä¼¼BLIPï¼‰ï¼Œä½†æ˜¯ä¼šè®©æ¨¡å‹å­¦ä¸åˆ°ä¸°å¯Œçš„çŸ¥è¯†

![image-20231021160448062](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231021160448062.png)

* æ–¹æ³•

![image-20231021160523997](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231021160523997.png)



:fire: :hammer_and_wrench: **Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation**, in NeurIPS 2023. [[pdf]](https://arxiv.org/abs/2306.13460) [[torch](https://github.com/yuezih/SMILE)]

* åŠ¨æœº
  * å›¾åƒæè¿°ä»»åŠ¡ï¼ˆä¸ºç»™å®šå›¾åƒç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼‰é•¿æœŸä»¥æ¥å—åˆ°ã€Œè¾“å‡ºè¿‡äºå¹³å‡¡ã€çš„é—®é¢˜çš„å›°æ‰°ï¼Œå³æ¨¡å‹å€¾å‘äºä¸ºä¸åŒå›¾åƒç”Ÿæˆç¼ºä¹ç»†èŠ‚çš„ç›¸ä¼¼æè¿°ã€‚
  * ä¼ ç»Ÿçš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰æ‰€æä¾›çš„ä¸¥æ ¼ç›‘ç£å¹¶ä¸å®Œå…¨é€‚åˆå›¾åƒæè¿°æ¨¡å‹çš„ä¼˜åŒ–ã€‚
    * **ç›´è§‚åœ°å°†å…¶ä¼˜åŒ–è¿‡ç¨‹è§£è€¦ä¸ºä½¿æ¨¡å‹ç”Ÿæˆæ›´ä¸°å¯Œçš„â€œä¸°å¯Œæ€§ä¼˜åŒ–â€å’Œä½¿æ¨¡å‹ç”Ÿæˆæ›´ç®€æ´çš„â€œç®€æ´æ€§ä¼˜åŒ–â€ï¼Œå¹¶å‡è®¾è¿™ä¸¤ç§ä¼˜åŒ–çš„ç›¸äº’æŠ—è¡¡æœ€ç»ˆä½¿å¾—æ¨¡å‹ç”Ÿæˆå¹³å‡¡æè¿°**
  * æå‡ºäº†ä¸€ä¸ªæ–°çš„å­¦ä¹ ç›®æ ‡â€”â€”**åŠæ¸—é€æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆSMILEï¼‰**ï¼Œå®ƒå…è®¸â€œä¸°å¯Œæ€§ä¼˜åŒ–â€è€Œé˜»æ­¢â€œç®€æ´æ€§ä¼˜åŒ–â€ï¼Œä»è€Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´ä¸°å¯Œã€åŒ…å«æ›´å¤šç»†èŠ‚çš„æè¿°ã€‚

![img](https://pic4.zhimg.com/80/v2-d5760f29fa07ee9ffd9b76d04ab4d2a7_1440w.webp)

![img](https://pic1.zhimg.com/80/v2-715054ccd2eeb037b04f4937ea818b6c_1440w.webp)

* æ–¹æ³•ï¼ˆåœ¨BLIPä¸ŠåŠ ä¼¤SMILEåœ¨Captionæ•°æ®é›†ä¸Šé¢åšå¾®è°ƒï¼‰

![image-20231109162455341](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231109162455341.png)

:hammer_and_wrench: **Exploring Diverse In-Context Configurations for Image Captioning**, in NeurIPS 2023. [[pdf](https://arxiv.org/abs/2305.14800)] [[torch](https://github.com/yongliang-wu/ExploreCfg)]

> æ¢ç´¢In-context Learningå¦‚ä½•åšImage Captionï¼Œæœ‰ç‚¹å¥‡æ€ªçš„è®ºæ–‡ğŸ¤”

* åŠ¨æœº
  * å‘ç°few-shotçš„æ ·æœ¬ï¼Œå¾€å¾€å’Œcaptionçš„è´¨é‡ç›¸å…³

![image-20231112202436751](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231112202436751.png)

* æ¢ç´¢äº†å››ç§å›¾åƒé€‰æ‹©çš„ç­–ç•¥

![image-20231112202526320](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231112202526320.png)

:hammer_and_wrench: **Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models**, in NeurIPS 2023.  [[pdf](https://arxiv.org/abs/2305.19595)]

> çœ‹èµ·æ¥åƒæ˜¯åšCaptionè¿‡æ»¤çš„

* åŠ¨æœº
  * è¿‡å»VLMç”ŸæˆCaptionä¼šæœ‰ä¸€äº›biasçš„é—®é¢˜
  * ä¸»è¦å½’ç»“äºï¼šæ•°æ®é›†ä¸­Captionçš„è´¨é‡ + Captionçš„å¯†åº¦ï¼ˆæ˜¯å¦æè¿°å®Œæ•´ï¼‰ã€‚äº’è”ç½‘æ•°æ®çš„é”…

![image-20231114114703984](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114114703984.png)

* æ–¹æ³•

![image-20231114114725730](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114114725730.png)

:fire: :hammer_and_wrench: **Caption Anything: Interactive Image Description with Diverse Multimodal Controls**, 2023-05. [[pdf](https://arxiv.org/abs/2305.02677)] [[torch](https://github.com/ttengwang/Caption-Anything)]

* åŠ¨æœºï¼ˆåˆ†å‰²+å¯æ§ä¿¡å·çš„captionï¼‰

![image-20231116101406270](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116101406270.png)

* æ–¹æ³•ï¼ˆTraining Freeï¼‰

![image-20231116101429416](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116101429416.png)

![image-20231116101440390](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116101440390.png)

:fire: :hammer_and_wrench: **Attractive Storyteller: Stylized Visual Storytelling with Unpaired Text**, in ACL 2023. [[pdf]](https://aclanthology.org/2023.acl-long.619/)] [[torch](https://github.com/DingyiYang/StyleVSG)]

* åŠ¨æœºï¼šç”Ÿæˆæœ‰styleçš„æ•…äº‹ï¼ˆå¾ˆå°‘æœ‰æ ‡æ³¨ï¼‰

![image-20231120213011448](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231120213011448.png)

* æ–¹æ³•

![image-20231120213035079](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231120213035079.png)

## :sunglasses: Video Understanding

### :video_camera: Features Learning

:white_check_mark: :fire: :hammer_and_wrench: **[TSN] Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**, in ECCV 2016.  [[pdf](https://arxiv.org/abs/1608.00859)]  [[torch](https://github.com/yjxiong/temporal-segment-networks)]

* æŠ½å–æ‰€æœ‰å¸§æ˜¯ä¸ç°å®çš„ï¼ŒTSNå°†å…¶**ç­‰é—´éš”åˆ†**ä¸º$K$ä¸ªç‰‡æ®µï¼ˆi.e., $K=16$),åœ¨æ¯ä¸ªç‰‡æ®µä¸­è°å¯„æŠ½å–ä¸€å¸§ä½œä¸ºè¾“å…¥

* æä¾›äº†éå¸¸å¸¸ç”¨çš„æ•°æ®äº‰å¼ºæ–¹å¼å’Œä¸€äº›è®­ç»ƒæ—¶å€™çš„trickï¼ˆä¸»è¦åŒ…æ‹¬location jittering, horizontal flipping, corner cropping, and scale jitteringï¼‰

* ä»ç„¶åˆ©ç”¨åŒæµçš„æ€è·¯ï¼Œè®©æ¯ä¸ªç‰‡æ®µä¿¡æ¯æœ€åé€šè¿‡ä¸€ä¸ªå…±è¯†ç½‘ç»œå†Fusion

  ![](https://pic4.zhimg.com/80/v2-67b66b3618606af8b81d1f77b1f92a3b_1440w.jpg)

:white_check_mark: :fire: :hammer_and_wrench: **[TRN] Temporal Relation Reasoning in Videos**, in ECCV 2018.  [[pdf]()] [[torch](https://github.com/zhoubolei/TRN-pytorch)]

![img](https://pic4.zhimg.com/80/v2-86fa6c271c9d2dfad07d4603ed457a83_1440w.jpg)

* èåˆå°ºåº¦ç¡®å®š (éœ€è¦å¤šå°‘ä¸ªè§†é¢‘å¸§æ¥èåˆ)ã€å¦‚å›¾æ‰€ç¤ºã€‘æœ‰2ï¼Œ3ï¼Œ4è¿™ä¸‰ç§å°ºåº¦
* æ¯ä¸ªå°ºåº¦ä¸‹éœ€è¦å¤šå°‘ç»„è§†é¢‘å¸§
* åœ¨åº”ç”¨å¤šå°ºåº¦TRNçš„æ—¶å€™ï¼Œä¸€èˆ¬ä¼šé¢å¤–å¢åŠ ä¸€ä¸ªå…¨å¸§çš„å°ºåº¦ï¼Œå³12å¸§ç‰¹å¾å…¨éƒ¨concatåˆ°ä¸€èµ·ï¼Œä»¥å……åˆ†åˆ©ç”¨æœ‰æ•ˆä¿¡æ¯ã€‚
* **å¹³è¡¡**æ•ˆæœå’Œè®¡ç®—é€Ÿåº¦ï¼Œ**ç®€å•å¥½ç”¨**

:white_check_mark: :fire: :hammer_and_wrench: **[TSM] TSM: Temporal Shift Module for Efficient Video Understanding**, in ICCV 2019.  [[pdf](https://arxiv.org/abs/1811.08383)] [[torch](https://github.com/mit-han-lab/temporal-shift-module)]

* å¯¹æŸäº›é€šé“shiftï¼Œå¾—åˆ°å‰ä¸€å¸§æˆ–è€…åä¸€å¸§çš„ç‰¹å¾

  ![image-20220709174802714](https://s2.loli.net/2022/07/09/cb1f8LWpV9JotO3.png)

* ç”±äºshiftæ˜¯æœ‰æŸå¤±çš„ï¼Œä¸ºæ­¤è®¾è®¡æ®‹å·®æ¥è¿›è¡Œå¼¥è¡¥ï¼ˆåŸæ¥çš„ä¸æ®‹å·®çš„å¯¹æ¯”ï¼‰

![image-20220709174842935](https://s2.loli.net/2022/07/09/2dTjLBJwQ5FUber.png)

:white_check_mark: :fire: :hammer_and_wrench: **[LRCN] Long-term Recurrent Convolutional Networks for Visual Recognition and Description**, in CVPR 2015.  [[pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf)] [[torch](https://github.com/garythung/torch-lrcn)]

* CNNæŠ½å‡ºæ¥çš„å¸§ç‰¹å¾å†æ”¾è¿›å»`LSTM`å¾—åˆ°æ¯å¸§çš„æ—¶åºç‰¹å¾

> å…³äºè§†é¢‘ç‰¹å¾æŠ½å–ï¼Œä¸‹é¢è®²ä¸€ä¸‹`netvlad`ç³»åˆ—çš„ç»“æ„,NextVladå°±æ˜¯ä¸“é—¨é’ˆå¯¹è§†é¢‘å¸§èåˆæ¥åšçš„ä¼˜åŒ–ã€‚
>
> [ç›¸å…³åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/385512915)

:fire: :hammer_and_wrench: **[NetVLAD] NetVLAD: CNN architecture for weakly supervised place recognition**, in CVPR 2016.  [[pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)] [[torch (simple)](https://github.com/lyakaap/NetVLAD-pytorch)]

* VLADç®—æ³•ï¼ˆå®é™…ä¸Šå°±æ˜¯Kmeansï¼‰ï¼š
  $$
  V(j, k)=\sum_{i=1}^{N} a_{k}\left(x_{i}\right)\left(x_{i}(j)-c_{k}(j)\right), \quad k \in K, j \in D
  $$

* æœ¬æ–‡ä½¿ç”¨`CNN`æ¨¡æ‹Ÿè¯¥VLADç®—æ³•çš„è¿‡ç¨‹

  * å¹³æ»‘åŒ–$\alpha$ ä½¿å…¶å˜æˆä¸€ä¸ª0-1åˆ†å¸ƒçš„æƒé‡å‚æ•°ï¼Œä½¿ç”¨$1 \times 1$å·ç§¯+softmax è¿›è¡Œè¯¥è¿‡ç¨‹ï¼Œå¹³æ»‘æ¨å¯¼å…¬å¼$\bar{a}_{k}\left(\mathbf{x}_{i}\right)=\frac{e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k}\right\|^{2}}}{\sum_{k^{\prime}} e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k^{\prime}}\right\|^{2}}}$

  

  ![image-20220712104829439](https://s2.loli.net/2022/07/12/aFlJSCw8dvBnzEH.png)

   

:fire: :hammer_and_wrench: **[NextVLAD] NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**, in ECCV workshop 2018.  [[pdf](https://arxiv.org/pdf/1811.05014.pdf)] [[tensorflow](https://github.com/linrongc/youtube-8m)]

> åŒæ—¶ï¼Œè¿˜æœ‰[å…³äºå¤šæ¨¡æ€ï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰Transformeræ¨¡å‹çš„åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/388361095)



:hammer_and_wrench: **[PEFT&åŠ¨ä½œè¯†åˆ«] AIM: Adapting Image Models for Efficient Video Action Recognition**, in ICLR 2023. [[pdf](https://arxiv.org/abs/2302.03024)] [[torch](https://adapt-image-models.github.io/)]

> **å¤§æ¨¡å‹æ—¶ä»£**ä¸‹å¾ˆå¥½çš„å°è¯•`Parameter Effectient Fine-tuning`

* åŠ¨æœº
  * åˆ°åº•å¯ä¸å¯ä»¥ç›´æ¥**å†»ä½ä¸€ä¸ªå¾ˆå¥½çš„Image Model**ï¼Œåªæ˜¯**åŠ ä¸€äº›å¾ˆå°‘é‡çš„å¯å­¦ä¹ çš„å‚æ•°åšè§†é¢‘ç†è§£**çš„ä»»åŠ¡å‘¢ï¼Ÿ
  
  ![image-20230401174941604](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230401174941604.png)
  
* æ–¹æ³•ï¼ˆå¾ˆå·§å¦™å¾ˆç®€å•ï¼‰

![image-20230401175010208](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230401175010208.png)



:hammer_and_wrench: **Mitigating and Evaluating Static Bias of Action Representations in the Background and the Foreground**, in ICCV 2023 Oral. [[pdf](https://arxiv.org/abs/2211.12883)] [[torch](https://github.com/lihaoxin05/StillMix)]

* åŠ¨æœº & è´¡çŒ®
  *  å‰æ™¯è¿˜æœ‰åæ™¯éƒ½æœ‰å¯èƒ½é€ æˆåŠ¨ä½œè¯†åˆ«çš„bias
    * å‰æ™¯ï¼šè¡£æœé¢œè‰²
    * åæ™¯ï¼šèƒŒæ™¯ï¼Œæ’çƒåœº
  * æå‡ºbenchmarkæ¥è¯„ä¼°bias

![image-20230902181442456](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902181442456.png)

* debiasçš„æ–¹æ³•ï¼ˆåšæ•°æ®å¢å¼ºï¼‰- ä»bankä¸­é‡‡æ ·ä¸€å¸§æ¥æ’å€¼
  * banké‡Œé¢éƒ½æ˜¯è¦æ±‚æŸä¸ªactionçš„æ¦‚ç‡è¦é«˜ï¼Œä½†æ˜¯æœ¬èº«ä¸åŒ…å«åŠ¨ä½œä¿¡æ¯çš„å›¾åƒ

![image-20230902181709607](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902181709607.png)



---



### :timer_clock: Temporal Grounding

> æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªååˆ†ç»å…¸çš„ä»»åŠ¡ï¼ˆTemporal Groundingï¼‰æ¥çœ‹çœ‹è§†é¢‘çš„ç‰¹å¾æ˜¯å¦‚ä½•åˆ©ç”¨çš„

:fire::hammer_and_wrench: **[Video-NLP] Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, in AAAI 2020.  [[pdf](https://arxiv.org/pdf/1912.03590.pdf)] [[torch](https://github.com/microsoft/VideoX)]

> extend version: **MS-2D-TAN**, in TPAMI 2021.  [[pdf](https://arxiv.org/pdf/2012.02646.pdf)]  [[torch](https://github.com/microsoft/VideoX)]

* 2D: start time & end time æ„æˆçš„é‚»æ¥çŸ©é˜µ

![image-20220723153302775](https://s2.loli.net/2022/07/23/lUBL6uPHRFCzvxt.png)

* **æ ¸å¿ƒæ€æƒ³**ï¼š

  * ä½¿ç”¨max pooling ï¼ˆæœ¬æ–‡ä½¿ç”¨ï¼‰ æˆ–è€… stack convolutionçš„è·å–moment featureï¼Œå¦‚ä¸Šå›¾`2D Temporal Feature Map Extraction`æ‰€ç¤º
  * ç”±äºè¿™æ ·å­çš„è®¡ç®—å¼€é”€å¤ªå¤§äº†ï¼Œä½¿ç”¨**ç‰¹å®šçš„é‡‡æ ·æ–¹å¼**è¿›è¡Œè°ƒæ•´ï¼Œè¯¦è§è®ºæ–‡ï¼ï¼ˆè·ç¦»è¿‘çš„é‡‡æ ·å¤šä¸€ç‚¹ï¼Œè¿œçš„é‡‡æ ·å°‘ä¸€ç‚¹ï¼‰
  * å¤šæ¨¡æ€èåˆï¼ˆ`Hadamard product`ï¼‰

  $$
  \mathbf{F}=\left\|\left(\mathbf{w}^{S} \cdot \mathbf{f}^{S} \cdot \mathbb{1}^{T}\right) \odot\left(\mathbf{W}^{M} \cdot \mathbf{F}^{M}\right)\right\|_{F}
  $$

  * æŸå¤±è®¡ç®—æ—¶å€™ï¼Œå¯¹`IoU`è¿›è¡Œä¸€ä¸ªscaleå˜æˆç›‘ç£ä¿¡å·

  $$
  y_{i}= \begin{cases}0 & o_{i} \leq t_{\min } \\ \frac{o_{i}-t_{\min }}{t_{\max }-t_{\min }} & t_{\min }<o_{i}<t_{\max } \\ 1 & o_{i} \geq t_{\max }\end{cases}
  $$

  * BCE loss:

  $$
  L o s s=\frac{1}{C} \sum_{i=1}^{C} y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)
  $$

  

:fire: :hammer_and_wrench: **Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2109.04872)] [[torch](https://github.com/MCG-NJU/MMN)] [[blog](https://zhuanlan.zhihu.com/p/446203594)]

* ä¸»å¹²ç½‘ç»œæ˜¯æ²¿ç”¨[TDN](https://arxiv.org/abs/2012.10071)

* ä½¿ç”¨äº†**metric learning**çš„æ–¹æ³•å¹¶ä¸”å¼•å…¥**è´Ÿæ ·æœ¬**æ¥åšTemporal Groundingçš„ä»»åŠ¡

  * è§†é¢‘é—´çš„è´Ÿæ ·æœ¬ï¼ˆ`IoU`æ¥æ ‡å®šç›‘ç£ä¿¡å·`yi`ï¼Œä¸`2D-TAN`ä¸€æ ·å¤„ç†å¾—æ¥çš„ï¼Œè®°å¾—`scale`ä¸€ä¸‹ï¼‰
  * æ–‡æœ¬ä¸­çš„è´Ÿæ ·æœ¬ï¼Œä»å…¶ä»–è§†é¢‘çš„æ–‡æœ¬è¯­å¥å½“ä¸­é€‰å–å‡ºæ¥

* è´¡çŒ®

  * æ„é€ äº†æ–°çš„ç›‘ç£ä¿¡å·ï¼šè§†é¢‘é—´çš„æ­£è´Ÿæ ·æœ¬(`IoU`æ¥é‡‡æ ·)ï¼Œ å¥å­å’Œè§†é¢‘å¯¹åº”çš„æ­£è´Ÿæ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬å¥å­ä»åˆ«çš„è§†é¢‘æŠ½å–è¿‡æ¥ï¼‰
  * ä¸€ä¸ªè§†é¢‘åªéœ€è¦å»ºæ¨¡ä¸€æ¬¡ï¼Œå¤§å¤§èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä»¥å¾€çš„fusionæ–¹æ³•éƒ½æ˜¯è¦æ–‡æœ¬-è§†é¢‘å¸§å¯¹æ¥å»ºæ¨¡

* Trick

  * ä¸ºäº†ç¼–ç å…¬å¹³ï¼Œä½¿ç”¨é¢„è®­ç»ƒå¥½çš„`DistilBERT`æ¥è¿›è¡Œç¼–ç å¥å­

* æŸå¤±å‡½æ•°è®¡ç®—

  * å’Œ`2D-TDN`ä¸€æ ·çš„`BCE_loss`
  * ç±»ä¼¼äº`InfoNCE loss`çš„è®¾è®¡å¯¹æ¯”æŸå¤±

  $$
  \begin{aligned}
  &p\left(i_{s} \mid v\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)}{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)+\sum_{j \neq i}^{N_{s}} \exp \left(\mathbf{f}_{j}^{S T} \mathbf{f}^{V} / \tau_{v}\right)} \\
  &p\left(i_{v} \mid s\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)}{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)+\sum_{j \neq i}^{N_{v}} \exp \left(\mathbf{f}_{j}^{V T} \mathbf{f}^{S} / \tau_{s}\right)} \\
  &L_{m m}=-\left(\sum_{i=1}^{N} \log p\left(i_{v} \mid s_{i}\right)+\sum_{i=1}^{N} \log p\left(i_{s} \mid v_{i}\right)\right)
  \end{aligned}
  $$

* 



:fire: **Event-Guided Procedure Planning from Instructional Videos with Text Supervision**, in ICCV 2023. [[pdf](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Event-Guided_Procedure_Planning_from_Instructional_Videos_with_Text_Supervision_ICCV_2023_paper.pdf)]

* åŠ¨æœº
  * è¿‡å»çš„æ–¹æ³•æ²¡æœ‰å¾ˆå¥½çš„è€ƒè™‘åˆ°äº‹ä»¶ä¿¡æ¯
  * å®é™…ä¸Švisual stateså’Œaction text labelä¹‹é—´æ˜¯æœ‰gapçš„
    * åœ¨æŸäº›labelä¸­å‡ºç°çš„ç‰©å“ï¼Œåœ¨ä¸€å¼€å§‹å’Œç»“æŸæ—¶å€™çš„stateæ˜¯æ²¡æœ‰å‡ºç°çš„

![image-20231120205912793](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231120205912793.png)

![image-20231120205928481](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231120205928481.png)

* æ–¹æ³•ï¼ˆå¢åŠ å¤šä¸€ä¸ªevent-awareçš„lossï¼‰

![image-20231120205959318](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231120205959318.png)

---



### :man_student: Video Question Answer

:fire: :hammer_and_wrench: **Invariant Grounding for Video Question Answering**, in CVPR 2022 Best Paper Finalist.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)] [[torch](https://github.com/yl3800/IGV)]

> è¿™ç¯‡æ–‡ç« æ„Ÿè§‰æ˜¯ä¸€ç¯‡å¾ˆæ ‡å‡†çš„`CVPR`çš„ä¸­è§„ä¸­çŸ©æ–‡ç« ï¼Œå†™ä½œç”¨è¯ä¸Šéå¸¸å‡ºè‰²çš„

* å…ˆåšäº†Groundingçš„æ£€æµ‹ï¼Œæ£€æµ‹å‡ºé—®é¢˜ç›¸å…³å¸§ï¼ˆæœ‰å› æœå…³ç³»`Casual`ï¼‰è¿˜æœ‰æ— å…³å¸§ï¼ˆè¡¥å¿å¸§`Complement`ï¼‰
* æ„å»ºè´Ÿæ ·æœ¬åˆ°æ— å…³å¸§å½“ä¸­ï¼Œä½¿ç”¨`memory bank`æ¥å­˜å‚¨æ‰€æœ‰æ ·æœ¬ (å› æ­¤è¦æ³¨æ„å­˜å‚¨çš„ç‰¹å¾ç»´åº¦ä¸èƒ½å¤ªå¤§)

**Video as Conditional Graph Hierarchy for Multi-Granular Question Answering**ï¼Œin AAAI 2022. [[pdf](https://arxiv.org/abs/2112.06197)] [[torch](https://arxiv.org/abs/2112.06197)]

* ç°æœ‰çš„æ–¹æ³•å¯¹è§†é¢‘é—®é¢˜çš„å›ç­”ç¼ºä¹**å¯è§£é‡Šæ€§**
* æ„å»ºäº†ä¸¤ç§è§†è§’æ¥çœ‹é—®é¢˜
  * bottom-upï¼Œ ä¸åŒçš„è§†é¢‘ç‰¹å¾å†³å®šäº†ä¸åŒçš„å±æ€§levelï¼ˆå®ä½“ï¼ŒåŸå­ï¼ŒåŠ¨ä½œï¼Œäº‹ä»¶ï¼‰
  * up-bottomï¼Œé—®é¢˜ä¸­çš„ä¸åŒå•è¯ï¼Œå…³è”äº†ä¸åŒçš„level
* æ„å»º**å›¾ç¥ç»ç½‘ç»œ**æ¥æ¨¡æ‹Ÿè¿™äº›levelæ€è€ƒçš„è¿‡ç¨‹

![image-20220910231620242](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220910231620242.png)

:fire: :hammer_and_wrench: **[äº¤é€šäº‹æ•…QAæ•°æ®é›†] SUTD-TraffificQA: A Question Answering Benchmark and an Effificient Network for Video Reasoning over Traffific Events**, in CVPR 2021. [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_SUTD-TrafficQA_A_Question_Answering_Benchmark_and_an_Efficient_Network_for_CVPR_2021_paper.pdf)] [[project](https://github.com/SUTDCV/SUTD-TrafficQA[)]

* å¯¹æ¯”èµ·å…¶ä»–QAæ•°æ®é›†
  * éœ€è¦æ¨¡å‹æœ‰å› æœæ¨ç†å’Œè®¤çŸ¥å‘å±•ï¼ˆcognitive developmentï¼‰
* å¤„ç†æ–¹æ³•
  * å¯¹ç²—ç»†ä¸¤ç§ç²’åº¦è¿›è¡Œè¯†åˆ«å’Œè®¡ç®—ï¼ˆä¸åŒçš„CNNç½‘ç»œï¼‰ï¼Œå¤§å¤§åŠ å¿«äº†æ¨¡å‹çš„è¿ç®—æ—¶é—´

![image-20220912152326067](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912152326067.png)

**Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering**, in TPAMI 2022. [[pdf](https://arxiv.org/pdf/2207.12647.pdf)]

> è¿™ç¯‡è®ºæ–‡æ¨¡å‹è¾ƒä¸ºå¤æ‚ï¼Œæ‰€ä»¥è¿™é‡Œåªè®²è¯‰å…¶æ ¸å¿ƒæ€æƒ³

* åŠ¨æœº

  * ç°æœ‰æ–¹æ³•åªå…³æ³¨äº†å¾ˆç®€å•çš„äº‹ä»¶ï¼Œæ¯”å¦‚è¯´çœ‹ç”µå½±ï¼Œæ— æ³•å…³æ³¨çœŸæ­£äº‹ä»¶çº§çš„å› æœå…³ç³»

    ![image-20220912224156198](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224156198.png)

  * è¯­è¨€å’Œå›¾åƒå½“ä¸­çš„å¹²æ‰°å› ç´ ï¼ˆConfounderï¼‰

    * è¿‡äºå…³æ³¨ä¸€äº›æ˜¾å¼çš„ä¸œè¥¿ï¼Œå¿½ç•¥äº†ä¸€äº›å¾ˆé‡è¦çš„ä¸œè¥¿

  ![image-20220912224356724](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224356724.png)

* æ–¹æ³•

![image-20220912224635966](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224635966.png)

* è¯¦ç»†ç»†èŠ‚è§è®ºæ–‡ï¼



:hammer_and_wrench: :fire: **Discovering Spatio-Temporal Rationales for Video Question Answering**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2307.12058)] [[torch](https://github.com/yl3800/TranSTR)]

* åŠ¨æœº
  * è§£å†³é•¿è§†é¢‘å’Œå¤æ‚è§†é¢‘çš„QA
  * é—®é¢˜è¿˜æœ‰è§†é¢‘å¸§å­˜åœ¨å†—ä½™ä¿¡æ¯

![image-20230902182028333](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902182028333.png)

* æ¨¡å‹æ–¹æ³•ï¼ˆ**è‡ªé€‚åº”é‡‡æ ·å¸§**ï¼‰
  * æ ¸å¿ƒå°±æ˜¯æ‰¾åˆ°å…³é”®ä¿¡æ¯ï¼Œä¸è¦å†—ä½™çš„ä¿¡æ¯

![image-20230902182128865](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902182128865.png)

:fire: **Redundancy-aware Transformer for Video Question Answering**, in MM 2023. [[pdf](https://arxiv.org/abs/2308.03267)]

> å’Œä¸Šä¸€ç¯‡è®ºæ–‡åŠ¨æœºå¾ˆç±»ä¼¼çš„ï¼Œéƒ½æ˜¯åŒä¸€ä¸ªä½œè€…ï¼ŒCVPR 22æœ€ä½³è®ºæ–‡å€™é€‰é‚£ä¸ªä½œè€…

* æ¨¡å‹æ–¹æ³•

![image-20230902182900805](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902182900805.png)

:fire: :hammer_and_wrench: **Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2308.08414)] [[torch](https://github.com/XLiu443/Tem-adapter)]

* åŠ¨æœº
  * è®²CLIPçš„å›¾æ–‡é¢„è®­ç»ƒæ–¹æ³•ï¼Œé€‚é…åˆ°VideoQAé¢†åŸŸ
  * ç›´æ¥é€‚é…å­˜åœ¨ä¸¤ä¸ªå¤©ç„¶çš„gap

![image-20230902182553786](/Users/gary/Library/Application Support/typora-user-images/image-20230902182553786.png)

* è¿‡å»çš„æ–¹æ³•

![image-20230902182618433](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902182618433.png)

* æˆ‘ä»¬çš„æ–¹æ³•

![image-20230902182647169](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902182647169.png)



:fire: :hammer_and_wrench: **A-CQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos**, in EMNLP 2023 [[pdf](https://arxiv.org/abs/2311.01620)] [[dataset&code](https: //github.com/PlusLabNLP/acquired)]

* åŠ¨æœº
  * æå‡ºä¸€ä¸ªæ›´åŠ å¤šæ ·åŒ–çš„VideoQAæ•°æ®é›†ï¼Œæ›´å¤šåäº‹å®é—®é¢˜

![image-20231114205907629](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114205907629.png)

* æ•°æ®é›†ä»‹ç»

![image-20231114205924567](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114205924567.png)

---



### :writing_hand: Video Caption

**[Video Caption] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2101.12059)]

:hammer_and_wrench: :fire: **[Video Caption] Robust Change Captioning**, in ICCV 2019. [[pdf](https://arxiv.org/pdf/1901.02527.pdf)] [[torch](https://github.com/Seth-Park/RobustChangeCaptioning)]

* è¾“å…¥ä¸ºå‰åå›¾åƒå¯¹ï¼Œäº”ç§å˜åŒ–ç±»å‹ï¼ˆcolor/material change,adding/dropping/moving an objectï¼‰
* æå‡ºä¸€ä¸ªæœ‰è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[CLEVR-Change](https://cs.stanford.edu/people/jcjohns/clevr/)ï¼ˆ80Kå›¾ç‰‡å¯¹ï¼‰ï¼Œå¹¶åœ¨æ— è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[Spot-the-Diff](https://github.com/harsh19/spot-the-diff)å–å¾—SOTAæ•ˆæœã€‚
* æ¨¡å‹ï¼šDual æ³¨æ„åŠ›ï¼Œ åˆ†è¾¨**è§†ç‚¹å˜åŒ–**ï¼Œ å…¶å®æ˜¯é€šè¿‡è¾“å…¥ä¸¤å¼ å·®ä¸å¤šçš„å›¾ç‰‡ï¼Œæå‰æ ‡å®šå¥½æ•°æ®é›†è·å¾—çš„ï¼Œæœ‰ç‚¹è¢«å‘çš„æ„æ€![image-20220522213419579](https://s2.loli.net/2022/05/22/fiUArgZIjlzw4p1.png)

:hammer_and_wrench: :fire: **[Video Caption] Semantic Grouping Network for Video Captioning**, in AAAI 2021. [[pdf](https://arxiv.org/pdf/2102.00831.pdf)] [[torch](https://github.com/hobincar/SGN)]

![image-20220621204108736](https://s2.loli.net/2022/06/21/DMmzxs7dKwyU6BE.png)

:hammer_and_wrench: :fire: **Hierarchical Context-aware Network for Dense Video Event Captioning**, in ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.156/)] [[torch](https://github.com/KirkGuo/HCN)]

* **å±€éƒ¨ä¿¡æ¯**+**å…¨å±€ä¿¡æ¯**ç»“åˆç”Ÿæˆdense caption ï¼ˆè¾“å…¥åŒ…æ‹¬**video** å’Œ **transcript**ï¼‰
* ä¸ºæ­¤è®¾è®¡äº†ä¸¤å¥—`Attention`æœºåˆ¶
  * falt attention + cross attention

![image-20220820000708776](https://s2.loli.net/2022/08/20/wdO5eWZcxgqsItT.png)

* è®¾è®¡äº†é—¨æœºåˆ¶æ¥decodeï¼ˆä¹‹å‰çš„æ–‡æœ¬ä¿¡æ¯ä¸æœªæ¥çš„æ–‡æœ¬ä¿¡æ¯ï¼‰



```mermaid
  graph LR
  SG(Semantic-Grouping) --å»æ‰å†—ä½™phrase--> ç›¸ä¼¼åº¦è®¡ç®—
  SG --attentionæœºåˆ¶ --> å¯¹å…¶phraseå’Œframe --> åŠ å…¥å¯¹æ¯”æŸå¤±,è®¡ç®—æ²¡æœ‰åŒ…å«negativeçš„æ¦‚ç‡
```

**å¯¹æ¯”æŸå¤±**$\mathcal{L}_{c a}=\sum_{(V, Y) \in \mathcal{D}} \sum_{t} \sum_{i}^{M_{t}}\left(-\log p_{c a}\left(s_{i, t}\right)\right)$, $p_{c a}\left(s_{i, t}\right)=\sum_{j=1}^{N} \alpha_{i, j, t}^{p o s}$    ($\alpha^{pos}$ ä¸ºæ­£æ ·æœ¬æ—¶å€™å¯¹é½æ³¨æ„åŠ›çš„æƒé‡) 



## :eye: Grounding

:fire: **Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model**, in CVPR 2022. [[pdf](https://arxiv.org/abs/2203.14940)] [[torch](https://github.com/dyabel/detpro)]

> å°†CoOPï¼ˆå›¾åƒåˆ†ç±»ï¼‰è®ºæ–‡æ€æƒ³æ”¾å…¥ODå½“ä¸­

* åŠ¨æœº
  * äººå·¥è®¾è®¡Promptè¾ƒä¸ºéº»çƒ¦
  * å‰æ™¯å’ŒèƒŒæ™¯åˆ†ç¦»åœ¨ODä¸­å¾ˆé‡è¦
  * å‰æ™¯çš„ä¸Šä¸‹æ–‡åˆ†çº§ä¹Ÿå¾ˆé‡è¦ï¼ˆé€šè¿‡IOUæ¥åˆ¤å®šï¼‰
* æ¨¡å‹æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰
  * IOUæ¥åˆ†ç†å¤„å‰æ™¯èƒŒæ™¯
  * æŸå¤±å‡½æ•°è®¾è®¡
    * **èƒŒæ™¯çš„æŸå¤±å’Œå“ªä¸ªclasséƒ½ä¸ç›¸ä¼¼**
  * é€šè¿‡IOUæ¥å¯¹å‰æ™¯åˆ†çº§å¤„ç†

* è¿ç»­å‹Promptå­¦ä¹ ï¼š

![image-20230307224847531](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230307224847531.png)

* åµŒå…¥ViLDæ¡†æ¶å½“ä¸­

![image-20230307224949152](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230307224949152.png)



:fire: :hammer_and_wrench: **Multi-Modal Classifiers for Open-Vocabulary Object Detection**, in ICML 2023. [[pdf](https://arxiv.org/abs/2306.05493)] [[code](https://www.robots.ox.ac.uk/~vgg/research/mm-ovod/)]

* åŠ¨æœº
  * è§†è§‰å’Œæ–‡æœ¬æ®µç›¸äº’è¾…åŠ©**å¼€æ”¾è¯è¡¨çš„OD**
  * ç±»ä¼¼ä¸€ä¸ªEmpirical Study

![image-20230616164215541](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230616164215541.png)

* **è§†è§‰å’Œæ–‡æœ¬**ç«¯çš„å¤„ç†

![image-20230616164353443](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230616164353443.png)

##  :apple: Causality Learning

:fire: :star: **Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond**, in TACL 2022. [[pdf](https://arxiv.org/abs/2109.00725)] [[blog](https://github.com/badbadcode/Causality-NLP-Reading-List/blob/master/notes/Feder%20et%20al_2021_Causal%20Inference%20in%20Natural%20Language%20Processing%20-%20Estimation%2C%20Prediction%2C%20Beyond.md)]

* **å…³äºNLP + Causality çš„ç»¼è¿°ï¼Œå†™å¾—éå¸¸è¯¦ç»†éå¸¸å¥½ï¼ï¼ï¼**

:hammer_and_wrench: **CauAIN: Causal Aware Interaction Network for Emotion Recognition in Conversations**, in IJCAI 2022. [[pdf](https://www.ijcai.org/proceedings/2022/0628.pdf)] [[torch](https://github.com/circle-hit/CauAIN)]

> åœ¨å¯¹è¯å½“ä¸­æ‰¾**æƒ…æ„Ÿç›¸å…³çš„å› æœçº¿ç´¢**

* åŠ¨æœºï¼š

  * ç°æœ‰æƒ…æ„Ÿåˆ†æç ”ç©¶ä¸­ï¼Œå¯¹æƒ…æ„Ÿçš„è¯†åˆ«å¾€å¾€ä»…ä¾æ®å½“å‰å¥å­ï¼Œè€Œå¿½ç•¥äº†**å¯¹è¯å†å²ä¸­å­˜åœ¨çš„èƒ½å¤Ÿå¸®åŠ©è¯†åˆ«å½“å‰æƒ…æ„Ÿçš„æ·±å±‚çº¿ç´¢**
    * å¯¹è¯æŸä¸€æ–¹è‡ªèº«çš„è¯è¯­ä¸­ï¼Œå­˜åœ¨å¯é€€é‡Œæƒ…æ„Ÿçš„å› æœè”ç³» **(Intra-cause)**
    * å¯¹è¯åŒæ–¹çš„è¯è¯­ä¸­ï¼Œå­˜åœ¨å¯ä»¥å¸®åŠ©æ¨ç†å¯¹æ–¹æƒ…æ„Ÿçš„äº¤äº’å› æœè”ç³» **(Inter-cause)**
  * ç›®å‰è¿˜æ²¡æœ‰ç›¸å…³çš„æ ‡æ³¨æœ‰æƒ…æ„Ÿçº¿ç´¢çš„æ•°æ®é›†ï¼Œå› æ­¤è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¸¸è¯†çŸ¥è¯†è‡ªä¸»**å¯»æ‰¾çº¿ç´¢**çš„æ–¹æ³•æ¥ç¡®å®šçº¿ç´¢æ‰€åœ¨å¥å­

  ![img](https://img-blog.csdnimg.cn/ad9e9bbada84442795c3953952768e74.png)

* æ–¹æ³•

  * åˆ©ç”¨ ATOMIC å¸¸è¯†çŸ¥è¯†è¯­æ–™åº“ï¼Œè·å¾—å¯¹è¯å†å²æ¯å¥çš„ 6 ç§å› æœçº¿ç´¢

    * 3 ç§æ¥è‡ªè‡ªèº«å› æœçº¿ç´¢ (Intra-cause)ï¼ŒxEffectï¼ŒxReactï¼ŒxWant
    * 3 ç§æ¥è‡ªäº¤äº’å› æœçº¿ç´¢ (Inter-cause)ï¼ŒoEffectï¼ŒoReactï¼ŒoWant

    ![image-20230104121326042](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230104121326042.png)

  * åˆ©ç”¨å› æœçº¿ç´¢ï¼Œå»ºæ¨¡å¯¹è¯å†å²

    * RoBERTa + GRU å»ºæ¨¡å¯¹è¯å†å²æ–‡æœ¬
    * åˆ©ç”¨å› æœçº¿ç´¢ï¼Œè·å–å¯¹è¯å†å²æ¯å¥ä¸å½“å‰å¥æƒ…æ„Ÿçš„ç›¸å…³ç¨‹åº¦åˆ†æ•°
    * åˆ©ç”¨ç›¸å…³ç¨‹åº¦åˆ†æ•°åŠ æƒåçš„å‘é‡è¿›è¡Œåˆ†ç±»è·å–æƒ…æ„Ÿ

    ![image-20230104121406446](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230104121406446.png)

:hammer_and_wrench: **Knowledge-Bridged Causal Interaction Network for Causal Emotion Entailment**, in AAAI 2023. [[pdf](https://arxiv.org/abs/2212.02995)] [[torch](https://github.com/circle-hit/KBCIN)]

> è¿™ä»½å·¥ä½œå’Œä¸Šé¢ğŸ‘†çš„[IJCAI 2022:CauAIN: Causal Aware Interaction Network for Emotion Recognition in Conversations](https://www.ijcai.org/proceedings/2022/0628.pdf)é‚£ä¸ªå·¥ä½œ**ideaæ˜¯ä¸€æ ·**çš„ï¼Œåªä¸è¿‡åšäº†ä¸¤ä¸ªä¸åŒçš„ä»»åŠ¡è€Œå·²

* ä¸»è¦åŠ¨æœºéƒ½æ˜¯æ‰¾å¯»å¯¹è¯å½“ä¸­çš„**å› æœçº¿ç´¢**

  * å¼•å…¥**å¸¸è¯†çŸ¥è¯†**æ¥å¼¥è¡¥gap

  ![image-20230130121710045](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230130121710045.png)

* æ–¹æ³•æ¨¡å‹

  * æ•°æ®æ¥æº ï¼ˆAOTMIC-2020ï¼‰

  ![image-20230130121639558](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230130121639558.png)

  * æ¨¡å‹

  ![image-20230130121841363](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230130121841363.png)

:fire: :hammer_and_wrench: **Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis**, in NAACL 2021. [[pdf](https://aclanthology.org/2021.naacl-main.155/)]  [[torch](https://github.com/xxxiaol/GCI/)]

* åŠ¨æœº
  * å°†å› æœæ¨ç†åº”ç”¨åˆ°éç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®å½“ä¸­ï¼Œå¸®åŠ©æ³•å¾‹ä»ä¸šè€…å†³ç­–
* æ–¹æ³•
  * æ¦‚è§ˆï¼šä»äº‹å®æè¿°æ–‡æœ¬ä¸­è‡ªåŠ¨æ„å»ºå› æœå›¾ï¼Œç”¨å› æœæ¨ç†æ¥è¾…åŠ©æ³•å¾‹å†³ç­–çš„åˆ¶å®šï¼Œæœ¬æ–‡ä¸­similar charge disambiguationä»»åŠ¡ä¸Šæµ‹è¯•äº†è¯¥æ¡†æ¶çš„æ•ˆæœã€‚
    1. ç”¨å…³é”®è¯æŠ½å–ï¼ˆç”¨YAKE+IDFè®¡ç®—å•è¯å¯¹ç½ªåçš„é‡è¦æ€§ï¼‰ï¼Œæ¥è¯†åˆ«å‡ºäº‹å®æè¿°ä¸­çš„key factorsã€‚
    2. å°†ç›¸ä¼¼çš„key factorsèšç±»åˆ°ç»„ä¸­ï¼Œæ¯ä¸ªç»„è§†ä¸ºä¸€ä¸ªç‹¬ç«‹èŠ‚ç‚¹ã€‚ï¼ˆå›¾çš„èŠ‚ç‚¹çš„æ¯ä¸ªç»„å’Œç½ªåï¼‰
    3. ç”¨å¯¹æœªè¯†åˆ«å˜é‡é²æ£’ï¼ˆæ— ç›‘ç£æŠ½å–å¯èƒ½å¯¼è‡´å…³é”®è¯ä¸å®Œå…¨ï¼Œå› æœå‘ç°æ—¶æœ‰æœªè¯†åˆ«confounderï¼‰çš„causal discovery algorithmï¼ˆGreedy Fast Causal Inference (GFCI)ï¼‰æ¥æ„é€ å› æœå›¾ã€‚ï¼ˆè¾“å‡ºæ˜¯Partial Ancestral Graph (PAG)ï¼‰ï¼ˆåœ¨é™„å½•ä¸­å¯ä»¥çœ‹åˆ°ï¼Œè¿™ç§ç®—æ³•èƒ½å¤Ÿè¯†åˆ«å‡ºéšfactorï¼‰ï¼ˆé™åˆ¶ï¼š1. ç¦æ­¢ç½ªåèŠ‚ç‚¹å‡ºè¾¹ã€‚2. ä»¥æ¡ˆä¾‹ï¼ˆäº‹ä»¶æè¿°æ–‡æœ¬ï¼‰çš„æ—¶é—´é¡ºåºæ¥é™åˆ¶å› æœå…³ç³»ï¼‰ï¼ˆæŠ½æ ·å› æœå›¾ï¼‰
    4. ä¼°ç®—æ¯æ¡è¾¹çš„causal strengthæ¥å‡å°‘ä¸å¯é è¾¹çš„å½±å“ã€‚ï¼ˆä¿æŒConfounderä¸å˜ï¼‰ï¼ˆæ–¹æ³•ï¼šAverage Treatment Effect (ATE)ï¼‰ï¼ˆä¼°ç®—ATEçš„æ–¹æ³•ï¼šPropensity Score Matching (PSM) åœ¨treated/untreated groupä¹‹é—´æ„å»ºç›¸ä¼¼æ ·æœ¬å¯¹ï¼‰
    5. å°†**å› æœçŸ¥è¯†ç»“åˆåˆ°NN**ä¸­ï¼šâ‘ åœ¨NN attention weightsä¸ŠåŠ å…¥causal strengthé™åˆ¶ï¼ˆåŠ æŸå¤±å‡½æ•°ï¼‰ã€‚â‘¡åœ¨å› æœå›¾ä¸ŠæŠ½å–å‡ºçš„å› æœé“¾ä¸Šä½¿ç”¨RNNã€‚

![image-20230308160457133](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230308160457133.png)

* ä¸¤ç§æ–¹å¼å°†å› æœæ¨ç†å’Œç¥ç»ç½‘ç»œç›¸ç»“åˆ

![image-20230308160530921](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230308160530921.png)



:hammer_and_wrench: :fire: **Deconfounded Video Moment Retrieval with Causal Intervention**, in SIGIR 2021. [[pdf](https://arxiv.org/abs/2106.01534)] [[torch](https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval)]

* åŠ¨æœº

  * ç›®å‰è§†é¢‘å®šä½å½“ä¸­å­˜åœ¨ä¸€äº›Bias
    * æ•°æ®é›†**é•¿å°¾åˆ†å¸ƒ**
      * æŸäº›QueryåŠ¨ä½œå‡ºç°é¢‘ç‡å¤ªé«˜äº†
    * Queryå’Œlocationsä¹‹é—´å­˜åœ¨**å¤ªå¼ºçš„ä¾èµ–**
      * openå¾€å¾€éƒ½æ˜¯è§†é¢‘å¼€å¤´ï¼Œcloseä¸€èˆ¬å¾€å¾€éƒ½æ˜¯å¯¹åº”è§†é¢‘ç»“æŸ

* æ–¹æ³•

  ![image-20230110165317100](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230110165317100.png)

  * æ¨¡å‹å›¾

  ![image-20230110165441609](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230110165441609.png)

:fire: :hammer_and_wrench: **Interventional Video Grounding with Dual Contrastive Learning**, in CVPR 2021. [[pdf](https://arxiv.org/pdf/2106.11013.pdf)] [[torch](https://github.com/nanguoshun/IVG)] ï¼ˆ2023å¹´1æœˆä»ä¸ºå¼€æºï¼‰

* åŠ¨æœº

  * å’Œä¸Šé¢SIGIRé‚£ä¸€ç¯‡è®ºæ–‡æœ‰ç‚¹ç±»ä¼¼ï¼Œä½†æ˜¯é€šè¿‡lossæ¥è¿›è¡Œå› æœæ¨æ–­
  * è¿‡å»æ¨¡å‹å¾€å¾€åªç†è§£å®ä½“ï¼Œè€Œæ²¡æœ‰å…³æ³¨åˆ°ä¸€äº›åŠ¨ä½œ

  ![image-20230110165932709](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230110165932709.png)

* æ–¹æ³•

  * é€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥è¿›è¡Œæ›´å¥½äº¤äº’

  ![image-20230110170055802](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230110170055802.png)

  * å› æœæ¨ç†æ¨¡å—ï¼ˆé€šè¿‡lossæ¥è°ƒæ•´ï¼‰

    * å› æœå›¾

    ![image-20230110165959351](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230110165959351.png)

    * é€šè¿‡æŠ½å–ä¸‰å…ƒç»„è¿‘ä¼¼$z$ï¼Œå…·ä½“è§è®ºæ–‡
    * lossè®¡ç®—

    ![image-20230110170418014](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230110170418014.png)





:fire: :hammer_and_wrench: **Two Causal Principles for Improving Visual Dialog**, in CVPR 2020. [[pdf](https://arxiv.org/abs/1911.10496)] [[torch](https://github.com/simpleshinobu/visdial-principles)] [[zhihu](https://zhuanlan.zhihu.com/p/363411361)]

> å«è€å¸ˆå›¢é˜Ÿ**ç¬¬ä¸€ç¯‡å› æœæ¨ç†**æ–¹å‘çš„æ–‡ç« 
>
> **ä¸€å¥è¯æ€»ç»“**ï¼š
>
> * ä»å› æœå›¾è§’åº¦å®¡è§†è§†è§‰å¯¹è¯ä»»åŠ¡ï¼Œåˆ‡æ–­å¯¹è¯å†å²ä¸ç­”æ¡ˆçš„ç›´æ¥å› æœæ•ˆåº”ï¼Œæ·»åŠ æ··æ‚å› å­[ç”¨æˆ·åå¥½]æ„å»ºæè¿°çœŸå®ä¸–ç•Œçš„å› æœå›¾ã€‚

* åŠ¨æœº

  * **VisDialæœ¬è´¨ä¸Šå¹¶éå¸¦æœ‰Historyçš„VQA**ã€‚ä½œè€…ä»¥å› æœæ¨æ–­çš„è§’åº¦å…¥æ‰‹ï¼Œå¼•å…¥ä¸¤æ¡å› æœåŸåˆ™ï¼Œå°†æ‰€æœ‰çš„VisDialåŸºçº¿æ¨¡å‹æå‡åˆ°äº†SOTAã€‚

* ä¸¤ä¸ªåŸåˆ™

  * $P_1$: `H`å¯¹è¯å†å²è®°å½•ï¼Œä¸åº”è¯¥ç›´æ¥å½±å“`A`

    ![image-20221124113615064](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221124113615064.png)

  * $P_2$: **ç”¨æˆ·åå¥½**ä¼šå¯¼è‡´å›ç­”çš„ä¸ä¸€æ ·

    ![image-20221124113806608](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221124113806608.png)

    * ç”±äºç”¨æˆ·åå¥½ä¸å¯è§‚æµ‹ï¼Œå› æ­¤ä½œè€…é€šè¿‡ä¸‰ç§æ–¹å¼æ¥**è¿‘ä¼¼é‡‡æ ·**ã€‚

      ![image-20221124114039418](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221124114039418.png)

  * æ„å»ºä¸¤ä¸ªåŸåˆ™çš„**å› æœå›¾**ï¼š

    ![image-20221124113458538](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221124113458538.png)

* å…·ä½“åé—¨è°ƒæ•´åŠå…¶æ¦‚ç‡è¿ç®—å…¬å¼ï¼Œ**è¯¦è§è®ºæ–‡åŠå…¶é™„å½•**

![image-20221124114137317](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221124114137317.png)

![image-20221124114218219](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221124114218219.png)

:fire: :hammer_and_wrench: **[å› æœå…³ç³»] Visual Commonsense R-CNN**, in CVPR 2020. [[pdf](https://arxiv.org/abs/2002.12204)] [[torch](https://github.com/Wangt-CN/VC-R-CNN)] [[blog](https://zhuanlan.zhihu.com/p/111306353)]

> å‡ºè‡ª[MReal](https://mreallab.github.io/)ï¼Œ å¼ å«æœ›è€å¸ˆå›¢é˜Ÿçš„å·¥ä½œï¼Œéå¸¸Solidçš„ä¸€ç¯‡å·¥ä½œ
>
> * ç›®æ ‡æ˜¯è®­ç»ƒåŸºäº`Faster-RCNN`è®­ç»ƒä¸€ä¸ªæ›´å¼ºçš„`feature extractor`å¯ä»¥æ•è·è§†è§‰ä¸Šçš„å¸¸è¯†ä¿¡æ¯ã€‚
> * è¿™ç¯‡è®ºæ–‡å®åœ¨**å¤ªå¤šç»†èŠ‚å’Œæ¨ç†**äº†ï¼Œå»ºè®®çœ‹æˆ‘è‡ªå·±çš„**GoodNoteä¸Šçš„ç¬”è®°**ï¼

* åŠ¨æœº

  * ç°åœ¨çš„æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°è§†è§‰å¸¸è¯†ï¼ˆ**Commonsense**ï¼‰ï¼šäººå’Œæ¤…å­ -> äººå¯ä»¥ååœ¨æ¤…å­ä¸Šã€‚ä½†åœ¨NLPä¸­ï¼Œå¸¸è¯†çš„ä¿¡æ¯å·²ç»æ”¾åœ¨ç‰¹å¾é‡Œé¢äº†

    ![image-20220913155431514](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220913155431514.png)

  * æ•°æ®é›†çš„åå·®ä¼šå¯¼è‡´æ— æ³•æ•æ‰åˆ°å¸¸è¯†ä¿¡æ¯

    * çœŸæ­£çš„**è§†è§‰å…³ç³»**æ— æ³•æè¿°ï¼ˆå·¦å›¾ï¼‰
    * ç»™å‡ºçš„**è§£é‡Š**ä¸å¤Ÿæ­£ç¡®ï¼ˆå³å›¾ï¼‰

  **å› æœç†è®ºå°±æ˜¯ç”¨æ¥å‘ç°==ç°è±¡èƒŒåçš„ä¸å˜è§„å¾‹==çš„ï¼Œæ˜¯ä¸€ç§é²æ£’çš„é¢„æµ‹ã€‚è¿™ä¸å¸¸è¯†æœ¬èº«ä¸å°±å¾ˆç›¸ä¼¼å—ï¼Œæˆ‘ä»¬äººç±»ä¹Ÿæ˜¯ä»ç”Ÿæ´»ä¸­ä¸æ–­æ€»ç»“ç§¯ç´¯è¿™äº›ä¸å˜çš„ã€é²æ£’çš„ç»éªŒæˆ–è€…å› æœè§„å¾‹ï¼Œå¹¶æŠŠä»–ä»¬å«åšå¸¸è¯†ã€‚** æ¯”å¦‚ï¼Œçœ‹è§å‡³å­çŸ¥é“å¯ä»¥åï¼Œçœ‹è§pizzaçŸ¥é“å¯ä»¥åƒã€‚

**Association å’Œ Interventionï¼ˆåˆ†å±‚ï¼‰çš„è®¡ç®—**
$$
\begin{gathered}
P(Y \mid X)=\sum_z P(Y \mid X, z) P(z \mid X)=\frac{P(Y, X)}{P(X)} \\
P(Y \mid d o(X))=\sum_z P(Y \mid X, z) P(z)=\sum_z \frac{P(Y, X, z) P(z)}{P(X, z)}
\end{gathered}
$$
å…¶ä¸­ $X, Y, z$åˆ†åˆ«ä»£è¡¨äº†å›¾ç‰‡ä¸­çš„object labelï¼ŒåŒæ—¶è¿™é‡Œæˆ‘ä»¬ç”¨ç‰©ä½“å‡ºç°çš„é¢‘ç‡æ¥ä»£æ›¿æ¦‚ç‡ï¼Œæ¯”å¦‚ $P(Sink|Hair drier)$å°±æ˜¯ç”¨â€œå«æœ‰$Sink$å’Œ$Hair drier$ä¸¤è€…çš„å›¾ç‰‡æ•°â€æ¯”ä¸Šâ€œåªå«æœ‰Hair drierçš„å›¾ç‰‡æ•°â€è®¡ç®—å¾—åˆ°çš„ã€‚ç”»å‡ºä¸¤è€…è®¡ç®—ç»“æœå·®å¼‚çš„å¯¹æ¯”å›¾ï¼ˆåªæ ‡æ˜äº†20ç±»ï¼‰ï¼š

![img](https://pic2.zhimg.com/80/v2-ddffe6ddaaf70839faa1d62a9ef25291_720w.jpg)

* ä¸¤ä¸ªCaseçš„åˆ†æ

  * $Sink å’Œ drier$ï¼Œæƒ³è¦æ¢å¯»åœ¨**å·²çŸ¥å¹é£æœº**çš„æƒ…å†µä¸‹ï¼Œå»é¢„æµ‹æ°´æ± çš„å¯èƒ½æ€§å¤§å° $P(Sink|drier)$
    * **åœºæ™¯å› ç´ **è€ƒè™‘åœ¨å†…ï¼Œå¯¹ä¸åŒçš„åœºæ™¯è¿›è¡Œåˆ†å±‚ï¼ˆå› ä¸ºåœºæ™¯å°±æ˜¯ç”±objectç»„æˆçš„ï¼‰ï¼Œå¾—åˆ°å®é™…çš„å› æœæ•ˆåº”ï¼Œæ¯”å•çº¯Associationç®—çš„æ•°å€¼è¦ä½
  * äººå’Œé©¬æ¡¶ï¼Œæ¢å¯»â€œé©¬æ¡¶â€å’Œâ€œäººâ€ä¹‹é—´å¯èƒ½å­˜åœ¨çš„å› æœæ•ˆåº”
    * æ•°æ®é›†ä¸­äººå’Œé©¬æ¡¶ä¸€èµ·å‡ºç°çš„æ ·æœ¬å…¶å®ä¸å¤šï¼ˆä¹Ÿä¸ä¼šæœ‰å¾ˆå¤šäººåœ¨é©¬æ¡¶æ—è¾¹æ‹ç…§ï¼‰
    * å¦‚æœæƒ³è¦åšå‡ºæ›´robustçš„é¢„æµ‹ï¼Œæˆ‘ä»¬å°±éœ€è¦è€ƒè™‘æ··æ‚å› å­**confounder**ï¼Œ æ¯”å¦‚ç“¶å­ã€æ°´æ± ã€æ¯å­ç­‰ç­‰ã€‚æŒ‰ç…§confounder è¡Œ**åˆ†å±‚è®¡ç®—**ï¼Œæœ€åå†åŠ æƒæ±‚å’Œã€‚

* æ–¹æ³•ï¼ˆå› æœå¹²é¢„**Intervention**ï¼‰

  * ä»£ç†ä»»åŠ¡ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ï¼š**ç»™å®šRoI Xçš„featureå»é¢„æµ‹RoI Yçš„ç±»åˆ«**

  * åŒ…æ‹¬å¾ˆå¤šæ½œåœ¨çš„**æ··æ‚å› å­**ï¼Œå¦‚æœç›´æ¥é¢„æµ‹å‘¨å›´ç‰©ä½“Yå°±ä¸å¯é¿å…çš„ä¼šè¢«ä¸Šæ–‡æåˆ°çš„æ··æ‚å› å­**confounder**æ‰€å½±å“ã€‚æ ¹æ®æˆ‘ä»¬åˆšåˆšä»‹ç»çš„**â€œdoç®—å­â€**çš„ç†è®ºï¼Œè§£å†³çš„åŠæ³•ä¹Ÿä¸éš¾ï¼Œåªè¦èƒ½æ‰¾åˆ°confounderç„¶åå¯¹ä»–ä»¬ä½¿ç”¨**backdoorç†è®º**è¿›è¡Œæ§åˆ¶å³å¯ã€‚

  * æ··æ‚å› å­æ˜¯ä»€ä¹ˆï¼Ÿ æˆ‘ä»¬ç›´æ¥æŠŠæ•´ä¸ªæ•°æ®é›†ä¸Šçš„**object RoIç‰¹å¾ï¼ˆFaster RCNNä¸­æ¥ï¼‰åœ¨æ¯ä¸ªç±»åˆ«ä¸Šå–å¹³å‡**ï¼Œå½“ä½œè¿™ä¸ªç±»åˆ«çš„è¡¨ç¤ºï¼Œè¿›è€Œæ„å»ºå‡ºä¸€ä¸ª **ç±»åˆ«æ•°x1024** çš„confounderå­—å…¸ä½œä¸º$Z$ï¼ˆæ¯”å¦‚MSCOCOæœ‰80ç±»ï¼Œå°±æ˜¯ 80x1024ï¼‰ï¼Œå®ƒåŒ…å«ç€æ‰€æœ‰å¯èƒ½çš„æ··æ‚å› å­ã€‚

  * åé—¨è°ƒæ•´

    ![img](https://raw.githubusercontent.com/Gary-code/pic/main/img/v2-514061ff24e803c016324ead8bcf84b1_720w.jpg)

    * æˆ‘ä»¬æŠŠconfounder dictionary $Z$ä¸­çš„ç‰©ä½“$z_i$â€œborrowâ€åˆ°å½“å‰å›¾ç‰‡ä¸­ï¼Œæ³¨æ„è¿™é‡Œçš„ç‰©ä½“$z_i$ä¸éœ€è¦æ˜¯å½“å‰å›¾ç‰‡ä¸­å­˜åœ¨çš„ï¼Œæ‰€ä»¥æ˜¯ä¸€ç§globalå±‚é¢çš„å®šä¹‰ã€‚
    * ç„¶åæŠŠå€Ÿæ¥çš„$z_i$â€œputâ€åˆ°$X, Y$å‘¨å›´å’Œ$X, Y$å¯¹æ¯”ï¼Œä¾‹å¦‚ä¸Šå›¾ä¸­çš„æŠŠ sinkã€handbagã€chairç­‰ç­‰ç§»åˆ° toilet å’Œ person å‘¨å›´è¿›è¡Œbackdoorçš„è®¡ç®—ã€‚

  * æ¨¡å‹

    * æ•´ä¸ªinterventionæ•´åˆæˆä¸€è·¯context predictorã€‚
    * åŒæ—¶ä¸ºäº†ä¸è®©ç½‘ç»œå¿˜æ‰è¯†åˆ«RoIæœ¬èº«ç±»åˆ«çš„èƒ½åŠ›ï¼Œcontext predictorçš„åŸºç¡€ä¸Šåˆä¿ç•™äº†åŸå…ˆçš„è‡ªèº«ç±»åˆ«é¢„æµ‹**self predictor**ã€‚

  ![img](https://pic2.zhimg.com/80/v2-d3a05b26274f54a8bd785209f1b6a4c1_720w.jpg)

  æ³¨æ„ï¼šVC R-CNNçš„å®ç°å’ŒåŸå…ˆçš„Faster R-CNNç›¸æ¯”ï¼Œ**å»é™¤äº†RPNç½‘ç»œ**ï¼ˆRegion Proposal Networkï¼‰ï¼Œä¸å†è®­ç»ƒç½‘ç»œproposeè¾¹ç•Œæ¡†ï¼Œè€Œæ˜¯ç›´æ¥å°†æ•°æ®é›†**ground-truthçš„bounding boxåæ ‡è¾“å…¥åˆ°å…¶ä¸­**ï¼Œç›´æ¥æå–regionçš„ç‰¹å¾ã€‚è€Œåœ¨è®­ç»ƒå®Œæˆåçš„featureæå–é˜¶æ®µï¼Œç›¸å¯¹åº”çš„ï¼Œåªè¦ç»™å®šå›¾ç‰‡å’Œbounding boxåæ ‡ï¼Œéƒ½å¯ä»¥è·å¾—å¯¹åº”çš„VCç‰¹å¾ã€‚å°±è¿™æ ·ï¼Œæˆ‘ä»¬åˆ©ç”¨bottomupç‰¹å¾å·²æœ‰çš„è¾¹ç•Œæ¡†åæ ‡æå–VCç‰¹å¾åï¼Œå°†å…¶å¹¶åœ¨å…ˆå‰çš„bottomupç‰¹å¾ä¸Šä½œä¸ºæ–°çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¼ ç»Ÿçš„ Vision&Language ä¸‰å¤§ä»»åŠ¡ä¸ŠæŒ‘é€‰äº†ç»å…¸modelå’ŒSOTA modelè¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°åœ¨å„ä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜æ˜¾çš„æå‡ï¼Œå°¤å…¶æ˜¯åœ¨image captioningä¸Šçš„æå‡å°¤å…¶å¤§ã€‚åŒæ—¶ä¸ºäº†éªŒè¯æ€§èƒ½çš„æå‡ä¸æ˜¯ç”±äºå‚æ•°å¢å¤šå¸¦æ¥çš„ï¼Œæˆ‘ä»¬è¿˜åœ¨åŸæœ‰ç‰¹å¾ä¸Šå¹¶ä¸Šäº†ablativeçš„ç‰¹å¾ï¼ˆå•ç‹¬objectç‰¹å¾ï¼Œç”¨correlationè®¡ç®—çš„ç‰¹å¾ï¼‰ï¼Œå…·ä½“å¯ä»¥å‚è€ƒè®ºæ–‡çš„å®éªŒéƒ¨åˆ†ã€‚



:hammer_and_wrench: **[æŒ‡ä»£è¡¨è¾¾] Deconfounded Visual Grounding**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2112.15324)] [[torch](https://github.com/JianqiangH/Deconfounded_VG)] (2023.1.17æœªå¼€æº)

* åŠ¨æœº

  * visual groundingä»»åŠ¡è¿‡å»å­˜åœ¨biasï¼Œæ¯”å¦‚è¯´ï¼Œç¾Šå¾€å¾€éƒ½æ˜¯å‡ºç°åœ¨å›¾ç‰‡ä¸­é—´

  ![](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230117235029986.png)

  * æ–¹æ³•

    * æ„å»ºå› æœå›¾

      ![image-20230117235128346](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230117235128346.png)

      * é€šè¿‡åé—¨è·¯å¾„å¯ä»¥è§‚å¯Ÿï¼Œä¸»è¦biasæ¥æºä¸`R`è¯­è¨€ä¿¡æ¯ï¼Œå› ä¸ºå…¶å®è§†è§‰ä¸Šçš„biaså…¶ä»–ä¸€äº›æ–¹å‘è®ºæ–‡éƒ½æœ‰æ‰€è§£å†³äº†

    * ç”±äº$G$ä¸å¯è§‚æµ‹ï¼Œå› æ­¤éœ€è¦**è¿‘ä¼¼é‡‡æ ·**

      * é€šè¿‡**ç”Ÿæˆå¼æ¨¡å‹é‡‡æ ·**ï¼Œè¿™é‡Œæœ‰å¾ˆä¸°å¯Œçš„ç†è®ºä¾æ®å…·ä½“è§è®ºæ–‡ã€‚$\hat{G}=Enc(R), R'=Dec(\hat{G})$
      * é€šè¿‡å› æœå¹²é¢„å¾—åˆ°unbiasçš„$R'$å³å¯
      * æ¨¡å‹å›¾

      ![image-20230117235726811](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230117235726811.png)

:fire: :hammer_and_wrench:  **Unbiased Scene Graph Generation from Biased Training**, in CVPR 2020. [[pdf](https://arxiv.org/abs/2002.11949)] [[torch](https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch)] [[zhihu](https://zhuanlan.zhihu.com/p/109657521)]

* åŠ¨æœº

  * æœŸæœ›ä»**æœ‰åè§çš„è®­ç»ƒä¸­ï¼ˆBiased Trainingï¼‰åˆ©ç”¨æ— åé¢„æµ‹è·å¾—æ— åè§çš„åœºæ™¯å›¾ï¼ˆUnbiased Scene Graphï¼‰**
  * ç°æœ‰æ–¹æ³•é—®é¢˜
    * human **walk on/ sit on/ lay on** beachç­‰åŒ…å«ä¸°å¯Œä¿¡æ¯çš„è°“è¯­ç®€å•â€œæ¦‚æ‹¬â€ä¸ºhuman **on** beachæˆ–å°†**behind/ in front of**â€œæ¦‚æ‹¬â€ä¸º**near**
    * æ— æ³•å¾ˆå¥½åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸Šé¢

* è´¡çŒ®

  * è®¾è®¡äº†ä¸€ä¸ªæ— åé¢„æµ‹çš„æ¨ç†ç®—æ³•**Causal TDE Inference**ï¼ˆéè®­ç»ƒæ–¹æ³•ï¼Œæ¨¡å‹â€œä¸å¯è§â€ï¼Œé€‚ç”¨äºä»»ä½•SGGæ¨¡å‹ï¼‰
  * è®¾è®¡äº†ä¸€ä¸ªæ–°çš„é€šç”¨SGGæ¡†æ¶**[Scene-Graph-Benchmark.pytorch](https://link.zhihu.com/?target=https%3A//github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch)**ï¼Œå…¶ä½¿ç”¨[maskrcnn-benchmark](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/maskrcnn-benchmark)è¿›è¡Œåº•å±‚ç›®æ ‡æ£€æµ‹ï¼Œé›†æˆäº†ç›®å‰æœ€å…¨çš„SG metricsï¼ˆåŒ…æ‹¬Recallã€Mean Recallã€No Graph Constraint Recallã€Zero Shot Recallç­‰ï¼‰ã€‚è¯¥æ¡†æ¶æä¾›é‡å†™çš„å„ç§SGG baselineæ¨¡å‹ï¼ˆå¦‚MOTIFSã€VTranEã€VCTreeï¼‰ï¼Œæœ‰ç€å½“ä¹‹æ— æ„§çš„State-of-The-Art SGClså’ŒSGGenç»“æœã€‚

  ![img](https://pic4.zhimg.com/80/v2-f0657ac8afb0b8fbcbe0fcf65e4944f3_720w.webp)

* é—®é¢˜å®šä¹‰ï¼ˆ**æœ‰åè§çš„æ•°æ®æ ‡æ³¨çš„åŸå› **ï¼‰

  * **the long-tail theoryï¼šperson** carry bagç¡®å®æ¯”**dog** carry bagçš„æ•°é‡å¤š
  * **bounded rationalityï¼ˆæœ‰é™ç†æ€§ï¼‰ï¼š**åœ¨äººç±»æ ‡æ³¨å…³ç³»æ—¶ï¼Œæ›´å€¾å‘äºæ ‡æ³¨ç®€å•çš„å…³ç³»ï¼Œå³æ ‡æ³¨person **beside** tableè€Œä¸æ˜¯person **eating on** tableï¼ˆ**å…³ç³»ä¸å¤Ÿå‡†ç¡®å±‚é¢**ï¼‰
  * **language or reporting biasï¼š**æˆ‘ä»¬æ›´å–œæ¬¢è¯´person **on** bikeï¼Œè€Œä¸æ˜¯person **ride on** bikeï¼ˆ**åªæ˜¯ç®€å•æè¿°äº†çœŸå®äº‹ä»¶çš„ä¸€éƒ¨åˆ†**ï¼‰

* æ— åçš„æ€æƒ³ï¼ˆ**contentï¼šå†…å› ï¼Œcontextï¼šå¤–å› **ï¼‰

  * **äººç±»**åœ¨æœ‰åè§çš„å¤§è‡ªç„¶ä¸­ç”Ÿé•¿ï¼Œåœ¨**æ‹¥æŠ±å¥½çš„contextçš„åŒæ—¶ï¼Œé¿å…ä¸å¥½çš„context**ï¼Œå¹¶ä¸contentä¸€èµ·åšå‡ºæ— åè§çš„å†³å®šã€‚

  * å…¶æ½œåœ¨çš„æœºåˆ¶æ˜¯**åŸºäºå› æœå…³ç³»çš„ï¼ˆcausality-basedï¼‰ï¼š**å†³ç­–æ˜¯é€šè¿‡è¿½æ±‚ç”±**contentå¼•èµ·çš„ä¸»è¦å› æœæ•ˆåº”**ï¼Œè€Œä¸æ˜¯è¿½æ±‚ç”±**contextå¼•èµ·çš„å‰¯ä½œç”¨**æ¥åšå‡ºçš„ã€‚ç„¶è€Œï¼Œ**æœºå™¨**æ˜¯åŸºäºå¯èƒ½æ€§çš„ï¼ˆlikelihood-basedï¼‰ï¼Œä¼šäº§ç”Ÿæœ‰åç»“æœã€‚

  * æ•…è®ºæ–‡è®¤ä¸ºï¼Œæ— åé¢„æµ‹çš„å…³é”®æ˜¯æ•™ä¼šæœºå™¨å¦‚ä½•åŒºåˆ†ä¸»è¦ä½œç”¨ï¼ˆmain effectï¼‰å’Œå‰¯ä½œç”¨

    * **content**ï¼šobjectå’Œsubjectçš„**visual features**
    * **context**ï¼šobject-subject union regionsçš„**visual features**ä»¥åŠobjectã€subjectçš„**ç±»åˆ«æ ‡ç­¾**

  * ä¸ºäº†åœ¨æ— åé¢„æµ‹ä¸­è¿½æ±‚ä¸»è¦ä½œç”¨ï¼Œè®ºæ–‡æå‡ºèµ‹äºˆæœºå™¨**åäº‹å®æ€ç»´ï¼ˆcounterfactual thinkingï¼‰:**
    ***If i had not seen** the content, would I still make the same prediction?*

  * **åäº‹å®æ€ç»´**ï¼š**äº‹å®ä¸åäº‹å®ä¹‹é—´çš„æ¯”è¾ƒï¼Œå°†ä¼šè‡ªç„¶åœ°æ¶ˆé™¤contextåå·®çš„å½±å“ï¼Œå› ä¸ºcontextæ˜¯ä¸¤è€…ä¹‹é—´å”¯ä¸€ä¸å˜çš„ä¸œè¥¿**ã€‚

  * å¦‚å›¾ï¼Œå·¦ä¾§å›¾ç‰‡æ˜¯æ‰€è°“çš„**äº‹å®åœºæ™¯**ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯**åŸå§‹åœºæ™¯**ï¼›å³ä¾§å›¾ç‰‡æ˜¯**åäº‹å®åœºæ™¯**ï¼Œå°±æ˜¯**å°†åŸå§‹åœºæ™¯ä¸­contentï¼ˆç‹—å’Œå†²æµªæ¿çš„è§†è§‰ç‰¹å¾ï¼‰å»é™¤ï¼Œå…¶ä»–éƒ¨åˆ†ï¼ˆå¦‚sceneå’Œobject classesï¼‰ä¿æŒä¸å˜ï¼Œå°±åƒobjectçš„è§†è§‰ç‰¹å¾ä»æœªå‡ºç°è¿‡ã€‚**é€šè¿‡è¿™ä¸¤è€…çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºå…³ç³»çš„ä¸»è¦è§†è§‰å½±å“ï¼ŒåŒæ—¶ä¹Ÿä¸ä¸¢å¤±contextã€‚

     ![img](https://pic2.zhimg.com/80/v2-d811612b212103ed4852016136302139_720w.webp)

* æ–¹æ³•

  * **æœ‰å&æ— å**è®­ç»ƒæ¶æ„

  ![image-20221009153731849](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221009153731849.png)

  * æ— åè§+åäº‹å®çš„å› æœå›¾æ„å»º

  ![image-20221009153828669](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221009153828669.png)

TDEï¼ˆTotal Direct Effectï¼‰æ–¹æ³•**æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„å‚æ•°**ï¼Œä¹Ÿå¯ä»¥è¯´æ²¡æœ‰é’ˆå¯¹æ¨¡å‹çš„æœ‰åè®­ç»ƒè¿›è¡Œä»»ä½•æ”¹åŠ¨ï¼Œå…¶ä½¿ç”¨åŸå§‹SGGæ¨¡å‹è¿›è¡Œäº†ä¸¤æ¬¡é¢„æµ‹ï¼Œå°†ä¸¤æ¬¡é¢„æµ‹çš„ç»“æœè¿›è¡Œ**å·®å€¼è¿ç®—**ï¼Œæœ€ç»ˆå¾—åˆ°æ— åè§çš„é¢„æµ‹ã€‚æ‰€ä»¥**TDEæ–¹æ³•æ˜¯æ¨¡å‹â€œä¸å¯è§â€çš„ï¼Œå¹¿æ³›é€‚ç”¨äºå„ç§SGGæ¨¡å‹**ã€‚



:fire: :hammer_and_wrench: **Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect**, in NIPS 2020. [[pdf](https://arxiv.org/abs/2009.12991)] [[torch](https://github.com/KaihuaTang/Long-Tailed-Recognition.pytorch)] [[zhihu](https://zhuanlan.zhihu.com/p/259569655)]

* åŠ¨æœº

  * æå‡ºäº†ä¸€ç§å´­æ–°çš„**é•¿å°¾é—®é¢˜**çš„é€šç”¨è§£å†³æ€è·¯ã€‚è€Œä¸”**å®ç°éå¸¸ç®€å•**ï¼Œèƒ½å¤Ÿå¹¿æ³›é€‚ç”¨äºå„ç§**ä¸åŒç±»å‹çš„ä»»åŠ¡**
  * è¿‡å»è§£å†³é•¿å°¾åˆ†å¸ƒçš„æ–¹æ³•**å­˜åœ¨çš„é—®é¢˜**
    * ã€**æœªåœå…ˆçŸ¥**ã€‘è™½ç„¶åˆ©ç”¨æ•°æ®é›†åˆ†å¸ƒçš„**re-sampling**å’Œ**re-weighting**è®­ç»ƒæ–¹æ³•å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£é•¿å°¾åˆ†å¸ƒçš„é—®é¢˜ã€‚ç„¶è€Œè¿™ç§åˆ©ç”¨å…¶å®æ˜¯è¿èƒŒç°å®å­¦ä¹ åœºæ™¯çš„ï¼Œä»–ä»¬éƒ½éœ€è¦åœ¨è®­ç»ƒ/å­¦ä¹ ä¹‹å‰ï¼Œäº†è§£â€œæœªæ¥â€å°†è¦çœ‹åˆ°çš„æ•°æ®åˆ†å¸ƒï¼Œè¿™æ˜¾ç„¶ä¸ç¬¦åˆäººç±»çš„å­¦ä¹ æ¨¡å¼ï¼Œä¹Ÿå› æ­¤æ— æ³•é€‚ç”¨äºå„ç§åŠ¨æ€çš„æ•°æ®æµã€‚
    * ã€**ä¸¤é˜¶æ®µ**ï¼Œä¸æ˜¯end-to-endã€‘ç›®å‰é•¿å°¾åˆ†ç±»æœ€ä¼˜çš„[Decoupling](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1910.09217)ç®—æ³•ä¾èµ–äº**2-stage**çš„åˆ†æ­¥è®­ç»ƒï¼Œè¿™æ˜¾ç„¶ä¸ç¬¦åˆæ·±åº¦å­¦ä¹ end-to-endçš„è®­ç»ƒä¼ ç»Ÿï¼Œè€Œè®ºæ–‡æœ¬èº«ä¹Ÿæ²¡æœ‰æå‡ºè®©äººä¿¡æœçš„ç†ç”±è§£é‡Š**ä¸ºä»€ä¹ˆç‰¹å¾æå–backboneéœ€è¦åœ¨é•¿å°¾åˆ†å¸ƒä¸‹å­¦**ï¼Œè€Œååclassifieråˆéœ€è¦re-balancingçš„å­¦ã€‚
    * ã€**è¿ç§»èƒ½åŠ›**ä¸è¶³ã€‘é•¿å°¾åˆ†å¸ƒä¸‹ç®€å•çš„å›¾ç‰‡åˆ†ç±»é—®é¢˜å’Œå…¶ä»–å¤æ‚é—®é¢˜ï¼ˆè¯¸å¦‚ç‰©ä½“æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ï¼‰ç ”ç©¶çš„å‰²è£‚ï¼Œç›®å‰é•¿å°¾åˆ†å¸ƒä¸‹å›¾ç‰‡åˆ†ç±»é—®é¢˜çš„ç®—æ³•æ—¥è¶‹å¤æ‚ï¼Œ**å¯¼è‡´å¾ˆéš¾è¿ç”¨äºæœ¬æ¥æ¡†æ¶å°±å¾ˆç¹ççš„æ£€æµ‹åˆ†å‰²ç­‰ä»»åŠ¡**ã€‚è€Œæˆ‘è§‰å¾—é•¿å°¾é—®é¢˜çš„æœ¬è´¨éƒ½æ˜¯ç›¸ä¼¼çš„ï¼ŒçœŸæ­£çš„è§£å†³æ–¹æ¡ˆä¸€å®šæ˜¯ç®€æ´çš„ï¼Œå¯ä»¥é€šç”¨çš„ã€‚
  * **åŸºäºä¸Šé¢è¿™äº›é—®é¢˜ï¼Œä¹Ÿå°±æœ€ç»ˆè¯ç”Ÿäº†æˆ‘ä»¬çš„è¿™ç¯‡å·¥ä½œã€‚æˆ‘ä»¬æå‡ºçš„[De-confound-TDE](https://link.zhihu.com/?target=https%3A//kaihuatang.github.io/Files/long-tail.pdf)çš„ä¼˜åŠ¿å¦‚ä¸‹ï¼š**
    1. æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹**å®Œå…¨ä¸ä¾èµ–äºæå‰è·å–çš„æ•°æ®åˆ†å¸ƒ**ï¼Œåªéœ€è¦åœ¨ä¼ ç»Ÿè®­ç»ƒæ¡†æ¶çš„åŸºç¡€ä¸Šç»Ÿè®¡ä¸€ä¸ªç‰¹å¾çš„ç§»åŠ¨å¹³å‡å‘é‡ï¼Œå¹¶ä¸”è¿™ä¸ªå¹³å‡ç‰¹å¾åœ¨è®­ç»ƒä¸­å¹¶ä¸ä¼šå‚ä¸æ¢¯åº¦è®¡ç®—ï¼ˆåªåœ¨æµ‹è¯•æ—¶ä½¿ç”¨ï¼‰ã€‚è¿™ä¹Ÿå°±è§£å†³äº†ä¼ ç»Ÿé•¿å°¾åˆ†ç±»æ–¹æ³•ä¾èµ–â€œæå‰è·å–æœªæ¥æ•°æ®åˆ†å¸ƒâ€çš„é—®é¢˜ã€‚
    2. å°½ç®¡æˆ‘ä»¬çš„æµ‹è¯•è¿‡ç¨‹å’Œè®­ç»ƒè¿‡ç¨‹æœ‰æ‰€ä¸åŒï¼Œä½†æˆ‘ä»¬çš„**æ¨¡å‹æ˜¯ä¸€æ¬¡è®­ç»ƒåˆ°ä½çš„**ï¼Œå¹¶ä¸éœ€è¦ä¾èµ–ç¹ççš„å¤šæ­¥è®­ç»ƒï¼Œè¿™å¤§å¤§ç®€åŒ–äº†æ‹“å±•è‡³å…¶ä»–ä»»åŠ¡æ—¶çš„ä¿®æ”¹æˆæœ¬ã€‚
    3. å¹¶ä¸”ï¼Œæˆ‘ä»¬æˆåŠŸçš„å°†è¿™ä¸ªæ–¹æ³•è¿ç”¨äºå›¾ç‰‡åˆ†ç±»ï¼ˆImageNet-LTï¼ŒLong-tailed CIFAR-10/-100ï¼‰å’Œç‰©ä½“æ£€æµ‹/å®ä¾‹åˆ†å‰²ï¼ˆLVIS datasetï¼‰ç­‰å¤šä¸ªä»»åŠ¡ï¼Œå‡å–å¾—äº†æœ€ä¼˜çš„ç»“æœï¼ˆæˆªæ­¢è‡³æˆ‘ä»¬æŠ•ç¨¿ä¹Ÿå°±æ˜¯2020å¹´5æœˆï¼‰ã€‚**è¿™è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½œä¸ºç»§re-balancingä¹‹ååˆä¸€ä¸ªåœ¨é•¿å°¾æ•°æ®ä¸‹é€šç”¨çš„Strong Single-Stage Baseline**ã€‚

* æ ¸å¿ƒæ€æƒ³

  * éœ€è¦åˆ©ç”¨åŸå§‹çš„é•¿å°¾åˆ†å¸ƒæ¥å­¦ä¹ ç‰¹å¾æå–çš„åŸå› åœ¨äºï¼Œå¤§é‡çš„å°¾éƒ¨ç±»åˆ«å…¶å®ä¸è¶³ä»¥æä¾›è¶³å¤Ÿçš„æ ·æœ¬æ¥å­¦ä¹ é²æ£’çš„ç‰¹å¾è¡¨è¾¾ã€‚äººç±»æè¿°ç½•è§çš„ç‰©ä½“æ—¶ï¼Œå¾€å¾€æ˜¯é€šè¿‡å’Œå·²çŸ¥å¸¸è§ç±»çš„æ¯”è¾ƒï¼Œ**æ¯”å¦‚ä¼šè¯´ç‹®é¹«æ˜¯æœ‰ç€ç‹®å­çš„èº«ä½“ï¼Œé¹°çš„ç¿…è†€å’Œå¤´çš„ç”Ÿç‰©ï¼Œè€Œä¸å¿…è¦å•ç‹¬æ‹¿ä¸€å †ç‹®é¹«çš„å›¾ç‰‡å‡ºæ¥ï¼Œè®©ä½ æ­»è®°ç¡¬èƒŒä½ç‹®é¹«çš„é•¿ç›¸**ã€‚

    ![img](https://pic2.zhimg.com/80/v2-d453e3a2168bc0085970561adb1f2b91_720w.webp)

  * **å‘ç°ä¼˜åŒ–å™¨çš„åŠ¨é‡é¡¹æ—¶ï¼Œè¿™è´§ä¸å°±æ˜¯åœ¨è®­ç»ƒæ•°æ®æ—¶å¼•å…¥æ•°æ®åˆ†å¸ƒï¼Œä»è€Œäº§ç”Ÿshortcutçš„å…ƒå‡¶ä¹ˆ**ã€‚

  * æ–¹æ³•ï¼Œè¯¦ç»†çš„æ¨åˆ°æœ‰ç‚¹å¤æ‚ï¼Œç›¸è¿‘è®ºæ–‡æˆ–è€…çŸ¥ä¹

    ![img](https://pic1.zhimg.com/80/v2-aeb0b5c18e021b302263ffd7e49587c4_720w.webp)

  * **æ–¹æ³•é€Ÿæˆæ­¥éª¤**
    * **åŸºæœ¬åªè¦æ”¹classifier**ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–çš„è®­ç»ƒæ­¥éª¤æˆ–å¤æ‚çš„é‡‡æ ·ç®—æ³•

  ![image-20230211224744104](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230211224744104.png)

  

:fire: :hammer_and_wrench: **Interventional Few-Shot Learning**, in NIPS 2020. [[pdf](https://arxiv.org/pdf/2009.13000v2.pdf)] [[torch](https://github.com/yue-zhongqi/ifsl)] [[blog](https://zhuanlan.zhihu.com/p/584951599)]

> åšå®¢è®²å¾—å¾ˆå…¨é¢ï¼Œå…·ä½“å¯ä»¥è§åšå®¢å¯¹æœ¬æ–‡çš„è®²è§£
>
> * æ–¹æ³•**æŒºä¼šç©**çš„ï¼

* åŠ¨æœº

  * few-shot learningè¿‡å»å¾€å¾€æ”¶åˆ°**é¢„è®­ç»ƒçŸ¥è¯†**confounderçš„å½±å“

  ![image-20230206215108229](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230206215108229.png)

  * è¿‡å»few-shot learning æ–¹æ³•

  ![image-20230206215028756](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230206215028756.png)

  

* é—®é¢˜å»ºæ¨¡ï¼ˆæ¢ç´¢Many-shot Learning å’Œ Few-shotåœ¨å› æœå…³ç³»ä¸Šçš„ä¸ä¸€è‡´åœ°æ–¹ï¼‰
  * æœ‰ç‚¹éš¾è§£é‡Šå…·ä½“è§è®ºæ–‡

![image-20230206215234701](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230206215234701.png)

![image-20230206215302623](/Users/gary/Library/Application Support/typora-user-images/image-20230206215302623.png)

* é—®é¢˜æ ¸å¿ƒ

$$
P(Y \mid d o(X=\boldsymbol{x}))=\sum_d P(Y \mid X=\boldsymbol{x}, D=d, C=g(\mathbf{x}, d)) P(D=d)
$$

* **æ¨¡å‹æ–¹æ³•**ï¼ˆçœŸä¼šç©ï¼ï¼‰

![img](https://pic1.zhimg.com/80/v2-7f68252d630d8d70b2bfd9562ea560d4_720w.webp)

:fire: :hammer_and_wrench: **Introspective Distillation for Robust Question Answering**, in NIPS 2021. [[pdf](https://arxiv.org/abs/2111.01026)] [[torch](https://github.com/yuleiniu/introd)] [[zhihu](https://zhuanlan.zhihu.com/p/445125531)]

* åŠ¨æœº
  * å¸Œæœ›æ¨¡å‹å¯ä»¥åŒæ—¶åœ¨**IDï¼ˆin-domainï¼‰å’ŒOODåœºæ™¯ä¸‹è·å¾—è‰¯å¥½çš„è¡¨ç°**
  * é€šè¿‡å¯¹ä¸“é—¨æ•æ‰IDå’ŒOODåç½®çš„ä¸¤ä½è€å¸ˆæ¨¡å‹**è¿›è¡ŒçŸ¥è¯†èåˆ**(å†…çœ)å¹¶è’¸é¦å‡ºä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹æ¥å®Œæˆè¿™ç§å¹³è¡¡
* æ–¹æ³•

![image-20230211230411971](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230211230411971.png)



:fire: :hammer_and_wrench: **Distilling Causal Effect of Data in Class-Incremental Learning**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2103.01737)] [[torch](https://github.com/JoyHuYY1412/DDE_CIL)]

* [[æ¨¡å‹å…¬å¼è§£é‡Š](https://zhuanlan.zhihu.com/p/358340627)]  [[è®ºæ–‡ä»‹ç»](https://www.163.com/dy/article/G4OHT10U0511DPVD.html)]

* åŠ¨æœº

  * å¯¹æ’èŠ‚ç‚¹çš„å­˜åœ¨ä½¿å¾—æ¨¡å‹å¯¹æ–°æ•°æ®ä¼šäº§ç”Ÿç¾éš¾æ€§é—å¿˜

    * å½“åœ¨ä¸€ä¸ªæ•°æ®é›†å½“ä¸­è®­ç»ƒå¥½çš„æ¨¡å‹æ‰”åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®å½“ä¸­å¾®è°ƒæ—¶å€™ä¼šå‘ç”Ÿé—å¿˜

  * è¿‡å»çš„æ–¹æ³•å½“ä¸­

    ![image-20220918101920897](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220918101920897.png)

    ![img](https://pic1.zhimg.com/80/v2-0a7452df24b00f8e7d186f0dcf0c0680_720w.jpg)

    

    * **data reply**: éœ€è¦è¾ƒå¤§çš„**å­˜å‚¨**ç©ºé—´ï¼›**distillation**: ä¸æ˜¯**ç«¯åˆ°ç«¯**çš„è¡¨ç¤ºå­¦ä¹ ã€‚å› æ­¤ä½œè€…è€ƒè™‘å§æ˜¯å¦å­˜åœ¨ä¸€ç§ç«¯åˆ°ç«¯å½±å“çš„è’¸é¦æ–¹æ³•ã€‚

* æ€è·¯

  1. æ„å»º**å› æœè§’åº¦ä¸‹çš„ç±»åˆ«å¢é‡å­¦ä¹ **è¿‡ç¨‹
  2. åˆ†æ**ç¾éš¾æ€§é—å¿˜å‘ç”Ÿçš„åŸå› **ï¼ˆ**causal effect** lostï¼‰
  3. åˆ†æç°æœ‰å·¥ä½œå¦‚ä½•å®ç°æœ‰æ•ˆçš„æŠ—é—å¿˜ã€‚ åœ¨è¿™äº›åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‘ç°**æ§åˆ¶å¯¹æ’èŠ‚ç‚¹**æ˜¯ä¸€ç§å°šæœªåˆ©ç”¨ã€ä½†éå¸¸æœ‰æ•ˆçš„æŠ—é—å¿˜æ–¹æ³•ï¼Œåœ¨å„ç§ç±»åˆ«å¢é‡å­¦ä¹ çš„è®¾å®šä¸Šå–å¾—äº†ç¨³å®šçš„æå‡ã€‚
  4. åŒæ—¶è§£å†³äº†æ•°æ®ï¼ˆæ–°æ—§ç±»åˆ«ï¼‰é‡‡æ ·åˆ†å¸ƒ**ä¸å‡åŒ€**å¯¼è‡´çš„**bias**é—®é¢˜ã€‚

* **æ–‡ç« ç»†èŠ‚è¯¦è§å¼€å¤´çš„åšå®¢é“¾æ¥**

:fire: :hammer_and_wrench: **Causal Attention for Vision-Language Tasks**, in CVPR 2021. [[pdf](https://arxiv.org/pdf/2103.03493.pdf)] [[torch](https://github.com/yangxuntu/catt)] [[Blog](https://www.cnblogs.com/gongqk/p/14772297.html)]

> æˆ‘çš„æƒ³æ³•å’Œåšå®¢æœ€åè¯´çš„ç±»ä¼¼ï¼Œä¼¼ä¹æœ¬æ–‡**æ¨ç†æ¨å¾—ä¸æ˜¯ç‰¹åˆ«å½»åº•**ï¼Œä½†æ˜¯ä»ç„¶æ˜¯ä¸€ç¯‡ä¼˜ç§€çš„è®ºæ–‡

* åŠ¨æœºï¼šè§£å†³å½“å‰`VL`æ¨¡å‹å½“ä¸­è®­ç»ƒæ•°æ®é›†å½“ä¸­bias

  ![æˆªå±2022-10-11 11.09.42](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.09.42.png)

* æ€æƒ³

  * æ„å»ºå› æœå›¾

  ![image-20221011111108239](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221011111108239.png)

  * æ–¹æ³•ï¼ˆ**å‰é—¨è°ƒæ•´æœºåˆ¶**åš**Causal Attention**ï¼Œæ­¤å¤„ä¸åšå±•å¼€è®¨è®ºï¼Œè¯¦è§è®ºæ–‡ï¼‰

    ![æˆªå±2022-10-11 11.12.27](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.12.27.png)

    ![æˆªå±2022-10-11 11.12.56](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.12.56.png)

* ç»“æœcase

  ![æˆªå±2022-10-11 11.13.42](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.13.42.png)

:fire: :hammer_and_wrench: **Counterfactual Zero-Shot and Open-Set Visual Recognition**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2103.00887)] [[torch](https://github.com/yue-zhongqi/gcm-cf)] [[zhihu](https://zhuanlan.zhihu.com/p/365089242)]

* **ç”Ÿæˆå¼çš„å› æœæ¨¡å‹ï¼Œåšå®¢å¾ˆè¯¦ç»†ï¼Œè¯¦è§çŸ¥ä¹ï¼**

* æ–¹æ³•

  * **æ ·æœ¬ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾ä¹‹é—´è§£è€¦**

    ![img](https://pic2.zhimg.com/80/v2-d3135e2e7959fcbd10218d71db857e61_720w.webp)

  ![img](https://pic4.zhimg.com/80/v2-2319bf19f8737e74ad7b7504903d78df_720w.webp)

:fire: :hammer_and_wrench: **Counterfactual VQA: A Cause-Effect Look at Language Bias**, in CVPR 2021. [[pdf](https://arxiv.org/pdf/2006.04315.pdf)] [[torch](https://github.com/yuleiniu/cfvqa)]

* åŠ¨æœº

  * è¿‡å»æ–¹æ³•ä¸»è¦æ˜¯æ¶ˆé™¤VQAä¸­è¯­è¨€å…ˆéªŒä¿¡æ¯çš„bias
    * æ•°æ®å¢å¹¿
    * ç”Ÿæˆè§†è§‰æˆ–è€…æ–‡æœ¬çš„è§£é‡Š
    * åäº‹å®æ ·æœ¬ç”Ÿæˆ
    * é›†æˆæ–¹æ³•
  * è¯­è¨€å½“ä¸­çš„Biasæœ‰å¥½æœ‰åï¼Œæˆ‘ä»¬åšçš„åº”è¯¥**åˆ†è¾¨å‡ºå¥½çš„biaså’Œåçš„bias**
    * åçš„biasï¼šé¦™è•‰ğŸŒæœ‰å¯èƒ½æ˜¯ç»¿è‰²çš„ï¼Œä½†å›ç­”å¾€å¾€æ˜¯é»„è‰²
    * å¥½çš„biasï¼šé—® What Colorçš„æ—¶å€™ï¼Œå¾€å¾€æ¨¡å‹éƒ½ä¼šæƒ³åˆ°è§†è§‰è¯æ±‡ï¼Œç¼©å‡äº†ç­”æ¡ˆçš„ç©ºé—´ï¼Œé¿å…ç”Ÿæˆå¾ˆç¦»è°±çš„ç­”æ¡ˆ

* æ–¹æ³•ï¼ˆåŸºäºensembleçš„`VQA`æ¨¡å‹åšçš„ä¸€ä¸ª**æ¨ç†æ¡†æ¶**ï¼Œç›¸å½“ç®€å•ï¼‰

  ![image-20230205205153879](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230205205153879.png)

  ![image-20230205205846733](/Users/gary/Library/Application Support/typora-user-images/image-20230205205846733.png)

* å› æœæ¨ç†è§’åº¦è§£é‡Šï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20230205205700958](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230205205700958.png)



**Learning Causal Effects on Hypergraphs**, Best Paper of KDD 2022. [[pdf](https://arxiv.org/pdf/2207.04049.pdf)]

* **è¶…å›¾**ä¸Šè¿›è¡Œå› æœå½±å“åˆ†æï¼Œå¯¹æ¯”ä¼ ç»Ÿçš„ä»¥**ä¸¤ä¸¤èŠ‚ç‚¹**ä¸ºå›¾çš„æ›´åŠ æœ‰æ„ä¹‰ã€‚
* [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/564481108)] [[æ–¹æ³•ç»†èŠ‚](https://zhuanlan.zhihu.com/p/567996036)]

**Entropic Causal Inference: Graph Identifiability**, in ICML 2022. [[pdf](https://proceedings.mlr.press/v162/compton22a.html)]

> æ–°çš„**å› æœæ¨æ–­æ¶æ„**ï¼Œå¯ä»¥**ä¸å€ŸåŠ©intervention**
>
> ç†è®ºæ€§å¾ˆå¼ºçš„ä¸€ç¯‡æ–‡ç« 

* ç†µå› æœæ¨æ–­ï¼š

  * é€šè¿‡å¯»æ‰¾æ•°æ®çš„ä¿¡æ¯-ç†è®ºä¸Šæœ€ç®€å•çš„ç»“æ„è§£é‡Šï¼Œå³**æœ€å°ç†µæ¨¡å‹**ï¼Œä»è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å› æœå›¾ã€‚

  * **åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œé¦–å…ˆæ¨å¹¿äº†æ¾å¼›å‡è®¾ä¸‹çš„å› æœå›¾å¯è¾¨è¯†æ€§ç»“æœ**ã€‚

  * ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¬¬ä¸€ä¸ªå¯è¯†åˆ«çš„ç»“æœï¼Œä½¿ç”¨ç†µçš„æ–¹æ³•å­¦ä¹ **è¶…è¿‡ä¸¤ä¸ªèŠ‚ç‚¹çš„å› æœå›¾**ã€‚

    * è¯¥æ–¹æ³•åˆ©ç”¨äº†ä¸€ä¸ªå±æ€§ï¼Œå³ä¸€ä¸ªæºèŠ‚ç‚¹å’Œå®ƒçš„åä»£èŠ‚ç‚¹ä¹‹é—´çš„ç¥–å…ˆå…³ç³»å¯ä»¥ç”¨äºŒå…ƒç†µæµ‹è¯•æ¥ç¡®å®šã€‚
    * æä¾›äº†ä¸€ç§åŸºäºæ­¤ç‰¹æ€§çš„æ™®é€šå›¾çš„**æœ‰åºå‰¥ç¦»ç®—æ³•**ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ª**å¯å‘å¼ç®—æ³•**ï¼Œå°å›¾æ˜¾ç¤ºäº†è¾ƒå¼ºçš„ç»éªŒæ€§èƒ½ã€‚
    * æœ€åï¼Œåœ¨çœŸå®çš„æ•°æ®é›†ä¸Šæµ‹è¯•çš„ç®—æ³•ã€‚

    ![img](https://pic4.zhimg.com/80/v2-05352bd30cf8d5f8373c2ae67c3203c3_720w.webp)

:hammer_and_wrench: **[2023.01æœªå¼€æº] Disentangle and Remerge: Interventional Knowledge Distillation for Few-Shot Object Detection from A Conditional Causal Perspective**, in AAAI 2023. [[pdf](https://arxiv.org/abs/2208.12681)] [[torch](https://github.com/ZYN-1101/DandR)]

> æœ¬æ–‡**æ–¹æ³•ä¸Šå’Œå®éªŒä¸Š**è¿˜æœ‰å¾ˆå¤šäº®ç‚¹ï¼Œè¿™é‡Œä¸»è¦å°±è¯´**å› æœæ¨ç†**

* åŠ¨æœº
  * çŸ¥è¯†è’¸é¦åšFew-shotä»»åŠ¡æ—¶å€™ï¼Œå¾€å¾€å¤§æ¨¡å‹å­˜åœ¨çš„çŸ¥è¯†**å¾ˆéš¾è¿ç§»**åˆ°ä¸‹æ¸¸ä»»åŠ¡å½“ä¸­
* å› æœå›¾

![image-20230129234912585](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230129234912585.png)

* æ¨¡å‹

![image-20230129235019260](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230129235019260.png)



**Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.5/)]

* åŠ¨æœº

  * å½“å‰ç¥ç»ç¦»æ•£æ¨ç†åœ¨åäº‹å®æ¨ç†ä¸Šæ•ˆæœä¸ä½³
  * å€Ÿç”¨**å› æœæ¨æ–­**çš„ç†è®ºå¯ä»¥åšåäº‹å®æ¨ç†

  ![image-20230130170116064](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230130170116064.png)

* æ–¹æ³•

  * ä¸¤æ­¥èµ°
    * **è¯†åˆ«äº‹å®**
    * é€šè¿‡ç¦»æ•£çš„æ“ä½œè¿›è¡Œ**å¹²é¢„**
  * æ¨¡å‹

  ![image-20230130170205387](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230130170205387.png)







:hammer_and_wrench: :star2: **Causality Inspired Representation Learning for Domain Generalization**, in CVPR 2022 **Oral**. [[pdf](https://arxiv.org/abs/2203.14237)] [[torch](https://github.com/BIT-DA/CIRL)]

> **å‚…ç«‹å¶å˜æ¢ç»“åˆå› æœæ¨ç†**ï¼Œè¶…çº§solidçš„å·¥ä½œ

* åŠ¨æœº

  * è¿‡å»é¢†åŸŸæ³›åŒ–çš„æ–¹æ³•éƒ½æ˜¯**åŸºäºç»Ÿè®¡ä¾èµ–**ï¼Œæ²¡æœ‰å­¦ä¹ åˆ°çœŸæ­£çš„**å› æœå…³ç³»**
  * ä»»åŠ¡çš„æ•°æ®è¡¨å¾å¾€å¾€å¯ä»¥åˆ†æˆ**causal factors $S$ and uncausal factors $U$**
  * å› æœæ¨æ–­åº”ç”¨è¿›æ¥çš„ä¸‰ä¸ªåŸåˆ™
    1. $S$ å’Œ $U$ ç›¸äº’åˆ†ç¦»
    2. $s_1, s_2,...,s_n$ä¹‹é—´ç›¸äº’ç‹¬ç«‹
    3. $X \rightarrow Y$ä¹‹é—´å› æœå……åˆ†ï¼Œå› æœçš„è¡¨å¾æ˜¯å……åˆ†çš„

* æ–¹æ³•

  * å‰æï¼šå‚…ç«‹å¶å˜æ¢å½“ä¸­

    * ç›¸ä½ï¼šé«˜å±‚çš„è¯­ä¹‰ä¿¡æ¯
    * æŒ¯å¹…ï¼šä½å±‚çš„ç»Ÿè®¡ä¿¡æ¯

    ![image-20230222154015844](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230222154015844.png)

  * æ ¸å¿ƒæ€æƒ³ä¸æµç¨‹æ¦‚è§ˆï¼Œ**å…·ä½“è§è®ºæ–‡**

    * å¯¹æŒ¯å¹…è¿›è¡Œå¹²é¢„ï¼Œä½¿å¾—å¹²é¢„å‰åçš„è¡¨å¾å°½å¯èƒ½çš„ç›¸ä¼¼
    * è¡¨å¾å½“ä¸­æ¯ä¸ªç»´åº¦çš„è¡¨å¾$s_i$å°½å¯èƒ½ä¸ç›¸è¿‘
    * ä½¿ç”¨åˆ†ç±»å™¨å’Œæ©ç å™¨ä¹‹é—´çš„å¯¹æŠ—æ£€æµ‹å› æœä¿¡æ¯è¾ƒå°‘çš„ç»´åº¦ï¼Œåˆ†åˆ«**æœ€å¤§åŒ–è¾ƒå¥½ç»´åº¦çš„åˆ†ç±»å™¨å’Œæœ€å°åŒ–è¾ƒå¥½ç»´åº¦çš„åˆ†ç±»å™¨**

    ![image-20230222154338573](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230222154338573.png)

    * **æ€»ä¼˜åŒ–ç›®æ ‡**ï¼š

    $$
    \min _{\hat{g}, \hat{h}_{1}, \hat{h}_{2}} \mathcal{L}_{c l s}^{s u p}+\mathcal{L}_{c l s}^{i n f}+\tau \mathcal{L}_{F a c}, \quad \min _{\hat{w}} \mathcal{L}_{c l s}^{s u p}-\mathcal{L}_{c l s}^{i n f},
    $$



:fire: **[CLIP] EI-CLIP: Entity-aware Interventional Contrastive Learning for E-commerce Cross-modal Retrieval**, in CVPR 2022. [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_EI-CLIP_Entity-Aware_Interventional_Contrastive_Learning_for_E-Commerce_Cross-Modal_Retrieval_CVPR_2022_paper.pdf)]

* åŠ¨æœº

  * åœ¨ç”µå•†äº§å“å½“ä¸­ï¼Œlanguageçš„semanticå’Œgeneral domainå½“ä¸­çš„ä¸å¤ªä¸€æ ·
  * æ¯”å¦‚ï¼šGolden Gooseåœ¨ç”µå•†äº§å“ä¸­å¯èƒ½æ˜¯ä¸€ä¸ªå“ç‰Œ

  ![image-20230306100430190](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230306100430190.png)

  * **äº§å“ä¸­ä¸åŒçš„metadataå¯¹æœ€åæ£€ç´¢çš„ç»“æœè´¡çŒ®ä¸ä¸€æ ·**

  ![image-20230306100443859](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230306100443859.png)

* æ–¹æ³•
  * CLIPç›´æ¥æ£€ç´¢æ— æ³•æå®šé‚£äº›ç”µå•†é¢†åŸŸçš„è¯­ä¹‰ä¿¡æ¯ï¼Œæ‰€ä»¥doä¸€ä¸‹æ–‡æœ¬çš„embedding
  * è¿‡å»çš„confounderè®¾è®¡éƒ½æ˜¯ç›´æ¥è€ƒè™‘äº†å…¨éƒ¨æ–‡æœ¬æ•°æ®ï¼Œè¿™é‡Œä½¿ç”¨metadataè¿›è¡Œè€ƒè™‘ï¼ŒåŒæ—¶åŠ æƒé€‰å‡º**é‡è¦çš„confounder**

![image-20230306102010632](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230306102010632.png)

**[é¢†åŸŸæ³›åŒ–] GCISG: Guided Causal Invariant Learning for Improved Syn-to-real Generalization**, in ECCV 2022. [[pdf](https://arxiv.org/abs/2208.10024)]

> ä¸ªäººè®¤ä¸ºè¿™ç¯‡è®ºæ–‡å†™å¾—æœ‰ç‚¹å¤¸å¼ äº†ï¼Œå®é™…ä¸Šå°±é‚£ä¹ˆå›äº‹ã€‚

* åŠ¨æœº

  * å’Œä¸Šé¢è®ºæ–‡ä¸€æ ·ï¼Œéœ€è¦è§£è€¦å‡º**ä»»åŠ¡æ— å…³çš„styleç‰¹å¾**å’Œ**ä»»åŠ¡ç›¸å…³çš„styleç‰¹å¾**

* å› æœå‘ç°

  * CNNç½‘ç»œå¯¹**textureï¼ˆçº¹ç†ï¼Œå¦‚styleï¼‰ç‰¹å¾æœ‰åè§**ï¼Œ [å‚è€ƒæ–‡çŒ®](https://arxiv.org/abs/1811.12231)
  * å› æœå›¾

  ![image-20230227101118298](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230227101118298.png)

* æ–¹æ³•ï¼ˆå¾ˆç®€å•ï¼‰

![image-20230227101000348](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230227101000348.png)





:hammer_and_wrench: **Causality-aware Concept Extraction based on Knowledge-guided Prompting**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.01876)] [[torch](https://github.com/siyuyuan/KPCE)]

> æ–¹æ³•æå…¶ç®€å•

* åŠ¨æœº

  * PLMåœ¨æ¦‚å¿µæŠ½å–å½“ä¸­å¾€å¾€ä¼šå­˜åœ¨Biasï¼ˆæ¦‚å¿µæŠ½å–å°±æ˜¯æŠŠæŸä¸ªspanæŠ½å–å‡ºæ¥ï¼‰

  ![image-20230505221541856](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230505221541856.png)

  ![image-20230505221654869](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230505221654869.png)

* å› æœåˆ†æ

  * ç”±äºè¯­è¨€æ¨¡å‹é‡Œé¢çš„å…ˆéªŒçŸ¥è¯†æ²¡åŠæ³•è§‚æµ‹ï¼Œæ‰€ä»¥é‡‡ç”¨å‰é—¨è°ƒæ•´çš„æœºåˆ¶ï¼ˆåŠ å¤šä¸€ä¸ªä¸­ä»‹ï¼‰

  ![image-20230505221755518](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230505221755518.png)

* æ–¹æ³•ï¼ˆæå…¶ç®€å•ï¼ŒåŠ å¤šä¸€ä¸ªåˆ†ç±»å‡ºæ¥çš„Topicï¼‰

  ![image-20230505221901782](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230505221901782.png)



:hammer_and_wrench: **COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective**, in ACL 2023.  [[pdf](https://arxiv.org/abs/2305.05191)] [[torch](https://github. com/HKUST- KnowComp/COLA)]

> å’Œ2022å¹´é‚£ç¯‡ICMLè®ºæ–‡ROCKåšçš„ä»»åŠ¡æ˜¯ä¸€æ ·çš„ï¼Œå°±æ˜¯çº¯æ–‡æœ¬å½“ä¸­çš„**å› æœäº‹ä»¶æ£€æµ‹**

* åŠ¨æœº

  * æœ‰ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰çš„å› æœå‘ç°ï¼Œä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†

  ![image-20230513220020021](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230513220020021.png)

* æ–¹æ³•ï¼ˆç†è®ºä¾æ®è¯¦è§è®ºæ–‡ï¼‰

![image-20230513220210226](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230513220210226.png)



:fire: **CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation**, in ACL Finding 2023. [[pdf](https://arxiv.org/abs/2306.00374)]

> æœ‰ç‚¹æ™¦æ¶©å†™çš„ï¼Œä¸æ˜¯å¾ˆçœ‹å¾—æ‡‚

* åŠ¨æœº
  * ä½¿ç”¨å› æœå›¾æ¥è§£å†³LMæœ‰æ¯’æ€§çš„é—®é¢˜
  * è¿‡å»detoxificationçš„æ–¹æ³•ä¼šè¿‡åˆ†æ¢ç´¢æ¯’æ€§ä¸å—ä¿æŠ¤çš„è™šå‡å…³ç³»
  * è¿‡å»æ–¹æ³•æ€»ç»“ï¼š

![image-20230607103152478](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230607103152478.png)

* æ€è·¯

![image-20230607103224807](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230607103224807.png)

* æ¨¡å‹æ–¹æ³•

  1. ä¸‰ä¸ªtoxicity attributesçš„æ£€æµ‹ï¼šoffenseï¼Œabuseï¼Œhate
  2. åäº‹å®ç”Ÿæˆ + è®¡ç®—ATEï¼ˆå¹³å‡åŒ–å½“å‰è¯çš„TEï¼‰

  ![image-20230607103351552](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230607103351552.png)

  3. æ„å»ºSCM + æ±‚ Loss

![image-20230607103515590](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230607103515590.png)

* å®éªŒï¼šå¾®è°ƒGPT-2è¿˜æœ‰OPT



:fire: :hammer_and_wrench: **Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.232/)] [[torch](https://github.com/myZeratul/Causal-Debias)]

> é€šè¿‡å­¦ä¹ å› æœä¸å˜æ€§ï¼Œæ¥ç¼“è§£PLMåœ¨fine-tuneæ—¶å€™çš„biasé—®é¢˜

* åŠ¨æœº
  * è¿‡å»çš„æ–¹æ³•è™½ç„¶å¯ä»¥åœ¨Pre-trainedå°±å¯ä»¥debiasï¼Œä½†æ˜¯biasçš„é—®é¢˜ï¼ˆåˆ»æ¿çš„è”æƒ³ï¼Œæ€§åˆ«ï¼Œracialç­‰ï¼‰ä¼šåœ¨fine-tuneæ—¶å€™é‡ç°

![image-20230727090904224](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230727090904224.png)

* æ–¹æ³•

![image-20230727091050764](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230727091050764.png)

$do(N=n)$æ±‚invariant loss
$$
\min \mathcal{L}_{\text {invariant }}=\mathbb{E}_n(\mathcal{R})+\operatorname{Var}_n(\mathcal{R})
$$
![image-20230727091347770](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230727091347770.png)



:fire: :hammer_and_wrench: **Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference**, in ACL 2023 Oral. [[pdf](https://arxiv.org/abs/2306.10790)] [[torch](https://github.com/zzz47zzz/CET)]

* åŠ¨æœº
  * é¢„è®­ç»ƒæ¨¡å‹Fine-tuneçš„æ—¶å€™å®¹æ˜“å­˜åœ¨ç¾éš¾æ€§é—å¿˜
    * **æ–‡ç« ä¸­æåˆ°Fine-tuneä¸ç¾éš¾æ€§é—å¿˜åœ¨ä¸€èµ·çš„æ—¶å€™ï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³target task**
    * è€ŒCL + ç¾éš¾æ€§é—å¿˜åœ¨ä¸€èµ·çš„æ—¶å€™ä¼šè€ƒè™‘æ‰€æœ‰task
* å› æœåˆ†æ

![image-20230729104513604](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230729104513604.png)

* æ¨¡å‹æ–¹æ³•
  * æœ‰ç‚¹ç±»ä¼¼CLä¸­Replayçš„æ–¹æ³•ï¼Œç”¨KNNæ‰¾æœ€ç›¸é‚»çš„æ ·æœ¬ï¼ˆä½†è¿™ä¸ªæ ·æœ¬å“ªé‡Œæ¥å‘¢ï¼Œæ„Ÿè§‰å¹¶ä¸æ˜¯é¢„è®­ç»ƒé‡Œé¢æ¥çš„ï¼Ÿï¼‰

![image-20230729105649434](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230729105649434.png)

* Fine-tuneçš„ç›®æ ‡å‡½æ•°

![image-20230729105714303](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230729105714303.png)



**[VQA] Reducing Vision-Answer Biases for Multiple-Choice VQA**, in TIP 2023. [[pdf](https://ieeexplore.ieee.org/abstract/document/10214252/)]

* åŠ¨æœº
  * è¿‡å»çš„VQAæ¨¡å‹å®¹æ˜“å­˜åœ¨Biasï¼Œä¸çœ‹é—®é¢˜å°±æ¥é€‰
  * åŸå› æ˜¯æ ‡æ³¨çš„é•¿å°¾åˆ†å¸ƒ
  * å¦‚ä½•å½±å“æ¨¡å‹ï¼šé€šè¿‡å¤šæ¨¡æ€èåˆç‰¹å¾

![image-20230824202610504](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230824202610504.png)

* æ–¹æ³•ï¼ˆåé—¨è°ƒæ•´ + å¤šæ¨¡æ€ç‰¹å¾çš„åäº‹å®lossï¼‰

![image-20230824202719842](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230824202719842.png)

**Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.37/)]

* åŠ¨æœº
  * è¿‡å»çš„æ–¹æ³•ä»è®­ç»ƒé›†ä¸­å­¦ä¹ åˆ°çš„label-specificçš„ç‰¹å¾ï¼Œå¯¼è‡´äº†bias
  * å› æœæ¨ç† + åäº‹å® å¼•å…¥åˆ°å¤šæ¨¡æ€fake newæ£€æµ‹ä¸­

![image-20230902195944598](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902195944598.png)

* å› æœå›¾åˆ†æ

![image-20230902200058716](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902200058716.png)

* æ¨¡å‹æ–¹æ³•ï¼ˆå…¬å¼æ¯”è¾ƒå¤šï¼Œå¯ä»¥çœ‹ä¸€ä¸‹åŸæ–‡ï¼‰

![image-20230902200152045](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902200152045.png)

:hammer_and_wrench: :fire: **A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.32/)] [[code](https://github.com/alestolfo/causal-math)]

> éå¸¸Perfectçš„å…³äºå› æœæ¨ç† + LLM + æ•°å­¦æ¨ç†çš„Empirical Study

* åŠ¨æœº
  * ç ”ç©¶æ¯ä¸ªLLMåœ¨ä¸åŒçš„æ•°å­¦é—®é¢˜è¾“å…¥å¹²é¢„ä¸‹çš„æ•æ„Ÿåº¦å’Œé²æ£’æ€§

![image-20230905105852822](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230905105852822.png)

* å› æœæ¨ç†æ¡†æ¶ï¼Œç ”ç©¶LLM

![image-20230905105935990](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230905105935990.png)

* å…·ä½“å®éªŒç»“æœï¼Œå¯ä»¥çœ‹è®ºæ–‡

:hammer_and_wrench: **Random Boxes Are Open-world Object Detectors**, in CVPR 2023. [[pdf](https://arxiv.org/abs/2307.08249)] [[torch](https://github.com/scuwyh2000/RandBox)]

* åŠ¨æœº
  * è¿‡å»çš„Open-worldçš„ODæ–¹æ³•å¤ªå…³æ³¨knownçš„å¯¹è±¡ï¼Œè€Œä¸å…³æ³¨unknownçš„å¯¹è±¡
  * æå‡ºrandomçš„boxï¼Œæœ‰ä¸¤ä¸ªä¼˜ç‚¹
    * é˜²æ­¢äº†è®­ç»ƒè¢«confoundedï¼ˆrandomizationå’Œknownçš„objectåˆ†å¸ƒç›¸äº’ç‹¬ç«‹ï¼‰
    * æ— åçš„è®­ç»ƒé¼“åŠ±æ›´å¤šçš„proposalæ¢ç´¢

![image-20230920105054836](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230920105054836.png)

![image-20230920105249978](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230920105249978.png)

* è¿‡å»OWODæ–¹æ³•

![image-20230920110822856](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230920110822856.png)

* å› æœåˆ†æï¼ˆåˆ‡æ–­åé—¨è·¯å¾„$R \leftarrow D \rightarrow Y$ï¼‰

![image-20230920105358323](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230920105358323.png)



* æ¨¡å‹æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20230920110744901](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230920110744901.png)



:fire: :hammer_and_wrench: **Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2307.07250)] [[torch](https://github.com/ByungKwanLee/Double-Debiased-Adversary)]

* åŠ¨æœº
  * è¿‡å»çš„æ¨¡å‹ä»ç„¶ä¼šå—åˆ°ä¸å¯æ¶ˆé™¤çš„å¯¹æŠ—æ¼æ´
  * ä½¿ç”¨Double Machine Learningçš„æ–¹æ³•æ¥è¯„ä¼°å› æœå‚æ•°

![image-20230930112534237](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230930112534237.png)

* å› æœå»ºæ¨¡ï¼ˆå…³é”®æ˜¯é‚£ä¸ª$\theta$ï¼‰

![image-20230930121446835](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230930121446835.png)

* æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼Œæ•°å­¦æ¨å¯¼æœ‰ç‚¹å¤æ‚ï¼‰

![image-20230930121524762](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230930121524762.png)



:fire: **Variational Causal Inference Network for Explanatory Visual Question Answering**, in ICCV 2023. [[pdf](https://openaccess.thecvf.com/content/ICCV2023/html/Xue_Variational_Causal_Inference_Network_for_Explanatory_Visual_Question_Answering_ICCV_2023_paper.html)]

> æœ¬æ–‡å…¬å¼æ¨å¯¼è¾ƒä¸ºå¤æ‚ï¼Œå…·ä½“å¯ä»¥è§è®ºæ–‡

* åŠ¨æœº
  * ä¿è¯ç”Ÿæˆçš„è§£é‡Šå’ŒAnswerä¹‹é—´çš„ä¸€æ”¯æ–°ï¼Œå¹¶ä¸”å¯ä»¥åŠ¨æ€æ•è·visual tokenå’Œquestion tokenç”Ÿæˆè§£é‡Š

![image-20231003110257827](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231003110257827.png)

* å› æœåˆ†æ

 ![image-20231003110348348](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231003110348348.png)

* æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰
  * æ ¸å¿ƒï¼šè®©æµ‹è¯•å’Œè®­ç»ƒæ—¶å€™çš„åˆ†å¸ƒä¸è¦åç§»å¤ªå‰å®³ï¼Œç”¨ä¸€ä¸ªKLæ•£åº¦è§„èŒƒ

![image-20231003110505687](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231003110505687.png)

* æŸå¤±å‡½æ•°ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20231003110557738](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231003110557738.png)



:fire: **A Multi-modal Debiasing Model with Dynamical Constraint for Robust Visual Question Answering**, in ACL Findings 2023. [[pdf](https://aclanthology.org/2023.findings-acl.311/)]

* åŠ¨æœºï¼šè¿‡å»ç›´æ¥ç›¸å‡çš„debiasæ–¹æ³•ï¼ŒæŸå®³äº†IDçš„æ€§èƒ½è¡¨ç°

![image-20231015110456869](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231015110456869.png)

* æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼Œæœ‰ç‚¹é“ç†ï¼Œåˆä¸æ˜¯å¤ªåˆç†ï¼‰ï¼ˆä¸ªäººè§‰å¾—å’ŒBackdoor Debiasingçš„è®ºæ–‡æ€è·¯æ¯”è¾ƒç±»ä¼¼ï¼‰

![image-20231015110559283](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231015110559283.png)

![image-20231015110629597](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231015110629597.png)



:fire: **Neuro-Symbolic Procedural Planning with Commonsense Prompting**, in ICLR 2023. [[pdf](https://arxiv.org/abs/2206.02928)]

* Prompt + Causalçš„
* æ–¹æ³•æœ‰ç‚¹å¥‡æ€ªï¼Œæˆ‘è¿˜æ²¡çœ‹æ‡‚



**A Causal Inference Look at Unsupervised Video Anomaly Detection**, in AAAI 2022. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/20053)]

* åŠ¨æœº
  * æ— ç›‘ç£çš„è§†é¢‘ADæœ‰biasé—®é¢˜ï¼ˆä¼ªæ ‡ç­¾å¸¦æ¥çš„ï¼‰
* åˆ†æ

![image-20231114205543073](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114205543073.png)

## :happy: Emotion

**Understanding Chat Messages for Sticker Recommendation in Messaging Apps**ï¼Œ in AAAI 2020. [[pdf](https://arxiv.org/abs/1902.02704)]

* è´¡çŒ®
  * å¼€å‘äº†ä¸€ä¸ªè¡¨æƒ…åŒ…æ¨èçš„APPï¼ˆæ ¹æ®å¯¹è¯æ¥æ¨èï¼‰
  * æ•°æ®é›†å’Œä»£ç éƒ½æ²¡æœ‰å¼€æº
* æ–¹æ³•ï¼ˆä¸¤é˜¶æ®µï¼‰

![image-20230529102818500](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529102818500.png)



:hammer_and_wrench: :fire: **Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog**, in WWW 2020. [[pdf](https://arxiv.org/abs/2003.04679)] [[torch](https://github.com/gsh199449/stickerchat)]

* è´¡çŒ®
  * æå‡º**340Kçš„å¤šè½®å¯¹è¯å’Œè¡¨æƒ…åŒ…å¯¹æ•°æ®é›†**

![image-20230529104712626](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529104712626.png)

* æ–¹æ³•ï¼ˆæŒºç®€å•ï¼‰

![image-20230529104740082](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529104740082.png)



**SER30K: A Large-Scale Dataset for Sticker Emotion Recognition**, in MM 2022 Oral. [[pdf](https://dl.acm.org/doi/abs/10.1145/3503161.3548407)] [[torch](https://github.com/nku-shengzheliu/SER30K)]

* è´¡çŒ®
  * **è¡¨æƒ…åŒ…æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†**

![image-20230529105113780](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529105113780.png)

* è¡¨æƒ…åŒ…çš„ç‰¹æ€§å’Œä¸€äº›ç›¸å…³çš„åº”ç”¨

![image-20230529105330713](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529105330713.png)



* æ•°æ®é›†ç‰¹ç‚¹ï¼ˆsticker emotion recognitionï¼‰

  * åŒ…æ‹¬æƒ…æ„Ÿå’Œå¯¹è¯çš„æ–‡æœ¬æ ‡æ³¨ï¼ˆä½†æ˜¯è¿™ä¸ªæ–‡æœ¬**å…¶å®åªæœ‰ä¸€å¥è¯**ï¼‰
  * æ•°æ®é›†ç»Ÿè®¡

  ![image-20230529105634054](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529105634054.png)

* æ–¹æ³•ï¼ˆBaselineï¼‰

![image-20230529105703308](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529105703308.png)

* Case Study

![image-20230529105733843](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529105733843.png)



:hammer_and_wrench: :fire: **Selecting Stickers in Open-Domain Dialogue through Multitask Learning**, in ACL Finding 2022. [[pdf](https://aclanthology.org/2022.findings-acl.241/)] [[torch](https://github.com/nonstopfor/Sticker-Selection)]

* è¡¨æƒ…åŒ…æ¨èä¾‹å­

![image-20230529154209146](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529154209146.png)



* æ–¹æ³•ï¼ˆå¤šä»»åŠ¡å­¦ä¹ çš„æ–¹æ³•ï¼‰

![image-20230529154301746](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230529154301746.png)



## :old_key: Traditional NLP Task

### :label: NER

>  Named Entity Recognition

**å½“å‰ç«èµ›NERä»»åŠ¡çš„baselineï¼š**

- BERT + BILSTM + CRF
- [åšå®¢è¿æ¥ NERé“æ‰“çš„baseline](https://zhuanlan.zhihu.com/p/166496466)

:fire: **Bidirectional LSTM-CRF Models for Sequence Tagging**, in 2015. [[pdf](https://arxiv.org/pdf/1508.01991v1.pdf)] [[code](https://paperswithcode.com/paper/bidirectional-lstm-crf-models-for-sequence)

* ä½¿ç”¨BiLSTM+CRFåšNERçš„å¼€å±±ä¹‹ä½œ
* [ç›¸å…³åšå®¢è¿æ¥](https://zhuanlan.zhihu.com/p/166496466)

:hammer_and_wrench: :fire: **Fast and Accurate Entity Recognition with Iterated Dilated Convolutions**, in EMNLP 2017.  [[pdf](https://arxiv.org/pdf/1702.02098.pdf)] [[tensorflow](https://github.com/iesl/dilated-cnn-ner)]

* Iterated Dilated Convolutions ç©ºæ´å·ç§¯ 


![image-20220825002628063](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825002628063.png)

* æ ¸å¿ƒæ€æƒ³

  * ä¼ ç»Ÿå·ç§¯çš„é—®é¢˜ï¼špoolingå±‚ä¼š**æŸå¤±ä¿¡æ¯**ï¼Œé™ä½ç²¾åº¦ã€‚é‚£ä¹ˆä¸åŠ poolingå±‚ä¼šä½¿**æ„Ÿå—é‡å˜å°**ï¼Œå­¦ä¸åˆ°å…¨å±€çš„ç‰¹å¾ã€‚å¦‚æœå•çº¯çš„å»æ‰poolingå±‚ã€æ‰©å¤§å·ç§¯æ ¸çš„è¯ï¼Œè¿™æ ·çº¯ç²¹çš„æ‰©å¤§å·ç§¯æ ¸åŠ¿å¿…å¯¼è‡´**è®¡ç®—é‡çš„å¢å¤§**ã€‚
  * CNNä¹Ÿå¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜

![image-20220825004251541](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825004251541.png)

* é€Ÿåº¦æ¯”ä»¥å‰çš„`Bi-LSTM-CRF`å¿«äº†éå¸¸å¤šï¼Œè€Œä¸”ç²¾åº¦æ²¡æœ‰ä¸‹æ»‘

:hammer_and_wrench: :fire: **BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision** , in KDD 2020.  [[pdf](https://arxiv.org/pdf/2006.15509.pdf)] [[torch](https://github.com/cliang1453/BOND)]

* è¿œè·ç¦»ç›‘ç£çš„é—®é¢˜

  * å™ªå£°ï¼Œè´Ÿæ ·æœ¬ä¸å¥½ç”Ÿæˆ
  * å®Œæ•´æ€§ä¸è¶³
  * `trade-off` åœ¨æ ‡æ³¨å‡†ç¡®åº¦å’Œè¦†ç›–èŒƒå›´ä¹‹é—´

* æƒ³æ³•

  * ç¬¬ä¸€é˜¶æ®µä½¿ç”¨`RoBERTa`å¾®è°ƒï¼Œé€‚åº”`NER`ä»»åŠ¡

    * ä½¿ç”¨`Early stopping` æ–¹æ³•é˜²æ­¢æ•°æ®è¿‡æ‹Ÿåˆè¿˜æœ‰å¯¹**æœªçŸ¥æ•°æ®å¢å¼ºæ³›åŒ–èƒ½åŠ›**
    * é¦–å…ˆé€šè¿‡`POS`è¯†åˆ«æ½œåœ¨å®ä½“ï¼Œç„¶åé€šè¿‡è¯­æ–™åº“è®¡ç®—æœ€å°æŸå¤±ç¡®å®šå®ä½“

    ![img](https://pic2.zhimg.com/80/v2-0951f0afefa53b2efc269f92136a3ae9_720w.jpg)

  * ç¬¬äºŒé˜¶æ®µè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ ï¼ˆteacher-studentæ¨¡å‹2ï¼‰

    * åº”å¯¹**å˜ˆæ‚å’Œä¸å®Œæ•´æ ‡æ³¨**çš„æŒ‘æˆ˜
    * teacherç”Ÿæˆä¼ªæ ‡ç­¾äº¤ç»™studentå»é¢„æµ‹
    * **é‡æ–°åŠ æƒçš„é«˜ç½®ä¿¡åº¦è½¯æ ‡ç­¾**
    * ==ç¬¬äºŒé˜¶æ®µçš„æ ¸å¿ƒå°±æ˜¯å¢å¼ºæ•°æ®çš„ç½®ä¿¡ç¨‹åº¦ï¼==

![img](https://pic4.zhimg.com/80/v2-8788aa61ea19b041e763b597d844ebcb_720w.jpg)

ä¸¤é˜¶æ®µçš„BONDæ¡†æ¶

* åœ¨é˜¶æ®µIä¸­ï¼Œç»è¿‡é¢„è®­ç»ƒçš„BERTç”¨äºæ—©åœçš„è¿œè·ç¦»NERä»»åŠ¡
* åœ¨é˜¶æ®µIIä¸­ï¼Œé¦–å…ˆä»é˜¶æ®µIä¸­å­¦ä¹ çš„æ¨¡å‹åˆå§‹åŒ–studentæ¨¡å‹å’Œteacheræ¨¡å‹ã€‚ç„¶åä½¿ç”¨teacheræ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹studentæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åŒæ—¶ï¼Œç”±æ—©åœçš„studentè¿­ä»£æ›´æ–°teacheræ¨¡å‹ã€‚



:hammer_and_wrench: :fire: **[A Boundary-aware Neural Model for Nested Named Entity Recognition](https://aclanthology.org/D19-1034.pdf)** , in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1034.pdf)] [[torch](https://github.com/thecharm/boundary-aware-nested-ner)]

* è§£å†³`Nested NER`çš„é—®é¢˜
* ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•
  * é¢„æµ‹è¾¹ç•Œ
  * é¢„æµ‹å®ä½“åˆ†ç±»

:hammer_and_wrench: :fire: **Cross-Domain NER using Cross-Domain Language** , in ACL 2020.  [[pdf](https://aclanthology.org/P19-1236/)] [[torch](https://github.com/jiachenwestlake/Cross-Domain_NER)]

* è§£å†³**è·¨é¢†åŸŸæ— ç›‘ç£**çš„æ ‡æ³¨é—®é¢˜
* æ ¸å¿ƒæ€æƒ³

![img](https://pic4.zhimg.com/80/v2-52cb3242e0a708e48b29ab2f8d81e027_720w.jpg)

æœ€åº•ä¸‹çš„ä¸€å±‚æ˜¯æ•°æ®å±‚ï¼Œæ ‡å‡†æƒ…å†µä¸€ä¸‹æ€»å…±æœ‰å››ä»½è¯­æ–™ï¼Œåˆ†åˆ«å¯¹åº”**ä¸¤ä¸ªdomainä¸‹çš„ä¸¤ä¸ªtaskï¼ˆNERå’Œè¯­è¨€å»ºæ¨¡ï¼‰**ã€‚å…¶ä¸­Source Domainï¼ˆå³ä¿è¯æœ‰æ ‡è®°æ•°æ®ç”¨äºNERçš„domainï¼‰å¯¹åº”ä¹‹å‰æåˆ°çš„News Domainï¼Œå› ä¸ºè®ºæ–‡ä¸­Source Domainä½¿ç”¨çš„æ˜¯æ–°é—»æ•°æ®ã€‚å¦å¤–å¦‚æœæ˜¯æ— ç›‘ç£æŠ½å–Target Domainæ•°æ®åˆ™åªæœ‰ä¸‰ä»½è¯­æ–™ã€‚

ç”±åº•å‘ä¸Šç¬¬äºŒå±‚æ˜¯Word Embeddingå±‚ï¼Œè®ºæ–‡ä¸­çš„Word Embeddingç»“åˆäº†è¯çº§åˆ«å’Œå­—ç¬¦çº§åˆ«çš„å‘é‡è¡¨ç¤ºã€‚å³æŠŠè¯å‘é‡å’Œä¸€ä¸ªè¯çš„å­—ç¬¦åºåˆ—å½¢æˆçš„çŸ©é˜µç»è¿‡CNNå¤„ç†åçš„**å‘é‡concatenateèµ·æ¥**ã€‚

ç¬¬ä¸‰å±‚æ˜¯åŒå‘LSTMï¼Œç”¨äºåºåˆ—å¤„ç†ç¬¬äºŒå±‚çš„æ•°æ®ï¼Œç”Ÿæˆå‰åå‘hidden stateã€‚

ç¬¬å››å±‚LMå’ŒCRFï¼Œå³task modelå±‚ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬å››å±‚æœ‰ä¸‰ä¸ªæ¨¡å—ï¼Œä¸¤ä¸ªæ˜¯ç”¨äºNERçš„CRFæ¨¡å‹ï¼Œåˆ†åˆ«å¯¹åº”Source Domainå’ŒTarget Domainã€‚å¦ä¸€ä¸ªæ˜¯åŸºäºç¬¬ä¸‰å±‚BiLSTMçš„è¯­è¨€æ¨¡å‹ï¼ŒNSSoftmaxæ˜¯æŒ‡è¿™ä¸ªè¯­è¨€æ¨¡å‹åˆ©ç”¨**Negative Sampling Softmaxçš„æ–¹å¼è¿›è¡Œè®­ç»ƒ**ã€‚



* æœ€å…³é”®çš„åœ°æ–¹å°±æ˜¯`Bi-LSTM`çš„å‚æ•°æ˜¯ç”Ÿæˆçš„ï¼Œä¸åŒdomainçš„ä¸åŒtaskéœ€è¦çš„LSTMçš„å‚æ•°å’Œ$I$æœ‰å…³

$$
\begin{equation}
\theta_{\mathrm{LSTM}}^{d, t}=\mathbf{W} \otimes \mathbf{I}_{d}^{D} \otimes \mathbf{I}_{t}^{T},
\end{equation}
$$





**NERçš„æœªæ¥**

æ—¢ç„¶æ¨¡å‹æ‰“ä¸åŠ¨äº†ï¼Œç„¶åæˆ‘æ‰¾äº†æ‰¾ ACL2020åšNERçš„è®ºæ–‡ï¼Œçœ‹çœ‹ç°åœ¨çš„NERè¿˜åœ¨åšå“ªäº›äº‹æƒ…ï¼Œä¸»è¦åˆ†å‡ ä¸ªæ–¹é¢

1. **å¤šç‰¹å¾**ï¼šå®ä½“è¯†åˆ«ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤æ‚çš„ä»»åŠ¡ï¼Œä¸éœ€è¦å¤ªæ·±å…¥çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå°±æ˜¯åŠ ç‰¹å¾ï¼Œç‰¹å¾è¶Šå¤šæ•ˆæœè¶Šå¥½ï¼Œæ‰€ä»¥å­—ç‰¹å¾ã€è¯ç‰¹å¾ã€è¯æ€§ç‰¹å¾ã€å¥æ³•ç‰¹å¾ã€KGè¡¨å¾ç­‰ç­‰çš„å°±ä¸€ä¸ªä¸ªåŠ å§ï¼Œç”šè‡³æœ‰äº›ä¸­æ–‡ NER ä»»åŠ¡é‡Œè¿˜åŠ å…¥äº†æ‹¼éŸ³ç‰¹å¾ã€ç¬”ç”»ç‰¹å¾...... å¿ƒæœ‰å¤šå¤§ï¼Œç‰¹å¾å°±æœ‰å¤šå¤š
2. **å¤šä»»åŠ¡**ï¼šå¾ˆå¤šæ—¶å€™åš NER çš„ç›®çš„å¹¶ä¸ä»…æ˜¯ä¸ºäº† NERï¼Œè€Œæ˜¯æœåŠ¡äºä¸€ä¸ªæ›´å¤§çš„ç›®æ ‡æˆ–ç³»ç»Ÿï¼Œæ¯”å¦‚ä¿¡æ¯æŠ½å–ã€é—®ç­”ç³»ç»Ÿç­‰ç­‰ã€‚å¦‚æœæŠŠæ•´ä¸ªå¤§ä»»åŠ¡åšä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œå°±éœ€è¦åšæˆä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼ŒæŠŠ NER ä½œä¸ºå…¶ä¸­ä¸€ä¸ªå­ä»»åŠ¡ï¼›å¦å¤–ï¼Œå•çº¯çš„ NER ä¹Ÿå¯ä»¥åšæˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚å®ä½“ç±»å‹è¿‡å¤šæ—¶ï¼Œä»…ç”¨ä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡æ¥åŒæ—¶æŠ½å–å®ä½“ä¸åˆ¤æ–­å®ä½“ç±»å‹ï¼Œä¼šæœ‰äº›åŠ›ä¸ä»å¿ƒï¼Œå°±å¯ä»¥æ‹†æˆä¸¤ä¸ªå­ä»»åŠ¡æ¥åš
3. **æ—¶ä»¤å¤§æ‚çƒ©**ï¼šæŠŠå½“ä¸‹æ¯”è¾ƒæµè¡Œçš„æ·±åº¦å­¦ä¹ è¯é¢˜æˆ–æ–¹æ³•è·Ÿ NER ç»“åˆä¸€ä¸‹ï¼Œæ¯”å¦‚ç»“åˆå¼ºåŒ–å­¦ä¹ çš„ NERã€ç»“åˆ **few-shot learning** çš„ NERã€ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯çš„ NERã€ç»“åˆè·¨è¯­ç§å­¦ä¹ çš„ NER ç­‰ç­‰çš„ï¼Œå…·ä½“å°±ä¸æäº† (Few-shot + Cross-domainæ˜¯ä¸ªä¸é”™çš„é€‰é¡¹ï¼)

ä½œè€…ï¼šç‹å²³ç‹é™¢é•¿
é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/166496466
æ¥æºï¼šçŸ¥ä¹
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚

---



### :older_man: Text Classification

:fire: :hammer_and_wrench: **Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classifification**, in ACL 2022. [[pdf](https://arxiv.org/abs/2108.02035)] [[torch]](https://github.com/thunlp/KnowledgeablePromptTuning)] [[çŸ¥ä¹åšå®¢](https://zhuanlan.zhihu.com/p/398009000)]

> å°†å¤–éƒ¨çŸ¥è¯†èå…¥å½“prompt-tuningå½“ä¸­åšTCä»»åŠ¡

* ä»€ä¹ˆæ˜¯ Prompt-tuning
  * ä¹‹å‰çš„Prompt-tuningæ–¹æ³•å¯è¢«ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œå…·ä½“æ–¹å¼æ˜¯é€šè¿‡æ„å»ºæ ‡ç­¾è¯è¡¨ï¼Œå°†åˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé¢„æµ‹å’Œæ ‡ç­¾ç›¸å…³è¯çš„é—®é¢˜ã€‚å› æ­¤ï¼Œè¿™æ ·çš„é—®é¢˜å¯ä»¥æ„å»ºä¸€ä¸ªå«æœ‰[MASK]çš„æ¨¡æ¿ï¼Œç„¶åè®©MLMï¼ˆæ©ç è¯­è¨€æ¨¡å‹ï¼‰å»é¢„æµ‹[MASK]ä½ç½®çš„å•è¯ã€‚è‡³æ­¤ï¼Œåˆ†ç±»ä»»åŠ¡è¢«è½¬åŒ–ä¸ºäº†ä¸€ä¸ªæ©ç è¯­è¨€å»ºæ¨¡é—®é¢˜ã€‚
  * ä¸‹é¢ç»™å‡ºä¸€ä¸ªä¾‹å­ï¼Œå½“æˆ‘ä»¬è¦å¯¹ä¸€ä¸ªå¥å­è¿›è¡Œåˆ†ç±»æ—¶ï¼Œå¯ä»¥å°è¯•æ„å»ºä¸‹é¢çš„æ¨¡æ¿ï¼š**A [MASK] question: x**
  * æ¯”å¦‚MLMé¢„æµ‹å‡ºåœ¨[MASK]ä½ç½®æ¦‚ç‡æœ€é«˜çš„è¯æ˜¯scienceï¼Œé‚£è¯¥å¥å¯ä»¥è¢«åˆ†ç±»ä¸ºSCIENCEç±»åˆ«ã€‚
* åŠ¨æœº
  * è¿‡å»prompy-tuningæ–¹æ³•ï¼ŒMLMåœ¨[MASK]ä½ç½®å¯ä»¥é¢„æµ‹å‡ºçš„å•è¯æ˜¯å¾ˆå¤šçš„ï¼Œç„¶è€Œç±»åˆ«æ•°åªæœ‰ç‰¹å®šæ•°é‡çš„ï¼Œå› æ­¤è¯¥é—®é¢˜å¾ˆé‡è¦çš„ä¸€ä¸ªéƒ¨åˆ†æ˜¯å¦‚ä½•==**æ„å»ºä¸€ä¸ªå•è¯è¡¨åˆ°ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„**==ã€‚è¿™ä¸ªæ˜ å°„èƒ½è®©MLMåœ¨é¢„æµ‹åˆ°ç±»åˆ«æ ‡ç­¾çš„ç›¸å…³è¯æ—¶ï¼Œå°±å¯ä»¥è¢«åˆ†åˆ°æŒ‡å®šç±»åˆ«å»ã€‚
  * è¿™æ ·çš„ä¸€ä¸ªæ˜ å°„ï¼Œè¿‡å»é€šå¸¸æ˜¯ç”±äººæ¥æ‰‹å·¥ç¼–è¾‘æˆ–ä½¿ç”¨æ¢¯åº¦ä¸‹é™æœç´¢ã€‚**ä½†æ˜¾ç„¶è¿™æ ·ä¼šå¸¦æ¥è¦†ç›–èŒƒå›´ä¸å…¨å¯¼è‡´çš„é«˜åå·®å’Œé«˜æ–¹å·®**ã€‚**çŸ¥è¯†åº“çš„ç»„ç»‡ç»“æ„ï¼Œå¤©ç„¶çš„å¸¦æœ‰èŒƒå›´å…³ç³»**ï¼Œåœ¨çŸ¥è¯†åº“çš„å›¾ç»“æ„ä¸­ï¼Œç›¸å…³è”çš„å®ä½“ä¼šæœ‰è¾¹ç›¸è¿ï¼Œä¸ç›¸å…³çš„å®ä½“å¯èƒ½è¦ç»è¿‡å¾ˆå¤šè·³æ‰èƒ½æ‰¾åˆ°å…³è”ï¼Œæˆ–æ— å…³è”ã€‚å› æ­¤å¦‚æœèƒ½å°†å¤–éƒ¨çš„çŸ¥è¯†åº“ä¿¡æ¯èå…¥ï¼Œæ„å»ºä¸€ä¸ªæ˜ å°„å™¨ï¼ˆæœ¬æ–‡ç§°è¯­è¨€è¡¨è¾¾å™¨ï¼‰ï¼Œå°±å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šé¿å…æ‰‹å·¥æ„é€ å’Œæ¢¯åº¦ä¸‹é™å¸¦æ¥çš„é«˜åå·®å’Œé«˜æ–¹å·®é—®é¢˜ã€‚
* æ–¹æ³•

![image-20221202123732640](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221202123732640.png)

---



### :framed_picture: Topic 

:fire: **[å±‚æ¬¡åŒ–ä¸»é¢˜] Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC**, in ICML 2017. [[pdf](https://proceedings.mlr.press/v70/cong17a.html)]

> ç†è®ºæ€§éå¸¸å¼ºçš„è®ºæ–‡
>
> * è¯¦ç»†çš„ç†è®ºæ¨å¯¼è¯·è§åŸæ–‡ã€‚

* caseå±•ç¤º

![image-20221031220028391](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221031220028391.png)



## :mailbox: Knowledge&LLM&VLM

### :rocket: LLM

:fire: :hammer_and_wrench: **Quark: Controllable Text Generation with Reinforced [Un]learning**, in NIPS 2022. [[pdf](https://arxiv.org/abs/2205.13636)] [[torch](https://github.com/GXimingLu/Quark)]

* åŠ¨æœº
  * æ•´ä½“æ€è·¯å’Œ`InstructGPT`å¾ˆç±»ä¼¼ï¼Œä½†æ˜¯ä¸å†è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨promptæ¥å–ä»£ï¼ˆè¯„åˆ¤ç”Ÿæˆçš„æ˜¯å¦æ˜¯å¥½å¥å­ï¼‰
* ä¸Instruct GPT ä¸åŒçš„åœ°æ–¹
  1. **æŠ›å¼ƒè®­ç»ƒæ‰“åˆ†æ¨¡å‹ï¼ˆreword modelï¼‰è¿™ä¸ªæ­¥éª¤**ï¼Œè§‰å¾—è®­ç»ƒä¸€ä¸ªæ‰“åˆ†æ¨¡å‹è¿‡äºç¹çï¼Œå¢åŠ è®­ç»ƒæˆæœ¬
  2. ç„¶è€Œä¹Ÿè¦ä¿è¯äººå·¥ç›‘ç£çš„ä¿¡æ¯ï¼ˆhuman feedbackï¼‰ä¼ å…¥ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œè®ºæ–‡æå‡ºä½¿ç”¨ä¸€ä¸ª==**æ‰“åˆ†æ ‡è®°**==ï¼ˆreward tokenï¼‰åµŒå…¥promptä¸­æ¥æ§åˆ¶ç”Ÿæˆï¼Œå¦‚æ­¤å¯ä»¥æ›´ä¸ºç®€æ´åœ°å®ç°human feedback+reinforcement learningçš„è®­ç»ƒæŠ€æœ¯ç­–ç•¥ã€‚
* æ¨¡å‹æ–¹æ³•
  1. **Explorationï¼Œ**åŒæ ·åŸºäºä¸€ä¸ªå¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆGPTï¼‰äº§ç”Ÿä¸€æ‰¹promptç”Ÿæˆç»“æœï¼Œæ¥ç€å¯¹å…¶è¯„ä»·æ‰“åˆ†ï¼Œå¹¶å­˜å‚¨ä¸€ä¸ªæ•°æ®æ± ä¸­ï¼ˆdata poolï¼‰
  2. **Quantizationï¼Œ**åˆ©ç”¨æ‰“åˆ†çš„ç»“æœï¼Œç»™æ•°æ®æ± ä¸­ä¸­çš„æ•°æ®è¿›è¡Œæ’åºï¼Œç„¶åè¿›è¡Œå±‚çº§é‡åŒ–åˆ†ç±»ï¼ˆå¦‚åˆ†å¥½ï¼Œä¸­ã€å·®ç±»ä¼¼ï¼‰
  3. **Learningï¼Œ**å°†å·²ç»åˆ†å±‚çš„æ ·æœ¬ï¼Œç»™ä¸€ä¸ªæ‰“åˆ†æ ‡è®°ï¼ˆå›¾ä¸­[R3]ï¼‰ä¸promptæ‹¼æ¥åœ¨ä¸€èµ·ä½œä¸ºè¾“å…¥ï¼Œå¯¹åŸå§‹çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒè®­ç»ƒå­¦ä¹ ã€‚

![image-20230412090327306](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230412090327306.png)

:fire: :hammer_and_wrench: **Large Language Models Are Human-Level Prompt Engineers**, in ICLR 2023. [[pdf](https://arxiv.org/abs/2211.01910)] [[torch](https://sites.google.com/view/automatic-prompt-engineer)] [[zhihu](https://zhuanlan.zhihu.com/p/619112790)]

* åŠ¨æœº
  * ä¸€å¥è¯æ¥æ¦‚æ‹¬æœ¬æ–‡çš„æ€è·¯: åˆ©ç”¨è®­ç»ƒæ•°æ®æ„é€ demonstrationï¼Œè¦æ±‚LLMåœ¨æŒ‡å®šæè¿°çš„æƒ…å¢ƒä¸‹ï¼Œäº§ç”Ÿå¯¹åº”çš„prompté›†åˆï¼Œç„¶åè¯„ä¼°é€‰æ‹©å‡ºæœ€ä½³çš„instructionã€‚
* å…·ä½“æ€è·¯
  1. ä½¿ç”¨LLMä½œä¸ºæ¨ç†æ¨¡å‹ï¼ŒåŸºäº**è¾“å…¥å’Œè¾“å‡ºçš„æè¿°é›†åˆ**ï¼Œäº§ç”Ÿ**æŒ‡ä»¤å€™é€‰é›†**ã€‚
  2. å¯¹äºå€™é€‰é›†ä¸­çš„æ¯ä¸ªæŒ‡ä»¤ï¼Œè®¡ç®—è¾“å…¥å’Œè¾“å‡ºLLMä¸‹çš„å¾—åˆ†ã€‚
  3. ä½¿ç”¨è¿­ä»£è’™ç‰¹å¡æ´›çš„æœç´¢æ–¹å¼ï¼Œé€šè¿‡æå‡ºè¯­ä¹‰ç›¸ä¼¼çš„æŒ‡ä»¤å˜ä½“æ¥æ”¹è¿›æœ€ä½³çš„æŒ‡ä»¤å€™é€‰ï¼Œå¢åŠ æŒ‡ä»¤çš„å¤šæ ·æ€§ã€‚

![image-20230508105808096](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230508105808096.png)

:fire: **[2023.5.6æœªå¼€æº] SCOTT: Self-Consistent Chain-of-Thought Distillation**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.01879)]

* åŠ¨æœº

  * åªæœ‰æ¯”è¾ƒå¤§çš„LLMæ‰æœ‰COTçš„èƒ½åŠ›ï¼Œè€Œä¸”å¤§çš„LLMä¹Ÿä¼šäº§ç”Ÿå¾ˆå¤šå¹»è§‰ä¿¡æ¯ï¼Œå¯¼è‡´è’¸é¦å‡ºæ¥çš„å°æ¨¡å‹åœ¨COTèƒ½åŠ›ä¸Šä¼šå¾ˆå·®

  ![image-20230506092740246](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230506092740246.png)

* æ–¹æ³•ã€teacherï¼šGPT-neox20B studentï¼šT5 3Bã€‘

  * å¤§æ¨¡å‹ä¸­é‡‡ç”¨Contrastive Decodingç¼–ç çš„ç­–ç•¥ï¼ˆæ„å‘³ç€ä½ å¿…é¡»è·å¾—è¯æ¦‚ç‡çš„åˆ†å¸ƒï¼Œæ¨¡å‹å¯ä»¥frozenæ‰ï¼‰
  * å°æ¨¡å‹å½“ä¸­åˆ©ç”¨å¤§æ¨¡å‹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆåŒæ—¶è®©teacherç”Ÿæˆä¸€äº›åäº‹å®çš„ä¹Ÿä¸€èµ·ä¸¢è¿›å»fine-tuningï¼‰

   ![image-20230506093023819](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230506093023819.png)

  * ç»†èŠ‚

    * teacherçš„contrastive decoingçš„ç­–ç•¥

    ![image-20230506093124595](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230506093124595.png)

    * studentçš„åäº‹å®æ¨ç†èƒ½åŠ›ï¼ˆéƒ½æ˜¯ç”±teacheräº§ç”Ÿçš„ï¼Œå…·ä½“æ¥è¯´å°±æ˜¯æŠŠteacherä¸­è¾“å…¥çš„ç­”æ¡ˆæ¢æˆé”™è¯¯çš„ç­”æ¡ˆæˆ–è€…ç©ºçš„å­—ç¬¦å°±å¯ä»¥äº†ï¼‰

    ![image-20230506093251574](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230506093251574.png)

:fire: **Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes**

> æ–¹æ³•ä¹Ÿå¤ªç®€å•äº†ï¼Œå…¶å®å°±æ˜¯å½“å‰LLMçš„èŒƒå¼ï¼Œç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸œè¥¿æ¥è’¸é¦å­¦ä¹ å°æ¨¡å‹

* æ–¹æ³•

![image-20230506095246261](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230506095246261.png)



:fire: **Poisoning Language Models During Instruction Tuning**, in ICML 2023. [[pdf](https://arxiv.org/abs/2305.00944)] [[torch](https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models[)]

* åŠ¨æœº

  * åœ¨è®­ç»ƒæ—¶å€™åŠ å…¥**è„æ•°æ®æ”»å‡»è¯­è¨€æ¨¡å‹**

  ![image-20230508103121615](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230508103121615.png)



:fire: :hammer_and_wrench: **Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation**, in ICML 2023. [[pdf](https://arxiv.org/abs/2305.00909)] [[torch](https://github.com/VITA-Group/ChainCoder)]

* åŠ¨æœº

  * è¿‡å»çš„ä»£ç ç”Ÿæˆéƒ½æ˜¯ä¸€æ¬¡æ€§çš„ï¼Œæ•ˆæœå¹¶ä¸å¥½ï¼Œåº”è¯¥COTé‚£æ ·å­ç”Ÿæˆ
  * å¿½ç•¥äº†å¥æ³•ç»“æ„ä½œä¸ºå…ˆéªŒ

* æœ€å¤§è´¡çŒ®

  * æå‡ºåŸºäºå¥æ³•æ ‘çš„Tokenizerï¼ˆæœ€é‡è¦çš„è´¡çŒ®ï¼‰

* æ–¹æ³•

  * Tokenizer

  ![image-20230508194557470](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230508194557470.png)

  $S3$å’Œ$S4$æ˜¯é‡ç‚¹ï¼Œtokenizerçš„encodeä¼ªä»£ç å¦‚ä¸‹ï¼š

  ![image-20230508194659740](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230508194659740.png)

* æ¨¡å‹ï¼ˆé¢„è®­ç»ƒ+å¾®è°ƒï¼‰

![image-20230508194740606](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230508194740606.png)



:hammer_and_wrench: **Distilling Script Knowledge from Large Language Models for Constrained Language Planning**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.05252)] [[torch](https://github.com/siyuyuan/coscript)]

* åŠ¨æœºï¼šæœ‰çº¦æŸçš„planç”Ÿæˆï¼Œæ¯”å¦‚è¯´ç»™ä¸€ä¸ªç³–å°¿ç—…äººåšè›‹ç³•åº”è¯¥ä¸è¦åŠ ç³–

![image-20230513212959042](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230513212959042.png)

* æœ‰çº¦æŸæ•°æ®é›†ç”Ÿæˆæ–¹å¼ä¸æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆå…ˆè¿‡åº¦ç”Ÿæˆå†è¿‡æ»¤ï¼‰

![image-20230513213047184](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230513213047184.png)



:fire: :hammer_and_wrench: **BadPrompt: Backdoor Attacks on Continuous Prompts**, in NIPS 2022.  [[pdf](https://arxiv.org/abs/2211.14719)] [[torch](https://github.com/papersPapers/BadPrompt)]

* åŠ¨æœº
  * æ²¡æœ‰äººç ”ç©¶è¿‡prompt-based model çš„åé—¨æ”»å‡»
  * few-shotçš„åœºæ™¯å¾€å¾€ç¼“è§£äº†æ”»å‡»ï¼Œä¼šå¯¼è‡´æ”»å‡»ä¸å¤Ÿæœ‰æ•ˆã€‚æ‰€ä»¥éœ€è¦æ›´é«˜çº§çš„æ”»å‡»ç­–ç•¥
  * ç›®æ ‡å¸Œæœ›ASRå’ŒCAéƒ½æ¯”è¾ƒé«˜

![image-20230516212217595](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516212217595.png)

* æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20230516212243228](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516212243228.png)



:fire: :hammer_and_wrench: **RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.08844)] [[torch](https://github.com/feyzaakyurek/rl4f[)]

> å¤ªæ£’çš„å·¥ä½œäº†ï¼

* åŠ¨æœº
  * è¿‡å»åé¦ˆéƒ½æ˜¯äººæ¥ç”Ÿæˆçš„ï¼Œèƒ½ä¸èƒ½ç”¨ä¸€ä¸ªæ¨¡å‹æ¥ç”Ÿæˆï¼Œç„¶åä¿®æ”¹é‚£ç§é»‘ç›’LLM

![image-20230516215115928](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516215115928.png)

* æ–¹æ³•ï¼ˆå¤ªæ£’çš„æ–¹æ³•äº†ï¼‰ã€æ‰¹è¯„çš„æ¨¡å‹å°±æ˜¯ä¸€ä¸ª`T5-large` (0.77M)ã€‘

![image-20230516215150724](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230516215150724.png)



:fire: :hammer_and_wrench: **Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark**, in ACL 2023. [[pdf]](https://arxiv.org/abs/2301.10226)] [[torch](https: //github.com/yjw1029/EmbMarker)]

* åŠ¨æœº
  * åˆ©ç”¨åé—¨æ¥ä¿æŠ¤è¯­è¨€æ¨¡å‹çš„ç‰ˆæƒ

![image-20230518162732603](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230518162732603.png)

* æ–¹æ³•

![image-20230518162756186](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230518162756186.png)



:hammer_and_wrench: :fire: **Tree of Thoughts: Deliberate Problem Solving with Large Language Models**, in 2023 05. [[pdf](https://arxiv.org/abs/2305.10601)] [[torch](https://github.com/ysymyth/tree-of-thought-llm)]

* [çŸ¥ä¹é“¾æ¥](https://zhuanlan.zhihu.com/p/631940032)
* æ¨¡å‹æ–¹æ³• TOT

![image-20230531094920757](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230531094920757.png)

* TOTè§£å†³é—®é¢˜ä¾‹å­

![image-20230531094947180](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230531094947180.png)

![image-20230531095005174](/Users/gary/Library/Application Support/typora-user-images/image-20230531095005174.png)

![image-20230531095334358](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230531095334358.png)



:hammer_and_wrench: **Plug-and-Play Knowledge Injection for Pre-trained Language Models**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.17691)] [[torch](https://github.com/ THUNLP/Knowledge-Plugin)]

* åŠ¨æœº
  * è¿‡å»åœ¨LMä¸­åŠ å…¥å¤–éƒ¨çŸ¥è¯†å¯ä»¥æé«˜æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½
  * ä½†æ˜¯å¾€å¾€éœ€è¦é‡æ–°è®­ç»ƒæˆ–è€…å¾®è°ƒLMï¼Œæ˜¾å¾—å¾ˆéº»çƒ¦
  * æœ¬æ–‡æå‡ºåªè®­ç»ƒä¸€ä¸ªMappingçš„ç½‘ç»œï¼Œå°†å¤–éƒ¨çŸ¥è¯†æŠ•å°„åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹å½“ä¸­ï¼ˆæœ¬æ–‡æ¢ç´¢äº†å¾ˆå¤šç§LMçš„æ¨¡å‹ï¼‰
* æ¨¡å‹æ–¹æ³•

![image-20230602105324893](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230602105324893.png)



:hammer_and_wrench: :fire: **Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Modelsâ€™ Memories**, in ACL 2023. [[pdf](https://arxiv.org/abs/2306.05406)] [[torch](https://github.com/ Amano-Aki/Mixture-of-Domain-Adapters)]

* åŠ¨æœº
  * å¸Œæœ›ç”¨Adapterçš„æ–¹å¼å°†PLMåœ¨æŸäº›ç‰¹å®šé¢†åŸŸä¸­å°å‚æ•°å½¢å¼å¾®è°ƒ
* æ–¹æ³•ï¼ˆä¸¤é˜¶æ®µï¼‰

![image-20230611215226591](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230611215226591.png)

* å®éªŒ
  * Baselineé€‰å–å°±æ˜¯å…¶ä»–çš„å¾®è°ƒæ–¹æ³•ï¼Œæ¯”å¦‚LoRaç­‰æ–¹å¼ï¼ˆå®éªŒç»“æœè¯¦è§è®ºæ–‡ï¼‰

:fire: :hammer_and_wrench: **Grounding Language Models to Images for Multimodal Inputs and Outputs**, in ICML 2023. [[pdf]](https://arxiv.org/abs/2301.13823) [[torch](https://jykoh.com/fromage)]

> å•å¡A6000è¿›è¡Œè®­ç»ƒï¼Œå¾ˆå‹å¥½

* åŠ¨æœº
  * åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡çš„è¾“å…¥å’Œè¾“å‡ºï¼ˆæ³¨æ„ï¼Œè¿™é‡Œçš„å›¾ç‰‡æ˜¯æ£€ç´¢å‡ºæ¥çš„ï¼‰

![image-20230613094222645](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230613094222645.png)

* æ–¹æ³•ï¼ˆä¸¤ç§å¾ˆç®€å•çš„ä¸è®­ç»ƒæ–¹æ³•ï¼‰
  * CLIPå’ŒLLMéƒ½æ˜¯å†»ç»“å‚æ•°çš„

![image-20230613094249427](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230613094249427.png)

* ä¾‹å­å±•ç¤º
  * æ¨ç†æ—¶å€™å¦‚ä½•è¿›è¡Œçš„ï¼Œå…·ä½“è¦è§è®ºæ–‡ä»£ç 

![image-20230613094451233](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230613094451233.png)



:hammer_and_wrench: **[COT] Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.482/)] [[code](https://github.com/Alsace08/SumCoT)]

> ç›¸å½“äºé‡æ„äº†ç°åœ¨çš„Summarizationæ•°æ®é›†

* åŠ¨æœº
  * è¿‡å»çš„benchmarkçš„å‚è€ƒsummarizationå¾€å¾€ç»“æœæ˜¯å˜ˆæ‚çš„ï¼Œä¸»è¦æ˜¯åœ¨äº‹å®å¹»è§‰å’Œä¿¡æ¯å†—ä½™æ–¹é¢
  * æˆ‘ä»¬å¸Œæœ›å°±ä¸»è¦å…³ğŸ§˜å…ƒç´ æ¥ç”Ÿæˆæ‘˜è¦

![image-20230717204206612](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230717204206612.png)

* æ–¹æ³•ï¼ˆç»™å®šå‡ ä¸ªå‚è€ƒé—®é¢˜è¿›è¡Œæé—®ï¼‰

![image-20230717204227177](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230717204227177.png)



:fire: :hammer_and_wrench: **Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.74/)]  [[torch](https://github.com/microsoft/dp-transformers)]

* åŠ¨æœº
  * æ–‡æœ¬ç”Ÿæˆçš„éšç§æ€§ï¼Œä¸è¦è®©è¯­è¨€æ¨¡å‹æ³„æ¼ä¸€äº›éšç§ä¿¡æ¯
  * å¼•å…¥å·®åˆ†éšç§çš„æ–¹æ³•æ¥Fine-tune GPT-2
* æ–¹æ³•ï¼ˆå…·ä½“æˆ‘ä¹Ÿä¸æ˜¯å¾ˆçœ‹å¾—æ‡‚ï¼Œå¯èƒ½è¦çœ‹ä»£ç æ‰çŸ¥é“å…·ä½“æ“ä½œï¼‰

![image-20230725170115857](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725170115857.png)

![image-20230725170734255](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230725170734255.png)



:fire: :hammer_and_wrench: **Editing Large Language Models: Problems, Methods, and Opportunities**, in 2023.05. [[pdf](https://arxiv.org/abs/2305.13172)] [[torch](https://github.com/zjunlp/EasyEdit)]

* åŠ¨æœº
  * LLMçŸ¥è¯†å®šå‘ç¼–è¾‘çš„empirical study

![image-20230902184416729](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902184416729.png)



:fire: :hammer_and_wrench: **LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2212.04088)] [[torch](https://dki-lab.github.io/LLM-Planner/)]

* åŠ¨æœºï¼šLLMè§£å†³å¯¼èˆªé—®é¢˜

![image-20231013234300711](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231013234300711.png)

* æ¨¡å‹æ–¹æ³•

![image-20231013234456103](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231013234456103.png)



:fire: :hammer_and_wrench: **CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2309.01093)] [[torch](https://toneyaya.github.io/cotdet)]

* åŠ¨æœº
  * è§£å†³Task-drivençš„ODä»»åŠ¡ï¼Œéœ€è¦è§†è§‰çŸ¥è¯†ä½œä¸ºæ¡¥æ¢ï¼ŒLLMå•ç‹¬å¾ˆé‚£åšåˆ°

![image-20231116103832300](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116103832300.png)

* æ–¹æ³•ï¼ˆæŒºå¥½çš„æ–¹æ³•ï¼‰

![image-20231116104002333](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116104002333.png)

### :hammer: Code

:hammer_and_wrench: **AST-Trans: Code Summarization with Efficient Tree-Structured Attention**, in ICSE 2022. [[pdf](https://dl.acm.org/doi/pdf/10.1145/3510003.3510224)] [[torch](https://github.com/zetang94/ICSE2022_AST_Trans)]

> ä»£ç æ‘˜è¦çš„ç”Ÿæˆ

* åŠ¨æœº
  * è¿‡å»ä»£ç è½¬æˆASTçš„æ–¹å¼ï¼ŒTreeå¤ªé•¿äº†å¾ˆéš¾è®©æ¨¡å‹å­¦åˆ°èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»
  * è¿™é‡Œæˆ‘ä»¬åªå…³æ³¨å…³é”®çš„ä¸¤ç§èŠ‚ç‚¹ï¼š**ç¥–å…ˆ-åä»£èŠ‚ç‚¹**å’Œ**å…„å¼ŸèŠ‚ç‚¹**

![image-20230518112438244](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230518112438244.png)

* æ–¹æ³•

  * è½¬æ¢æˆçŸ©é˜µæœ‰æ•ˆçš„encoding

  ![image-20230518112515126](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230518112515126.png)

  * åé¢è¿˜æœ‰ä¸€ç³»åˆ—å¾ˆç»†èŠ‚çš„æ“ä½œï¼ˆä¸å¤ªæ¸…æ¥šæœ‰æ²¡æœ‰é“ç†ï¼‰

* å®éªŒ
  * å¤æ‚åº¦åˆ†æ
  * å¾ˆç»†èŠ‚çš„æ¶ˆè
  * è¿™ä¼šè®®å±…ç„¶å†™10é¡µçš„è®ºæ–‡ï¼ŒçœŸå¤š



:fire: :hammer_and_wrench: **Multi-target Backdoor Attacks for Code Pre-trained Models**, in ACL 2023. [[pdf](https://arxiv.org/abs/2306.08350)] [[torch](https://github.com/Lyz1213/Backdoored_PPLM)]

> ä»£ç ä»“åº“å¾ˆå®Œå–„ï¼Œä½†è¿™ç¯‡è®ºæ–‡å†™ä½œä¸€èˆ¬èˆ¬å§

* åŠ¨æœº
  * åœ¨é¢„è®­ç»ƒæœŸé—´å¯¹ä»£ç æ¨¡å‹æ³¨å…¥åé—¨
  * é’ˆå¯¹ä¸¤ç§ä»»åŠ¡ï¼šåˆ†ç±»ä»»åŠ¡ + ç”Ÿæˆä»»åŠ¡
* æ–¹æ³•

![image-20230620211344890](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230620211344890.png)

* æ”»å‡»æµç¨‹ä¸æ€è·¯
  1. è®¾ç½®Trigger Set
     1. è‡ªç„¶è¯­è¨€çš„Trigger
     2. ä»£ç Trigger
        * å¿…é¡»ä¿ç•™åŸå§‹æ„æ€
        * æ°¸çœŸçš„assertè¯­å¥æˆ–è€…æ°¸å‡çš„ifè¯­å¥
        * ä¿è¯ä¸ä¼šè¢«ç¼–è¯‘å™¨å¿½ç•¥ï¼ˆå¦‚ï¼Œif(sin(0.7)<1)ï¼‰
     3. Trigger Output (Target Label)
        1. åˆ†ç±»ä»»åŠ¡ï¼šç‰¹å®šæ ‡ç­¾å°±å¥½
        2. ç”Ÿæˆä»»åŠ¡ï¼šå¢åˆ æ”¹
  2. é¢„è®­ç»ƒä»»åŠ¡ (æ³¨å…¥åé—¨)
     1. Seq2Seq Learning
        1. ä¸‰åˆ†ç±»ï¼ˆå¢åˆ æ”¹ï¼‰
        2. NL <---> Code
     2. Tokenè¡¨ç¤ºå­¦ä¹ ï¼ŒEOSåšæ–‡ç« 
  3. æ¨¡å‹éƒ¨ç½²
* æ¨¡å‹é¢„è®­ç»ƒä¸æ•°æ®é›†
  * CodeT5å’ŒPLBART
  * CodeSearchNetçš„æ•°æ®é›†æ¥æ”»å‡»
* æ”»å‡»ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå®éªŒï¼‰
  * ä»£ç understanding
  * ä»£ç ç”Ÿæˆ



### :rainbow: VLM

> å¤šæ¨¡æ€é¢„è®­ç»ƒ

:fire: :hammer_and_wrench: **[Cross-Modal&Contrastive Learning] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning**, in ACL(long paper) 2021. [[pdf](https://aclanthology.org/2021.acl-long.202/)] [[project from Baidu](https://unimo-ptm.github.io/)]

:hammer_and_wrench: **[MultiModal] UniT: Multimodal Multitask Learning with a Unified Transformer**, ICCV 2021. [[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.pdf)] [[project from Fair](https://mmf.sh/)]



:fire: **:hammer_and_wrench:** **Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.196/)] [[torch (2022.12.01æœªå¼€æº)](https://github.com/INK-USC/CMKT)]

> æœ¬æ–‡æ˜¯ä¸€ç¯‡**å®éªŒæ€§çš„æ–‡ç« **ï¼Œå®éªŒçš„æ–¹æ³•å†™å¾—ä¸é”™ï¼
>
> ä»€ä¹ˆå«åšï¼š**intermediate pre-training**?
>
> * åœ¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ä¸Šï¼Œè¡¥å……ä¸€äº›æ•°æ®é›†æˆ–è€…è¯­æ–™åº“ç»§ç»­é¢„è®­ç»ƒ

* åŠ¨æœº

  * ç›®å‰çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨ç†è§£**æ—¥å¸¸ç‰©å“å±æ€§**æ–¹é¢çš„èƒ½åŠ›è¿˜å¾ˆç¼ºä¹ï¼Œå› ä¸ºå®ƒä»¬çš„é¢„è®­ç»ƒè¯­æ–™ä¸­**å¾ˆå°‘æœ‰è¿™æ ·çš„äº‹å®æ€§çŸ¥è¯†**ï¼Œå³æ‰€è°“çš„reporting biasï¼Œä¸‹å›¾å³æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

  ![image-20221201194524047](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221201194524047.png)

* æ–¹æ³•ï¼ˆä»¥ä¸‹ä¸¤ç§æ–¹æ³•æ¥å¼¥è¡¥PLMåœ¨è¿™æ–¹é¢çš„ç¼ºé™·ï¼‰

  * text knowledge transfer: å³ä½¿ç”¨image captionè¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œè®¾è®¡äº†å¦‚ä¸‹ä¸¤ä¸ªè®­ç»ƒç›®æ ‡ï¼š

    * MLMï¼šåœ¨image captionä¸Šè¿›è¡ŒMLMï¼Œç›¸å½“äºè¿›è¡Œäº†domain adaptiveçš„é¢„è®­ç»ƒï¼Œå’ŒBertä¸€æ ·çš„è®¾ç½®
    * Text Contrastive Learning (TCL): é‡‡ç”¨å’ŒSimCSEä¸€æ ·çš„æ–¹å¼ï¼Œè¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œbatchä¸­çš„å…¶ä»–æ ·æœ¬éƒ½æ˜¯è´Ÿæ ·æœ¬

  * cross-modal knowledge transfer: å³ä½¿ç”¨å›¾ç‰‡å’Œæ–‡æœ¬ä»¥åŠ`V&L`çš„è®­ç»ƒæ¥è¿›è¡ŒçŸ¥è¯†è¿ç§»ï¼Œè®¾è®¡äº†å¦‚ä¸‹å‡ ä¸ªè®­ç»ƒç›®æ ‡ï¼š

    * Voken Classification: vokené‡‡ç”¨tokenå±‚é¢çš„text2imageæ£€ç´¢æ¥è¿ç§»è§†è§‰çŸ¥è¯†ï¼Œå®ƒå‡è®¾æ¯ä¸€ä¸ªtokenéƒ½æœ‰ä¸€ä¸ªè§†è§‰åŸŸçš„vokenä¸å…¶å¯¹åº”ï¼Œè®­ç»ƒç›®æ ‡å°±æ˜¯åœ¨æ‰€æœ‰é¢„å…ˆè®¾å®šå¥½çš„vokenä¸­å°†æ­£ç¡®çš„vokenæ£€ç´¢å‡ºæ¥

    * Masked Language Modeling with Visual Clues: ç»™å®šå›¾ç‰‡ä½œä¸ºçº¿ç´¢ï¼Œé¢„æµ‹maskæ‰çš„tokenï¼Œæ¯”MLMå¤šäº†å›¾ç‰‡ä½œä¸ºè¾“å…¥ï¼Œç›®æ ‡å‡½æ•°æ˜¯ä¸€æ ·çš„

    * Cross-Modal Contrastive Learning (CMCL): å’ŒCLIPä¸€æ ·ï¼ˆä¸åŒåœ¨äºè¿™é‡ŒæŠŠ**è§†è§‰ç½‘ç»œç»™å†»ä½**ï¼‰ï¼Œæ˜¯è·¨æ¨¡æ€çš„å¯¹æ¯”å­¦ä¹ 

      * è´Ÿæ ·æœ¬æ„å»ºæ–¹æ³•ï¼ˆä¹‹ä¸€ï¼‰ï¼ˆå¯¹æŠ—æ€§è´Ÿæ ·æœ¬ï¼‰
        * åŒæ—¶è¦è®¡ç®—ç”Ÿæˆè´Ÿæ ·æœ¬å’ŒåŸæ¥å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œ**è¿‡æ»¤æ‰ä¸€äº›ç”Ÿæˆçš„å‡è´Ÿæ ·æœ¬**

      ![image-20221201195315054](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221201195315054.png)

      * è¿™äº›è¿‡æ»¤çš„å‡è´Ÿæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ­£æ ·æœ¬ä½¿ç”¨ï¼Œä½œä¸ºæ­£æ ·æœ¬ï¼

    * Cross-Modal Knowledge Distillation (CMKD): å°†åœ¨MSCOCOæ•°æ®é›†ä¸Šè¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€æ¨¡å‹ä½œä¸ºteacher modelï¼Œå°†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ä½œä¸ºstudentï¼Œåœ¨çº¯æ–‡æœ¬è¯­æ–™Wiki103ä¸Šè¿›è¡ŒçŸ¥è¯†è’¸é¦

  * æ¨¡å‹ç¤ºæ„å›¾

  ![image-20221201195203819](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221201195203819.png)

* **ç»“è®º**

  * **ç®€å•åœ¨captionsæ•°æ®é›†**ä¸Šç»§ç»­é¢„è®­ç»ƒå¯ä»¥å–å¾—çŸ¥è¯†è¿ç§»çš„æ•ˆæœ
  * è·¨æ¨¡æ€çš„çŸ¥è¯†è¿ç§»åœ¨**å¾ˆå°çš„è®­ç»ƒæ ·æœ¬**æƒ…å†µä¸‹å¯ä»¥æå¤§æé«˜**ä¸‹æ¸¸ä»»åŠ¡**çš„æ€§èƒ½
  * **å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•**å¯¹è§†è§‰çŸ¥è¯†ï¼ˆå¯¹è±¡å±æ€§ç­‰ï¼Œå¦‚ç¬¬ä¸€å¼ å›¾è¯´çš„ï¼‰çš„å­¦ä¹ æ˜¯æœ€å¥½çš„

:hammer_and_wrench: :fire:**[Relation CLIP] RelCLIP: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning**, in EMNLP 2022. [[pdf](https://aclanthology.org/2022.emnlp-main.317/)] [[torch]()]

* åŠ¨æœº
  * è®©`CLIP`å¯ä»¥è¯†åˆ«å¯¹è±¡ä¹‹é—´çš„relation

* æ–¹æ³•ï¼ˆå¾ˆç®€å•ï¼‰

Commonsense Knowledge å°±æ˜¯**Conceptual Captionæ•°æ®ä¸­æ¥**

![image-20230321093338484](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230321093338484.png)

è´Ÿæ ·æœ¬æ”¶é›†ï¼š

![image-20230321094303920](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230321094303920.png)

:star: **[MM-KG + CLIP] Contrastive Language-Image Pre-Training with Knowledge Graphs**, in NIPS 2022. [[pdf](https://arxiv.org/abs/2210.08901)] [[rebuttal](https://openreview.net/forum?id=4T3kbrzfeR)]

* åŠ¨æœº

  * ä»¥å‰çš„CLIPæ¨¡å‹å¯¹è‡ªç„¶è¯­è¨€å½“ä¸­å¤æ‚çš„è¯­ä¹‰ç‰¹å¾ï¼Œæ¯”å¦‚**é¢œè‰²å’Œä½ç½®**ç­‰
  * å°†çŸ¥è¯†åŠ å…¥åˆ°**CLIPçš„é¢„è®­ç»ƒ**è¿‡ç¨‹å½“ä¸­ï¼Œä»¥**è¾“å…¥çš„å½¢å¼**åŠ è¿›å»çš„

  ![image-20230213181225549](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230213181225549.png)

  

  * **é¢„è®­ç»ƒæ•°æ®é›†æ¥æºå…·ä½“è§è®ºæ–‡**
  * æ–¹æ³•

  ![image-20230213181333946](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230213181333946.png)




:hammer_and_wrench: **PuMer: Pruning and Merging Tokens for Efficient Vision Language Models**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.17530)] [[torch](https://github.com/ csarron/PuMer)]

* åŠ¨æœº
  * Imageæˆ–è€…Textçš„Tokenå¯èƒ½æœ‰çš„æ˜¯å¤šä½™çš„ï¼Œä¼šå¸¦æ¥é¢å¤–çš„è®¡ç®—ä»£ä»·
  * åº”è¯¥æŠŠTokenå‰ªææˆ–è€…åˆå¹¶èµ·æ¥ï¼Œé«˜æ•ˆå¯¹å…¶å¤šæ¨¡æ€ä¿¡æ¯
* æ–¹æ³•ï¼ˆè®­ç»ƒå’Œæ¨ç†æ¡†æ¶ä¸€æ ·ï¼‰

![image-20230602112025512](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230602112025512.png)

:fire: **Learning to Estimate Shapley Values with Vision Transformers**, in ICLR 2023. [[pdf](https://arxiv.org/abs/2206.05282)] [[torch](https://github.com/suinleelab/vit-shapley)]

> å¾ˆæ£’çš„ä¸€ç¯‡Shapley Values ç»“åˆ ViTçš„è®ºæ–‡

* åŠ¨æœº
  * éªŒè¯random masking + æ²¡æœ‰ground truthçš„lossä¹Ÿå¯ä»¥å¾ˆå¥½çš„è®­ç»ƒå‡ºExplainerè¯„ä¼°ViTä¸­çš„Shapley Value
* æ–¹æ³•

![image-20230607002458191](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230607002458191.png)

:fire: **Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models**, in ICCV 2023. [[pdf](https://arxiv.org/pdf/2308.11186)]

* åŠ¨æœº

  * ä¼ ç»Ÿçš„CLIPå¯¹äºä¸€äº›unseen class æ³›åŒ–èƒ½åŠ›ä¸æ˜¯å¾ˆå¥½

  * CLIP + å¤–éƒ¨çŸ¥è¯† ï¼ˆä¸ªäººè§‰å¾— CVPRçš„é‚£å‡ ç¯‡æ›´å¥½ï¼‰

![image-20230902183343344](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902183343344.png)

* æ¨¡å‹æ–¹æ³• ï¼ˆè¿ç»­ + ç¦»æ•£çš„çŸ¥è¯†promptï¼‰

![image-20230902183402422](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230902183402422.png)



:hammer_and_wrench: **[å¤šè¯­è¨€CLIP] mCLIP: Multilingual CLIP via Cross-lingual Transfer**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.728/)] [[torch](https://github.com/ huawei-noah/noah-research/NLP/mclip)]

* åŠ¨æœºï¼šå¤šè¯­è¨€çš„CLIPæ¨¡å‹
* æ–¹æ³•
  1. å…ˆè®­ç»ƒå¥½æ¨¡å‹å¤šè¯­è¨€çš„æ–‡æœ¬ç¼–ç å™¨
  2. å†å°†åŸæ¥çš„CLIPè¿›è¡Œä¸‰è§’è’¸é¦

![image-20230907223536040](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230907223536040.png)

![image-20230907223622103](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230907223622103.png)



:hammer_and_wrench: **Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.376/)] [[torch](https://github.com/thecharm/TMR)]

* åŠ¨æœº
  * å¤šæ¨¡æ€ä¿¡æ¯å¯¹é½é—®é¢˜
  * å¤šæ¨¡æ€ä¿¡æ¯å¯¹é½å’Œè·¨è¯­è¨€æ•£åº¦å¾ˆç±»ä¼¼

![image-20230910094115148](/Users/gary/Library/Application Support/typora-user-images/image-20230910094115148.png)

* æ¨¡å‹æ–¹æ³•ï¼ˆä½¿ç”¨äº†å¤–éƒ¨çš„LAION-400è¿™ç§æ•°æ®æ¥é¢„è®­ç»ƒæ¨¡å‹ï¼‰
  * Back-Translationï¼šDiffusionæ¨¡å‹ç”Ÿæˆå›¾ç‰‡
  * ä½èµ„æºå­¦ä¹  + High-Resource Divergence Estimation

![image-20230910095421482](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910095421482.png)

![image-20230910095446366](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910095446366.png)

* Case Study

![image-20230910095543108](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910095543108.png)



:fire: :hammer_and_wrench: **Preserving Modality Structure Improves Multi-Modal Learning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2308.13077)] [[torch](https://github. com/Swetha5/Multi_Sinkhorn_Knopp)]

* åŠ¨æœº
  * è¿‡å»çš„æ¨¡å‹å¾€å¾€å¿½ç•¥äº†æ¨¡æ€ç‰¹å®šçš„è¡¨å¾
* æ–¹æ³•
  * Multi-Assignment Sinkhorn-Knoppç®—æ³•åˆ†é…å­¦ä¹ Anchor

![image-20230910104252077](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910104252077.png)



 :hammer_and_wrench: **[å¤æ‚çš„å›¾åƒæ£€ç´¢ä»»åŠ¡] A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.02265)] [[torch](https://github.com/YunxinLi/NDCR)]

* åŠ¨æœº
  * åšå¤æ‚çš„è¿™ç§æ£€ç´¢è¦ç±»ä¼¼äºäººè„‘åˆ†æ”¯-ç»„åˆçš„æ€æƒ³

![image-20230918113349556](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918113349556.png)

* æ¨¡å‹æ–¹æ³•ï¼ˆæ„Ÿè§‰é‚£ä¸ª**ç¥ç»ç¬¦å·æ¨ç†**å¾ˆå€¼å¾—å‚è€ƒï¼‰

![image-20230918113445960](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918113445960.png)

* **ç¥ç»ç¬¦å·æ¨ç†æ¨¡å—**

![image-20230918113517987](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918113517987.png)

* å®éªŒç»“æœï¼ˆå®é™…ä¸Šæ‰€æœ‰æ¨¡å‹ç›®å‰ç»“æœéƒ½ä¸å¤ªå¥½ï¼‰ã€**IMAGECODEæ•°æ®é›†ï¼ˆé‡Œé¢å¥½åƒä¹Ÿæœ‰è§†é¢‘çš„ï¼‰**ã€‘

![image-20230918113620317](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918113620317.png)



:hammer_and_wrench: :fire: **Bayesian Prompt Learning for Image-Language Model Generalization**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2210.02390)] [[torch](https://github.com/saic-fi/Bayesian- Prompt-Learning)]

* åŠ¨æœº
  * è¿‡å»çš„prompt tuningæ–¹æ³•å¯¹äºunseençš„domainè¡¨ç°ä¸è¡Œï¼Œæ³›åŒ–èƒ½åŠ›ä¸å¤Ÿã€‚ï¼ˆsuffer from distribution shiftï¼‰
  * Bayesian PTçš„æ–¹æ³•å¯ä»¥prevents learn- ing spurious features, and exploits transferable invariant features
* æ–¹æ³•

![image-20231007094433389](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231007094433389.png)

* æ ¸å¿ƒå…¬å¼ï¼ˆéœ€è¦å…ˆçœ‹**å˜åˆ†æ¨ç†**ç½‘è¯¾ï¼‰

![image-20231013111140013](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231013111140013.png)



:fire: :hammer_and_wrench: **A Unified Objective for Novel Class Discovery**, in ICCV 2021 Oral. [[pdf](https://arxiv.org/abs/2108.08536)] [[torch](https://ncd-uno.github.io/)] [[blog](https://mp.weixin.qq.com/s/sEhXntgjonNJuxMWKQ__mA)]

* åŠ¨æœº
  * è¿‡å»çš„æ–¹æ³•å¾€å¾€éƒ½æ˜¯ç°åœ¨labeledæ•°æ®é›†ä¸Šé¢é¢„è®­ç»ƒå¥½æ¨¡å‹ï¼Œå†è®¾è®¡clustering functionåœ¨unlabeledä¸Šåšä½œ
  * æœ¬æ–‡è®¾è®¡unifiedçš„Objectiveï¼ˆå¾ˆå·§å¦™ï¼Œä¹Ÿå¾ˆç‰›é€¼ï¼‰
* æ–¹æ³•

![image-20231114201847341](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231114201847341.png)

:fire: :hammer_and_wrench: **E2VPT: An Effective and Efficient Approach for Visual Prompt Tuning**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2307.13770)] [[torch](https://github.com/ChengHan111/E2VPT)]

* åŠ¨æœº
  * VPTæ”¹è¿›ç‰ˆæœ¬ï¼šPrompt Tuning + Pruningçš„æ–¹æ³•
* æ¨¡å‹ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20231116102805839](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231116102805839.png)

:fire: **SINC: Self-Supervised In-Context Learning for Vision-Language Tasks**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2307.07742)]

* åŠ¨æœº
  * æ¢ç´¢VLMç›´æ¥åœ¨é¢„è®­ç»ƒæ—¶å€™çš„in-context-learningèƒ½åŠ›
  * è¿‡å»ICLéƒ½æ˜¯ä¾èµ–äºè¯­è¨€ç©ºé—´ï¼Œä¼šå‡ºç°æ¨¡ç‰ˆæ•æ„Ÿ + å¹»è§‰çš„é—®é¢˜
  * è€Œä¸”LLMçš„è®­ç»ƒä»£ä»·å¾ˆå¤§

![image-20231122153701843](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231122153701843.png)

* æ¨¡å‹æ–¹æ³•

![image-20231122153917503](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231122153917503.png)

* Promptçš„è®¾è®¡

![image-20231122154115854](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231122154115854.png)

:fire: :hammer_and_wrench: **Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation**, in NeurIPS 2023. [[pdf](https://arxiv.org/abs/2309.12742)] [[torch](https://github.com/yue-zhongqi/ICON)]

* åŠ¨æœº
  * UDAå¾€å¾€ä¼šæœ‰domain-specificå’Œdomain-invariantä¹‹é—´çš„spurious correlationçš„é—®é¢˜
  * è¿‡å»æ–¹æ³•å°†target domainçœ‹ä½œæ˜¯ä¸€ä¸ªè¾…åŠ©çš„æ•°æ®ï¼Œæ²¡æœ‰å¾ˆå¥½çš„åˆ©ç”¨èµ·æ¥
* æ–¹æ³•ï¼ˆå’Œä»–ä»¬CVPRé‚£ç¯‡è§†é¢‘å¼‚å¸¸æ£€æµ‹çš„å¾ˆåƒï¼‰

![image-20231123152426035](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231123152426035.png)

* è®­ç»ƒç›®æ ‡

![image-20231123152448484](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231123152448484.png)



### :running: Continual Learning

:hamburger: :hammer_and_wrench: :fire: **Learning to Prompt for Continual Learning**, in CVPR 2022. [[pdf](https://arxiv.org/abs/2112.08654)] [[code](https://github.com/google-research/l2p)]

> å®˜æ–¹ä»£ç æ˜¯`tensorflow`çš„ï¼Œgithubä¸Šæœ‰`pytorch`çš„å®ç°

* åŠ¨æœº
  * è¿‡å»æ•°æ®é‡æ”¾çš„æ–¹æ³•æœ‰æ•°æ®éšç§è¿˜æœ‰å¤§bufferçš„é—®é¢˜
  * å¸Œæœ›ä¸éœ€è¦é‚£ä¹ˆå¤§çš„bufferæ¥å­˜è¿‡å»çš„æ•°æ®ï¼Œè€ƒè™‘ä½¿ç”¨Prompt Tuningçš„æ–¹æ³•æ¥åš

![image-20230910091659822](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910091659822.png)

* æ–¹æ³•
  * å°±æ˜¯ç”¨é¢„æŠ½å–çš„è¡¨å¾é€‰æ‹©å¯¹åº”çš„`Prompt`æ¥Tuning

![image-20230910091831179](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910091831179.png)

:hammer_and_wrench: **S-Prompts Learning with Pre-trained Transformers: An Occamâ€™s Razor for Domain Incremental Learning**, in NeurIPS 2022. [[pdf](https://arxiv.org/abs/2207.12819)] [[torch](https://github.com/iamwangyabin/S-Prompts)]

* åŠ¨æœºï¼ˆ`S-Prompt`å’Œ`DualPrompt`æ˜¯åŒæ—¶æœŸçš„ä¸œä¸œï¼Œä»–ä»¬ä¸¤ä¸ªå¹¶æ²¡æœ‰ç›´æ¥æ ‡ä»·å“¦ï¼ŒS-Promptä¸»è¦æ¯”è¾ƒçš„æ˜¯L2Pï¼‰
  * ç›®æ ‡æ˜¯åšDILï¼Œè®©æ¯ä¸ªdomainçš„promptéƒ½å­˜åœ¨ä¸€ä¸ªå­ç©ºé—´ä¸­ï¼Œå½¼æ­¤ä¸äº’ç›¸å½±å“

![image-20230924102201828](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230924102201828.png)

* æ–¹æ³•

![image-20230924102352469](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230924102352469.png)



:fire: :hammer_and_wrench: **DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning**, in ECCV 2022. [[pdf](https://arxiv.org/abs/2204.04799)] [[code](https://github.com/google-research/l2p)]

> `L2P`çš„è¿›é˜¶å·¥ä½œï¼ŒåŒç»„çš„å·¥ä½œ

* åŠ¨æœº
  * L2Påªè€ƒäº†ä¸€ç§Promptï¼Œæ²¡æœ‰è€ƒè™‘ä»»åŠ¡ç‰¹å®šçš„å’Œä»»åŠ¡é—´ç›¸ä¼¼çš„ç‰¹å¾
  * äººç±»åœ¨è¿ç»­å­¦ä¹ æ—¶å€™å¾€å¾€ä¼šè€ƒè™‘è¿™ä¸¤æ–¹é¢çš„èƒ½åŠ›
    * Complementary Learning Systems (CLS) [[å¼•æ–‡1](https://pubmed.ncbi.nlm.nih.gov/7624455/),[å¼•æ–‡2](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(16)30043-2)] (CLS) suggests that humans learn continually via the synergy between two learning systems: the hippocampus focuses on learning pattern-separated representation on specific experiences, and the neocortex focuses on learning more general and transferable representation from past experience sequences.
* æ¨¡å‹æ–¹æ³•
  * å­¦ä¹ ä¸¤ç§Prompt

![image-20230910092436783](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910092436783.png)



:fire: :hammer_and_wrench: **CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning**, in CVPR 2023. [[pdf](https://arxiv.org/abs/2211.13218)]] [[torch](https://github.com/GT-RIPL/CODA-Prompt)]

* åŠ¨æœº
  * è¿‡å»çš„PT-CLæ–¹æ³•ç‰ºç‰²äº†æ–°çš„ä»»åŠ¡æ€§èƒ½ï¼Œè€Œä¸”ï¼Œå…¶å®å¹¶ä¸æ˜¯æ‰€æœ‰çš„ä»»åŠ¡éƒ½éœ€è¦åŒæ ·é‡è¦çš„promptï¼Œåº”è¯¥promptä¹‹å‰å…ˆåšä¸€ä¸‹attentionï¼ˆæœ¬æ–‡ä¸­attentionä¹Ÿæ˜¯å¯å­¦ä¹ çš„ï¼‰

![image-20231018105108528](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018105108528.png)

* æ–¹æ³•

![image-20231018105148711](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018105148711.png)

![image-20231018105215146](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018105215146.png)



:hammer_and_wrench: :fire: **[2023-10 æœªå¼€æº] ConStruct-VL: Data-Free Continual Structured VL Concepts Learning**,in CVPR 2023. [[pdf](https://arxiv.org/abs/2211.09790)] [[torch](https://github.com/jamessealesmith/ConStruct-VL)]

* åŠ¨æœº
  * è¿‡å»çš„CLæ–¹æ³•å­¦æ¦‚å¿µè¿˜ä¸æ˜¯å¾ˆè¡Œï¼ˆConceptï¼‰ï¼Œæ‰€ä»¥æœ¬æ–‡æå‡ºConcept Learningçš„CL Benchmark

![image-20231018105622946](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018105622946.png)

* æ¨¡å‹æ–¹æ³• ï¼ˆLoRAé«˜æ•ˆå¾®è°ƒ + ç”Ÿæˆæ•°æ®é‡æ”¾ä¼ªæ ‡ç­¾ï¼‰

![image-20231018105644301](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018105644301.png)



:fire: **Introducing Language Guidance in Prompt-based Continual Learning**, in ICCV 2023. [[pdf](https://arxiv.org/pdf/2308.15827.pdf)]

> å¯ä»¥å’Œ`L2P`ä»¥åŠ`DualPrompt`æ— ç¼è¡”æ¥èµ·æ¥

* åŠ¨æœº
  * ä½¿ç”¨è‡ªç„¶è¯­è¨€æ¥å¼•å¯¼Promptçš„é€‰æ‹©
  * æ‰€æœ‰ä»»åŠ¡çš„æ ·æœ¬è¡¨å¾éƒ½å¯ä»¥æ˜ å°„åˆ°ç›¸åŒçš„è¯­ä¹‰ç©ºé—´å½“ä¸­
* æ–¹æ³•

![image-20230910093733775](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230910093733775.png)

:fire: :hammer_and_wrench: **When Prompt-based Incremental Learning Does Not Meet Strong Pretraining**, in ICCV 2023. [[pdf](https://arxiv.org/pdf/2308.10445)] [[torch](https://github.com/TOM-tym/APG)]

* åŠ¨æœº
  * è¿‡å»çš„Prompt-baseï¼ˆL2Pï¼ŒDualPromptï¼‰æ–¹æ³•æ— æ³•å¾ˆå¥½å¼¥è¡¥é¢„è®­ç»ƒå’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„gap

![image-20230919164942525](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230919164942525.png)

* æ¨¡å‹æ–¹æ³•ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20230919165013021](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230919165013021.png)



:fire: :hammer_and_wrench: **Continual Pre-training of Language Models**, in ICLR 2023. [[pdf](https://pretrainedarxiv.org/abs/2302.03241)] [[torch](https://github.com/UIC-Liu-Lab/ContinualLM)]

* åŠ¨æœº
  * PLMåˆ°æ–°çš„domainä¹‹åï¼Œè¦ä¿è¯çŸ¥è¯†è¿ç§» + æ— ç¾éš¾æ€§é—å¿˜
  * è¿‡å»çš„æ–¹æ³•åœ¨PLM domainè¿ç§»æ—¶å€™ä¸æ˜¯å¾ˆé€‚ç”¨
* æ–¹æ³•ï¼ˆè®¡ç®—unitçš„importanceï¼‰

![image-20230930092000716](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230930092000716.png)



:fire: :hammer_and_wrench: :hushed: **SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2303.05118)] [[torch](https://github.com/GengDavid/SLCA)]

* åŠ¨æœº
  * CLçš„ä¸€å¤§æ ¸å¿ƒæ˜¯`lr`

![image-20230930095356560](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230930095356560.png)

* æ–¹æ³•
  * lrå°ä¸€ç‚¹
  * æ¯ä¸ªclassieréƒ½è¿›è¡Œalignmentå¤„ç†ï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰

![image-20230930095448740](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230930095448740.png)

:fire: :hammer_and_wrench: **Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2303.06628)] [[torch](https://github.com/Thunderbeee/ZSCL)] [[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/643766605)]

* åŠ¨æœº
  * è¿‡å»CLæ–¹æ³•ä¸å•å•ä¼šå¿˜è®°ä¸Šä¸€ä¸ªä»»åŠ¡ï¼Œè¿˜ä¼šå¿˜æ‰é¢„è®­ç»ƒçš„ä¸œè¥¿ï¼Œå¯¼è‡´å¾ˆå·®çš„zero-shotè¿ç§»èƒ½åŠ›

![image-20231018104708625](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018104708625.png)

* æ–¹æ³•

![image-20231018104827078](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018104827078.png)

![](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231018104827078.png)

:fire: :hammer_and_wrench: **CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation**, in ICCV 2023. [[pdf](https://arxiv.org/abs/2308.07146)] [[torch](https://github.com/KevinLight831/CTP)]

* åŠ¨æœº
  * å¤šæ¨¡æ€é¢„è®­ç»ƒå¾ˆå°‘è¢«æ¢ç´¢
  * å’Œä¼ ç»Ÿçš„CILä¸å¤ªä¸€æ ·
    * VLPå¯ä»¥åˆ©ç”¨å˜ˆæ‚çš„ç½‘ç»œæ•°æ®ï¼Œå¹¶ä¸”ä¸éœ€è¦æ‰‹åŠ¨æ ‡æ³¨å›ºå®šçš„ç±»åˆ«
    * æ–°çš„æŒ‘æˆ˜ï¼šæ–°æ—§çŸ¥è¯†å¦‚ä½•ä¿å­˜ï¼Œæ›´æ–°å¤šæ¨¡æ€çš„encoder
  * æå‡ºæ–°çš„å¤šæ¨¡æ€VLCPçš„Benchmark
* æ–¹æ³•

![image-20231110200626396](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231110200626396.png)

:hammer_and_wrench: :fire: **Generating Instance-level Prompts for Rehearsal-free Continual Learning**, in ICCV 2023. [[pdf](https://openaccess.thecvf.com/content/ICCV2023/papers/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.pdf)]] [[torch](https://github.com/naver-ai/dap-cl)]

* åŠ¨æœº
  * promptä¸å†æ˜¯ä»poolé‡Œé¢å»é€‰æ‹©ï¼Œè€Œæ˜¯ç›´æ¥å»ç”Ÿæˆ
  * è¿™æ ·åœ¨ä¸€äº›ä¸ä¾èµ–ViTé¢„è®­ç»ƒå‚æ•°çš„ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ï¼Œä¸€äº›æ¯”è¾ƒç‹¬ç‰¹çš„domainä¸Šçš„è¡¨ç°

* æ–¹æ³•

![image-20231113212002719](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231113212002719.png)



:fire: :hammer_and_wrench: **Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality**, in NeurIPS 2023 Spotlight. [[pdf](https://arxiv.org/pdf/2310.07234.pdf)] [[torch](https://github.com/thu-ml/HiDe-Prompt)]

* åŠ¨æœº
  * ä½œè€…å‘ç°åœ¨è‡ªï¼ˆæ— ï¼‰ç›‘ç£çš„ç½‘ç»œå½“ä¸­ï¼ŒMoCoç­‰ï¼Œä¼ ç»Ÿçš„promptçš„æ–¹æ³•åšCLæ•ˆæœä¸å¥½
  * æå‡ºå…¨æ–°çš„Unsupervised Learning + CLçš„èŒƒå¼
* æ–¹æ³•ï¼ˆwithin-task prediction, task-identity inference, and task-adaptive predictionï¼‰

![image-20231113212255755](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231113212255755.png)

:hammer_and_wrench: **Towards Continual Adaptation in Industrial Anomaly Detection**, in MM 2022. [[pdf](https://dl.acm.org/doi/abs/10.1145/3503161.3548232)] [[torch](https://github.com/vijaylee/Continual_Anomaly_Detection)]

* åŠ¨æœº
  * ç¬¬ä¸€ä¸ªå¤šå·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆ**äºŒåˆ†ç±»**ï¼‰ + CLçš„
  * ç”Ÿæˆbridgeäº†Unsupervised Learning + CL
    * å®é™…ä¸Šæ˜¯ç”Ÿæˆäº†è´Ÿæ ·æœ¬ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„
* æ–¹æ³•ï¼ˆå­˜å‚¨æ¯ä¸ªdomainçš„åˆ†å¸ƒä¿¡æ¯ï¼‰

![image-20231113212623593](/Users/gary/Library/Application Support/typora-user-images/image-20231113212623593.png)



:fire: :hammer_and_wrench: **Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection**, in ICCV 2023.  [[pdf]](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Augmented_Box_Replay_Overcoming_Foreground_Shift_for_Incremental_Object_Detection_ICCV_2023_paper.pdf)] [[torch](https://github.com/YuyangSunshine/ABR IOD.git)]

* åŠ¨æœº
  * Replayçš„æ–¹æ³•åœ¨ODCLä¸­ä¸è§å¾—æœ‰æ•ˆ
  * åŸå› æ˜¯Foreground-shift

![image-20231115205416533](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231115205416533.png)

* æ–¹æ³•ï¼šReplay+å¢å¹¿

![image-20231115205616322](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231115205616322.png)

---



### :world_map: Knowledge

:fire: **[å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ç»¼è¿°] Multi-Modal Knowledge Graph Construction and Application: A Survey**, in 2022. [[pdf](https://arxiv.org/pdf/2202.05786.pdf)] [[zhihu](https://zhuanlan.zhihu.com/p/484096631)]



:hammer_and_wrench: **End-to-end Knowledge Retrieval with Multi-modal Queries**, in ACL 2023. [[pdf](https://arxiv.org/pdf/2306.00424.pdf)] [[dataset&torch](https://github.com/luomancs/ReMuQ)]

* åŠ¨æœº
  * ç«¯åˆ°ç«¯çš„åŸºäºå¤šæ¨¡æ€çš„çŸ¥è¯†æ£€ç´¢è¿‡å»çš„å·¥ä½œæ¢ç´¢ä¸è¶³
  * è¿‡å»å¾€å¾€å°±æ˜¯å…ˆè½¬æˆCaptionå†æ£€ç´¢æ–‡æœ¬ç«¯çŸ¥è¯†ï¼Œæˆ–è€…ç›´æ¥ç”¨å›¾ç‰‡æ£€ç´¢çŸ¥è¯†

![image-20230604092623360](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230604092623360.png)

* æ•°æ®é›†
  * åŸºäºWebQAæå‡ºæ–°çš„çŸ¥è¯†æ£€ç´¢æ•°æ®é›†

![image-20230604092703138](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230604092703138.png)

* æ–¹æ³•ï¼ˆå¯¹æ¯”å­¦ä¹ Lossï¼‰

![image-20230604092722382](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230604092722382.png)



:hammer_and_wrench: :fire: **Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge**, in ACL Findings 2023. [[pdf](https://arxiv.org/abs/2306.04657)] [[torch](https://github.com/Hanscal/DCKS)]

* åŠ¨æœº
  * è¿‡å»ä¹Ÿå°†å¸¸è¯†çŸ¥è¯†å¼•å…¥è¿›åŒç†å¿ƒå¯¹è¯é‡Œé¢ï¼Œä½†æ˜¯çŸ¥è¯†å¯èƒ½ä¼šå†—ä½™ï¼Œéœ€è¦å»é€‰æ‹©çŸ¥è¯†ã€‚

![image-20230612101031434](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230612101031434.png)

* æ¨¡å‹æ–¹æ³•

![image-20230612101049910](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230612101049910.png)

* çŸ¥è¯†é€‰æ‹©ç­–ç•¥ï¼ˆè¿­ä»£æ¥æ¯æ¬¡æ’é™¤æ‰æœ€ä¸ç›¸å…³çš„çŸ¥è¯†ï¼‰

![image-20230612101155806](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230612101155806.png)



:hammer_and_wrench: **HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level**, in ACL 2023. [[pdf](https://arxiv.org/abs/2305.06588)] [[torch](https://github.com/LHRLAB/HAHE)]

> **è¶…å…³ç³»çŸ¥è¯†å›¾è°±**çš„é¢„è®­ç»ƒè¡¨ç¤ºå­¦ä¹ 

* ä»€ä¹ˆæ˜¯è¶…å…³ç³»çŸ¥è¯†å›¾è°±

![image-20230918095649749](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918095649749.png)

* è¡¨ç¤ºå­¦ä¹ ï¼ˆä¸€æ ·çš„ï¼Œè¿˜æ˜¯ä¸¤ä¸ªå¸¸è§„ä»»åŠ¡ï¼‰
  * å®ä½“æŠ½å–
  * å…³ç³»é¢„æµ‹
* è¶…å›¾è¡¨ç¤º

![image-20230918095825890](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918095825890.png)

* æ¨¡å‹æ–¹æ³•

![image-20230918095847947](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918095847947.png)

![image-20230918095920522](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230918095920522.png)



:hammer_and_wrench: **KILM: Knowledge Injection into Encoder-Decoder Language Models**, in ACL 2023. [[pdf](https://aclanthology.org/2023.acl-long.275.pdf)] [[torch](https://github.com/alexa/kilm)]

* åŠ¨æœº
  * çŸ¥è¯†æ³¨å…¥PLMå½“ä¸­ï¼ˆè¿™ä¸ªideaå¾ˆä¸€èˆ¬å§ï¼‰
* æ¨¡å‹æ–¹æ³•

![image-20230920094808476](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20230920094808476.png)



### :ice_cream: Hallucination

:fire: :hammer_and_wrench: **Evaluating Object Hallucination in Large Vision-Language Models**, in EMNLP 2023. [[pdf](https://arxiv.org/abs/2305.10355)]] [[torch](https://github.com/RUCAIBox/POPE)] [[blog](https://mp.weixin.qq.com/s/bcIBN6raLcilgRcj-3ltTg)].

> å¯¹è±¡å¹»è§‰çš„è¯„ä¼°æµ‹è¯„æŒ‡æ ‡

* åŠ¨æœº
  * è¿‡å»ç”¨Chairæ¥è¯„æµ‹ï¼Œç°åœ¨æå‡ºäº†ä¸€ç§åŸºäº**è½®è¯¢çš„ç‰©ä½“æ¢æµ‹è¯„æµ‹**æ–¹æ³• (Polling-based Object Probing Evaluation, POPE)
  * å®éªŒç»“æœè¡¨æ˜ POPE å…·æœ‰æ›´å¥½çš„ç¨³å®šæ€§ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ‰©å±•åˆ°æœªæ ‡æ³¨æ•°æ®é›†ä¸Š
* æ–¹æ³•

![image-20231029180106082](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231029180106082.png)

* ç»“æœ
  * InstructBLIPçš„æ•ˆæœæ¯”è¾ƒå¥½

![image-20231029180139292](/Users/gary/Library/Application Support/typora-user-images/image-20231029180139292.png)

* å¹»è§‰ä¸VQAæ€§èƒ½çš„å…³ç³»

ä¸ºäº†è¿›ä¸€æ­¥ç ”ç©¶å¹»è§‰é—®é¢˜å’Œ LVLMs æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åœ¨ A-OKVQA å’Œ GQA ä¸Šæµ‹è¯•äº†éƒ¨åˆ† LVLMs çš„æ€§èƒ½ã€‚ç”±äº LVLMs çš„å›ç­”å¤§éƒ½å±äºè¾ƒé•¿çš„å¼€æ”¾å¼å›ç­”ï¼Œæˆ‘ä»¬éš¾ä»¥ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿçš„ VQA è¯„æµ‹æ–¹æ³•ã€‚ä¸ºæ­¤æˆ‘ä»¬å€ŸåŠ©äº† ChatGPT æ¥è¾…åŠ©æµ‹è¯„ï¼Œæˆ‘ä»¬æä¾›ç»™ ChatGPT çš„ prompt å¦‚ä¸‹:

`You are an examiner who can judge whether a studentâ€™s answer matches the correct answers. Next, I will provide you with the correct answer and a studentâ€™s answer. Please judge whether the studentâ€™s answer matches the correct answers.`

æµ‹è¯„ç»“æœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚InstructBLIP åœ¨å¹»è§‰å’Œ VQA ä»»åŠ¡ä¸­éƒ½å–å¾—äº†æœ€å¥½çš„è¡¨ç°ï¼Œè€Œ MiniGPT-4 å’Œ LLaVA åœ¨äºŒè€…ä¸Šçš„ç»“æœè¶‹åŠ¿å¹¶ä¸ä¸€è‡´ã€‚

è®¤ä¸ºè¿™äº›å·®å¼‚å’Œ LVLMs çš„ä½¿ç”¨çš„æŒ‡ä»¤å½¢å¼æœ‰å…³ï¼Œä¾‹å¦‚ MiniGPT-4 çš„æŒ‡ä»¤æ˜¯**è¾ƒç®€å•çš„å›¾åƒæè¿°ä»»åŠ¡**ï¼Œè€Œ LLaVA çš„æŒ‡ä»¤åŒ…å«**æ›´å¤šè¾ƒå¤æ‚çš„å¯¹è¯æˆ–æ¨ç†ä»»åŠ¡**ï¼Œä½¿å…¶æ›´æ“…é•¿å¤„ç† VQA ä»»åŠ¡ã€‚æ€»ä¹‹ï¼Œä¸Šè¿°ç»“æœè¯´æ˜åœ¨è¯„ä¼°ç°æœ‰ LVLMs çš„æ€§èƒ½æ—¶ï¼Œå¹»è§‰å’Œ VQA æ€§èƒ½éƒ½éœ€è¦è¢«è€ƒè™‘ã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/mmbiz_png/G7ia3FZ0o0Oq5zb0h73dZrPafghJu43fL9C0LL6e6ZGJR53O0Y9ycJbI5xXoErMACWGOBOeDCdDgzZxdicyLibQNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

:hammer_and_wrench: :fire: **Can We Edit Multimodal Large Language Models?**, in EMNLP 2023.  [[pdf](https://arxiv.org/abs/2310.08475)] [[torch](https://github.com/zjunlp/EasyEdit)]

* åŠ¨æœº
  * æå‡ºå¤šæ¨¡æ€çŸ¥è¯†ç¼–è¾‘çš„Benchmarkï¼ˆå…·ä½“è§è®ºæ–‡ï¼‰
  * å‘ç°æ¨¡å‹ç¼–è¾‘çš„æ–¹æ³•è¿˜å¯ä»¥ï¼Œä½†è¿˜æœ‰å¾ˆå¤§è¿›æ­¥ç©ºé—´
* æ–¹æ³•ï¼ˆæ¨¡å‹ç¼–è¾‘ï¼‰

![image-20231113213749107](/Users/gary/Library/Application Support/typora-user-images/image-20231113213749107.png)



## :man_scientist: AI4Science

### :factory: Industrial

:fire: :hammer_and_wrench: **SimpleNet: A Simple Network for Image Anomaly Detection and Localization**, in CVPR 2023. [[pdf](https://arxiv.org/abs/2303.15140)] [[torch](https://github.com/DonaldRR/SimpleNet)]

* åŠ¨æœº
  * å™ªéŸ³åœ¨ç‰¹å¾ä¸Šé¢æ¥åŠ ï¼Œç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œè€Œä¸æ˜¯åœ¨å›¾åƒä¸Šé¢åŠ 
* æ–¹æ³•ï¼ˆå¾ˆSimpleï¼‰

![image-20231113213001981](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20231113213001981.png)

:fire: **Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore**, in ICLR 2023. [[pdf](https://arxiv.org/abs/2301.12082)]

* åŠ¨æœº
  * å¼€å‘äº†ä¸€ç§æ–°çš„FSADæ–¹æ³•ï¼Œç§°ä¸ºGraphCoreï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å°‘é‡æ­£å¸¸æ ·æœ¬æ¥å®ç°æ–°äº§å“çš„å¿«é€Ÿè®­ç»ƒå’Œæœ‰ç«äº‰åŠ›çš„ADç²¾åº¦æ€§èƒ½ã€‚ä¸€æ–¹é¢ï¼Œé€šè¿‡åˆ©ç”¨å°‘é‡æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿè®­ç»ƒå¹¶åŠ å¿«å¼‚å¸¸æ¨ç†çš„é€Ÿåº¦ã€‚å¦ä¸€æ–¹é¢ï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥è®­ç»ƒæ–°äº§å“æ ·æœ¬ï¼Œæ‰€ä»¥ä¸ä¼šå‘ç”Ÿä»æ—§äº§å“åˆ°æ–°äº§å“çš„å¼‚å¸¸é€‚åº”å’Œè¿ç§»ã€‚
    - æå‡ºäº†ä¸€ç§ç”¨äº FSAD çš„ç‰¹å¾å¢å¼ºæ–¹æ³•ï¼Œä»¥ç ”ç©¶ CNN ç”Ÿæˆçš„è§†è§‰ç‰¹å¾çš„ç‰¹æ€§ã€‚
    - æå‡ºäº†ä¸€ç§æ–°é¢–çš„**å¼‚å¸¸æ£€æµ‹æ¨¡å‹** **GraphCore**ï¼Œå°†æ–°çš„ VIIF æ·»åŠ åˆ°åŸºäºå†…å­˜åº“çš„ AD èŒƒä¾‹ä¸­ï¼Œè¿™å¯ä»¥å¤§å¤§å‡å°‘å†—ä½™è§†è§‰ç‰¹å¾çš„æ•°é‡ã€‚
    - å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ VIIF æ˜¯æœ‰æ•ˆçš„ï¼Œå¯ä»¥æ˜¾ç€æé«˜ FSAD åœ¨ MVTec AD å’Œ MPDD æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚
* æ–¹æ³•ï¼ˆè¯¦è§è®ºæ–‡ï¼‰

![å›¾ç‰‡](https://mmbiz.qpic.cn/mmbiz_png/ibPsADKUH1kuEfwNwXic6fTPEmu0gz7UrZZHoIIweowKxTiat19rE0wibcLJMdicxbmLKB55v1g6FBOwxLBElBAZGdw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

:fire: :hammer_and_wrench: **WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation**, in CVPR 2023. [[pdf](https://arxiv.org/abs/2303.14814)] [[torch](https://github.com/caoyunkang/WinClip)]

* åŠ¨æœº
  * ç›´æ¥ç”¨CLIPæ¨¡å‹Zero-shotæ¥åšå¼‚å¸¸æ£€æµ‹äºŒåˆ†ç±»é—®é¢˜
* æ–¹æ³•ï¼ˆPromptçš„è®¾è®¡æ‰æ˜¯æœ€é‡è¦çš„ï¼‰
  * ä½œè€…è®¾è®¡äº†ä¸€ä¸ªPromptçš„æ¨¡ç‰ˆï¼ˆè¯¦è§è®ºæ–‡é™„å½•éƒ¨åˆ†ï¼‰

![image-20231113213509342](/Users/gary/Library/Application Support/typora-user-images/image-20231113213509342.png)
