# Knowledge-enriched Text Generation paper reading

ğŸ˜ Awesome list of papers about knowledge-enhanced Question generation with notes.

:white_check_mark: : **already reading carefully**

:fire:: **high citation in recent years**

:hammer_and_wrench:: **available code**

> Content

[TOC]



---

## :grey_question: Question Generation

---

### :mountain_snow: **Textual Question Generating Crosstalk**

**ä¸€ã€åˆ©ç”¨ç­”æ¡ˆå’Œè¯­è¨€ç‰¹å¾**

1. **ä¸¤ç¯‡Ground Breaking Work**

:white_check_mark: :fire: **Neural question generation from text: A preliminary study**, in EMNLP 2017. [[pdf](https://arxiv.org/abs/1704.01792)] 

* åœ¨ç¼–ç æ—¶é¢å¤–è€ƒè™‘äº†ç­”æ¡ˆä½ç½®ä¸è¯­æ³•ä¿¡æ¯ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚(ç°åœ¨æ¥çœ‹éå¸¸**basic**é‡è¦çš„ä¿¡æ¯ï¼)
  * word case åšè®­ç»ƒæ—¶å€™çš„teacher forcing
  * answer position feature
  * lexical features
    * **POS**
    * **NER**

```mermaid
graph LR
en((encoder)) --bi-GRU--> fe((feature-Rich)) --> word-vecotr
fe --> lexcial-feature-embedding-vectors --> POS+NER
fe --> answer-position-embedding --> BIO-tagging

word-vecotr --> åŒå‘çš„éšè—å±‚
POS+NER --> åŒå‘çš„éšè—å±‚
BIO-tagging --> åŒå‘çš„éšè—å±‚

de((decoder)) --å¸¦æ³¨æ„åŠ›æœºåˆ¶,ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›--> maxout-hidden+å…·ä½“éœ€è¦çœ‹referenceè®ºæ–‡
de --> GRU

de --> Copy-Mechanism,ä¸€æ ·ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ› --> è®¡ç®—å‡ºæ¦‚ç‡ä»sourceå¥å­ä¸­ç›´æ¥copyå•è¯
```



:white_check_mark: :fire: :hammer_and_wrench: **Learning to Ask: Neural Question Generation for Reading Comprehension**, in ACL 2017. [[pdf]](https://arxiv.org/abs/1705.00106) [[official code (torch)](https://github.com/xinyadu/nqg)]
* å°†ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œåº”ç”¨äºé—®é¢˜ç”Ÿæˆ
* é‡‡ç”¨seq2seq+attentionæ¨¡å‹æ¶æ„
* æ‘†è„±äº†è½¬æ¢è§„åˆ™ä¸æ¨¡ç‰ˆçš„å±€é™ï¼Œå–å¾—äº†ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„æ€§èƒ½
* åŠ å…¥äº†paragraph-level


```mermaid
graph LR
ä»»åŠ¡éš¾ç‚¹ --æ›´åŠ æ¥è¿‘äºäººç±»--> åŒä¹‰è¯æ›¿æ¢+çŸ¥è¯†å¼•å…¥ --> ç›¸å…³å·¥ä½œ --> è¿‡å»:rule-based 
ç›¸å…³å·¥ä½œ --> å…¶ä»–æ•°æ®æ˜ å°„è‡ªç„¶è¯­è¨€

Seq2Seq --> en((encoder)) --bidirectional--> softè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° --> lstm((LSTM))  --> only-sentence
lstm --> sentence+paragraph --> truncateæˆªæ–­,å½“ç„¶æ›´å¥½çš„æ–¹æ³•æ˜¯åˆ‡ç‰‡

Seq2Seq --> de((decoder)) --word-level-prediction--> LSTM((LSTM)) --> éšè—å±‚åˆå§‹åŒ– --basic-model --> å¥å­encoderçš„æœ€åéšè—å±‚
LSTM --oours--> å¥å­+æ®µè½çš„encoderè¾“å‡º


```

2. **ç­”æ¡ˆç¼–ç **

:white_check_mark: :fire: **Improving Neural Question Generation using Answer Separation**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1809.02393)] 
* å¾ˆå¤šåŸºç¡€æ“ä½œ
* åœ¨ç­”æ¡ˆä¸Šåšäº†ç®€å•é«˜æ•ˆçš„é¢„å¤„ç†
  * Mask åŸæ–‡ä¸­çš„ç­”æ¡ˆ
  * å¯¹ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯åšæŠ½å–ï¼Œè®¡ç®—attention

3. **è¯­è¨€ç‰¹å¾å¼ºåŒ–**

> ä¼ ç»Ÿçš„æœ‰**POS**ï¼ˆè¯æ€§æ ‡æ³¨ï¼‰å’Œ**NER**ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚åç»­è¿˜æœ‰ä¸€äº›æ›´åŠ ç»†å¾®çš„å¤„ç†

:fire: **Learning to Generate Questions by Learning What not to Generate**, in WWW 2019.  [[pdf](https://arxiv.org/pdf/1902.10418.pdf)] 

* clue å’Œ copyçš„æœºåˆ¶
* ![image-20220703112845472](https://s2.loli.net/2022/07/03/hoa1kTVzIpXGeQK.png)
* ![image-20220703113749291](https://s2.loli.net/2022/07/03/bdToKkIADvemV6B.png)
* æ–‡ç« è´¡çŒ®
  * å¸®åŠ©æ¨¡å‹å†³ç­–ä»€ä¹ˆæ—¶å€™ç”Ÿæˆï¼Œä»€ä¹ˆæ—¶å€™copy
  * ç”Ÿæˆå¤šä¸ªé—®é¢˜

4. ç–‘é—®è¯ç±»å‹ï¼ˆquestion typeï¼‰

**Question-type Driven Question Generation**, in EMNLP 2019.  [[pdf](https://arxiv.org/pdf/1909.00140.pdf)]

* å¼•å…¥å¯¹ç–‘é—®è¯çš„é¢„æµ‹æ¨¡å—ï¼Œå¹¶ä¸”åŠ å…¥å¯¹åº”çš„æŸå¤±å‡½æ•°
* ![image-20220703121312077](https://s2.loli.net/2022/07/03/uhcs1erWpQ9UFKL.png)
* ![image-20220703121503229](https://s2.loli.net/2022/07/03/GTO7j5c1AkXNnqY.png)
* ![image-20220703121520488](https://s2.loli.net/2022/07/03/seIfghRSbvpO68a.png)
* [æŸå¤±å‡½æ•°å¼•æ–‡](https://aclanthology.org/P17-1099.pdf)ï¼š <img src="https://s2.loli.net/2022/07/03/UZv17XkMyLRPOgd.png" alt="image-20220703122045296" style="zoom: 50%;" />

**äºŒã€æ®µè½çº§åˆ«ç‰¹å¾**

1. **å¼ºåŒ–æ®µè½çº§åˆ«æ–‡æœ¬çš„ç‰¹å¾**

:fire: :hammer_and_wrench: **Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks**, in EMNLP 2018. [[pdf](https://github.com/seanie12/neural-question-generation)] [[torch](https://github.com/seanie12/neural-question-generation)]

* ä¸»è¦è´¡çŒ®éƒ½åœ¨æ¨¡å‹ä¸Šé¢ï¼ŒåŸºäºseq2seqè®¾è®¡ï¼š![image-20220704150006013](https://s2.loli.net/2022/07/04/trhe5uTRkUqyfsx.png)

  * gate self-attention: ä¸ªäººè§‰å¾—æ˜¯ä¸€å¥—å¾ˆå¸¸ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸‹ï¼Œä¹Ÿéå¸¸ç®€å•

  * Maxout ==**Pointer**== & Decoding **å…¨æ–°çš„å¤„ç† copy æœºåˆ¶**  (æœ‰ç©ºå¯ä»¥è‡ªè¡Œå»çœ‹çœ‹ä»£ç ï¼)

    * ä¹‹å‰copyå¾—åˆ†ï¼š$\operatorname{sc}^{\text {copy }}\left(y_{t}\right)=\left\{\begin{array}{l}\sum_{k, \text { where } x_{k}=y_{t}} r_{t, k}, \quad y_{t} \in \chi \\ -i n f, \text{otherwise}\end{array} \quad\right.$ , é—®é¢˜åœ¨äºè‹¥æ–‡ç« ä¸­æŸä¸ªå•è¯é‡å¤å‡ºç°å¤šæ¬¡ï¼Œåˆ™å¯¹è¯¥å•è¯copyä¹Ÿä¼šå¤šï¼Œå½±å“è¯­å¥é€šé¡ºã€‚

    * ä¸ºæ­¤æ”¹è¿›ä¸ºMaxout Pointerï¼š
      $$
      \operatorname{sc}^{\text {copy }}\left(y_{t}\right)= \begin{cases}\max _{k, \text { where } x_{k}=y_{t}} r_{t, k}, & y_{t} \in \chi \\ -i n f, & \text { otherwise }\end{cases}
      $$

:fire: **Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model**,in ICLR 2020. [[pdf](https://arxiv.org/abs/1908.04942)] [[torch](https://github.com/hugochan/RL-based-Graph2Seq-for-NQG.)]

* å°†passageå’Œanswerçš„è¡¨ç¤ºï¼ˆåŒ…å«bertå‘é‡ï¼Œgloveå‘é‡ï¼Œè¯æ±‡ç‰¹å¾ç­‰ï¼‰è¿›è¡Œå¤šæ¬¡åå¤çš„äº¤äº’è¿›è¡Œç¼–ç ï¼ˆ**éå¸¸ç»†èŠ‚**çš„deep alignment networkï¼‰
* åˆ©ç”¨GNNæ¥ç¼–ç ï¼ˆä½¿ç”¨äº†ä¸¤ç§æ–¹å¼ï¼‰ï¼š
  * å¯¹sentenceåš**dependency parsing**ï¼Œç„¶åç›¸é‚»çš„å¥å­é“¾æ¥å¾—åˆ°passageçš„å›¾
  * é€šè¿‡self attentionçš„æ–¹å¼å¾—åˆ°passage çš„å›¾ï¼ˆæƒå€¼çŸ©é˜µï¼‰

:fire: **Improving Question Generation With to the Point Context**, in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1317.pdf)]

* è”åˆå»ºæ¨¡éç»“æ„åŒ–å¥å­ï¼ˆåŸæ–‡ï¼‰å’Œç»“æ„åŒ–ç­”æ¡ˆç›¸å…³å…³ç³»ï¼ˆ answer-relevant relation é¢„å…ˆä»å¥å­ä¸­æå–ï¼‰ä»¥ç”Ÿæˆé—®é¢˜(**æŠ“å–é‡ç‚¹ä¸Šä¸‹æ–‡**)
* ä½œè€…å‘ç°ä¸Šä¸‹æ–‡ä¸­ï¼Œè·ç¦»ansewræ¯”è¾ƒè¿œçš„è¯å¹¶ä¸ä¸€å®šä¸é‡è¦ï¼Œç›¸å¯¹çš„è·Ÿanswerç´§è´´çš„è¯ä¹Ÿæœ‰å¾ˆå¤šæ— å…³çš„ï¼Œä¸ºäº†æ•æ‰è¿™ç§å…³ç³»ï¼Œä½œè€…ä½¿ç”¨**OpenIE**è¿™ä¸ªå·¥å…·æŠ½å–ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨çš„**å…³ç³»ä¸‰å…ƒç»„**ã€‚

**ä¸‰ã€å¤šä»»åŠ¡è®­ç»ƒ**

**Multi-Task Learning with Language Modeling for Question Generation**, in EMNLP 2019. [[pdf](http://aclanthology.lst.uni-saarland.de/D19-1337.pdf)]

* æŠŠè¯­è¨€æ¨¡å‹ï¼ˆé¢„æµ‹å‰åè¯ï¼‰å’ŒQGä½œä¸ºmulti-taskä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚
* ä¸¤ä¸ªä»»åŠ¡æ˜¯å±‚çº§çš„å…³ç³»ï¼Œå…ˆè¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„æµ‹ï¼Œç„¶åå°†è¯­è¨€æ¨¡å‹çš„hiddenä½œä¸ºç‰¹å¾æä¾›ç»™åé¢seq2seq

:fire: **Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1912.00879)]

* å‡ºå‘ç‚¹æ˜¯æ˜¯**è§£å†³ç”Ÿæˆé”™è¯¯çš„ç–‘é—®è¯**å’Œ**copyåŸæ–‡ä¸­æ— å…³è¯**çš„é—®é¢˜

* ä½œè€…è®¤ä¸ºç”Ÿæˆé”™è¯¯è¯çš„åŸå› æ˜¯æ²¡æœ‰æ­£ç¡®çš„åˆ©ç”¨**answer position**ä¿¡æ¯ï¼Œcopyæ— å…³è¯çš„åŸå› æ˜¯ç¼ºä¹**å±€éƒ¨è¯­ä¹‰ä¿¡æ¯**ã€‚

* ä¸ºäº†åˆ†åˆ«ç¼“è§£è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½œè€…ä¹Ÿæ˜¯è®¾è®¡äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼š

  * è¯­ä¹‰åŒ¹é…åˆ†ç±»ï¼šè¿™ä¸ªä»»åŠ¡çš„è®¾è®¡å‡ºå‘ç‚¹ä¹Ÿæ˜¯SQuADçš„æ•°æ®ç‰¹ç‚¹ï¼Œå¯¹äºä¸€ä¸ªpassageå­˜åœ¨å¤šä¸ªanswer-questionè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å¯¹è¿™æ ·çš„æ•°æ®å®¹æ˜“äº§ç”Ÿä¸€äº›å®½æ³›ä¸å…·ä½“çš„é—®é¢˜ã€‚æ‰€ä»¥ä½œè€…æŠŠpassage-questionä½œä¸ºæ­£æ ·æœ¬ï¼Œpassage-random selected questionï¼Œ random selected passage-questionä½œä¸ºè´Ÿæ ·æœ¬è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚

  ![img](https://pic3.zhimg.com/80/v2-6e137d01fd833693fcc6cf526a39fb8e_720w.jpg)

  * answer-postionä½ç½®é¢„æµ‹ï¼šäº†è®©æ¨¡å‹æ›´å¥½çš„åˆ©ç”¨answerä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹answeråœ¨ä¸Šä¸‹æ–‡ä¸­startå’Œendä½ç½®çš„æ¨¡å‹ï¼ˆpointer networkï¼‰ï¼Œå…¶ä¸­åŸºç¡€çš„ç¼–ç éƒ¨åˆ†é‡‡ç”¨BiDAFçš„æ–¹å¼ã€‚
  * ç„¶å**QGå’Œè¿™ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ä¸€èµ·è®­ç»ƒ**ï¼Œæ•ˆæœå¯ã€‚

:hammer_and_wrench: **Varifocal Question Generation for Fact-checking**, in EMNLP 2022. [[pdf](https://arxiv.org/abs/2210.12400)] [[torch]()]

> ä½¿ç”¨QGè¾…åŠ©åšFact-checkingä»»åŠ¡ï¼ˆå¾ˆæœ‰ç”¨çš„ä¸€ä¸ªåº”ç”¨åœºæ™¯ï¼‰ï¼Œä¹Ÿæœ‰å¯¹åº”çš„æ•°æ®é›†

![image-20221113161641588](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221113161641588.png)





:fire: :hammer_and_wrench: **Generative Language Models for Paragraph-Level Question Generation**, in EMNLP 2022 oral. [[pdf](https://arxiv.org/abs/2210.03992)] [[demo](https://autoqg.net/#/)]

> å¤§æ¨¡å‹åšQGçš„Benchmark
>
> * éå¸¸solidçš„ä¸€é¡¹å·¥ä½œ

* åŠ¨æœº
  * ä¹‹å‰éƒ½æ˜¯åŸºäºè¯„ä»·æŒ‡æ ‡æ¥è¿›è¡Œè®¡ç®—çš„ï¼Œå¦‚`BLEU`ç­‰
  * å­˜åœ¨å¾ˆå¤šæ–¹é¢çš„çº¦æŸï¼Œæ¯”å¦‚å¯å›ç­”æ€§ï¼Œè¾“å…¥ä¸ä¸€æ ·ç­‰
* è´¡çŒ®
  * ç»Ÿä¸€åˆ°ä¸€ä¸ªbenchmarkå½“ä¸­
    * ç‰¹å®šé¢†åŸŸçš„dataset
    * å¤šç§è¯­è¨€ï¼ˆ8ç§ï¼‰
  * åœ¨LMä¸­è¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¸Šä¸€ç‚¹æåˆ°çš„æ•°æ®ä¸­è¿›è¡ŒéªŒè¯
  * å¤šæ–¹é¢éªŒè¯
    * è‡ªåŠ¨è¯„ä»·æŒ‡æ ‡
    * äººå·¥è¯„ä»·æŒ‡æ ‡



:hammer_and_wrench: **Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-Centric Summarization**, in ACL 2022. [[pdf](https://arxiv.org/abs/2203.14187)] [[torch](https://github.com/zhaozj89/Educational-Question-Generation)]

* åŠ¨æœº

  * ä»ç«¥è¯æ•…äº‹å½“ä¸­ç”Ÿæˆ`hugh-cognitive-demand`çš„é—®é¢˜å¾ˆæœ‰æ„ä¹‰
  * è¿‡å»éƒ½æ˜¯`low-dognitive-demand (LCD)` é—®é¢˜æè¿°ï¼Œæ¯”å¦‚è¯´ ä»–æ˜¯è°ï¼Ÿ
  * æ•…äº‹çš„äº‹ä»¶ä¹‹é—´æ˜¯æœ‰è”ç³»çš„ï¼Œéœ€è¦é—®å‡ºactionï¼Œcausal relationshipçš„é—®é¢˜

* æ–¹æ³•

  ![image-20221114170953173](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221114170953173.png)



:hammer_and_wrench: **CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation**, in ACL 2022.  [[pdf](https://aclanthology.org/2022.acl-long.475.pdf)] [[torch](https://github.com/sion-zcfei/CQG)]

* åŠ¨æœº

  * è¿‡å»çš„å¤šæŒ‘QGæ–¹æ³•æ— æ³•ä¿è¯é—®é¢˜çš„**å¤æ‚ç¨‹åº¦**ï¼ˆcomplexityï¼‰
  * ä¸¤å¤§æŒ‘æˆ˜
    * å»ºç«‹ä¸åŒæ–‡æ¡£ä¿¡æ¯çš„è”ç³»
    * complex chains of entities

  ![image-20221115112431373](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221115112431373.png)

* æ–¹æ³•

  * å…ˆè¯•ç”¨GATæŠ½å–å¤šæ–‡æ¡£ä¹‹é—´è”ç³»ï¼Œ`Standford corenlp toolkit`æ¥å»ºç«‹å®ä½“å›¾

    * é‡ç‚¹åœ¨äºæ‰¾åˆ°å…³é”®çš„å®ä½“ï¼Œå’Œgtè¿›è¡Œlossçš„è®¡ç®—

  * è®¾è®¡flag tagæ¥çº¦æŸchain of entitiesï¼Œä¿è¯é—®é¢˜ç”Ÿæˆçš„å¤æ‚ç¨‹åº¦ï¼ˆæ³¨æ„ï¼šå¹¶ä¸æ˜¯`teacher forcig`ï¼‰
    $$
    \operatorname{flag}_i^t= \begin{cases}0 & x_i \text { is not a constrain } \\ 1 & x_i \text { does not appear in } y_{1: t} \\ 2 & x_i \text { appear in } y_{1: t}\end{cases}
    $$
    

    ![image-20221115112909494](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221115112909494.png)

  * æ¨¡å‹å›¾

  ![image-20221115155002446](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221115155002446.png)

  


****

### :sunrise: Visual QG

:fire: **Generating Natural Questions About an Image**, in ACL 2022.

> ç¬¬ä¸€ç¯‡VQGçš„å·¥ä½œ

* åŠ¨æœº

  * ä¸captionç›¸æ¯”ï¼ŒVQGæ›´å¸Œæœ›ç¨³ä¸€äº›è¶…å‡ºè¯­è¨€è¡¨è¾¾æœ¬èº«ï¼Œå…·æœ‰é€»è¾‘æ¨ç†ï¼Œ**å¸¸è¯†çŸ¥è¯†**çš„ä¸€äº›é—®é¢˜

  ![image-20221111221831899](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221111221831899.png)

  ![image-20221111221858832](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221111221858832.png)

* æ–¹æ³•

  * å¾ˆç®€å•çš„æ–¹æ³•ï¼Œå°±æ˜¯ä¸€äº›ç®€å•çš„baselineå°è¯•

:hammer_and_wrench: **[No Visual] Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing**, in AAAI 2021. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17544)] [[torch](https://github.com/VISLANG-Lab/EGSS)]

* Multi-feature Encoder: ä½¿ç”¨äº†POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰+ NERï¼ˆå…³ç³»æŠ½å–ï¼‰

:hammer_and_wrench: **Multiple Objects-Aware Visual Question Generation**, in ACM MM 2021. [[pdf](https://dl.acm.org/doi/abs/10.1145/3474085.3476969)]
* **å†™ä½œä¸Šå†™å¾—å¾ˆå®åœ¨ï¼Œå¾ˆå®¹æ˜“æ‡‚**ï¼Œæœ‰å¾ˆå¤šæ‰¿ä¸Šå¯ä¸‹çš„å¥å­ã€‚
* é¦–æ¬¡å°†**å¯¹è±¡**èå…¥åˆ°é—®é¢˜ç”Ÿæˆä»»åŠ¡å½“ä¸­

![image-20220629230122528](https://s2.loli.net/2022/06/29/6Mn4HPG9ZiCjOqv.png)

:hammer_and_wrench:  **Difficulty-Controllable Visual Question Generation**, in APWeb-WAIM 2021. [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-85896-4_26.pdf)]

* **éš¾åº¦å¯æ§**çš„é—®é¢˜ç”Ÿæˆï¼šé‡‡ç”¨äº†æ•™è‚²å­¦é¢†åŸŸæ”¶é›†å¥½çš„é—®é¢˜éš¾åº¦æ ‡ç­¾(DIF), è¯¦è§[é“¾æ¥](https://www.apims.net/index.php/apims/article/view/9)
* åœ¨VQA2.0æ•°æ®é›†çš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŒºåˆ†ä¸ºå®¹æ˜“å’Œéš¾çš„é—®é¢˜æ•°æ®é›†
  * å¼•å…¥ä¸¤ä¸ªVQAçš„æ¨¡å‹æ¥è¿›è¡Œå›ç­”ï¼Œéƒ½å›ç­”å¯¹çš„ä¸ºå®¹æ˜“ï¼Œéƒ½å›ç­”é”™è¯¯å°±æ˜¯éš¾çš„

* ![image-20220629231350190](https://s2.loli.net/2022/06/29/POftb69si7hnINX.png)
  * å…¶ä¸­Difficulty Variableå°±æ˜¯$\{0, 1\}$



:hammer_and_wrench: **Learning to Caption Images Through a Lifetime by Asking Questions**, in ICCV 2019.  [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9009050)] [[torch](https://github.com/fidler-lab/Caption-Lifetime-by-Asking-Questions)]

* å°†Caption å’Œ VQG ä¸€èµ·æ¥åšï¼Œæå‡ç”Ÿæˆçš„æ€§èƒ½

:hammer_and_wrench: **Inferential Visual Question Generation**, in MM 2022.  [[pdf](https://dl.acm.org/doi/10.1145/3503161.3548055)] [[dataset & code](https://github.com/bcxbg/InVQG)]

> æ— éœ€æ¨¡å‹è®­ç»ƒçš„ï¼Œå°±æ˜¯äººä¸º**è§„å®šå‡½æ•°æ¨¡ç‰ˆ**è§„åˆ™çš„æ–¹æ³•ç”Ÿæˆçš„ã€‚

* åŠ¨æœº
  * è¿‡å»VQGçš„æ–¹æ³•ç”Ÿæˆé—®é¢˜ä¸å¤Ÿchallenge æˆ–è€…ä¾èµ–äºäººå·¥æ ‡è®°
* è¿‡å»çš„VQG
  * åªæ˜¯ç®€å•çš„åè½¬ä¸€ä¸‹VQA
  * æ•°æ®é›†å½“ä¸­æœ‰å¾ˆå¤šlong tailå’Œè¯­ä¹‰è”ç³»å¤ªå¼±äº†
  * ç”Ÿæˆé—®é¢˜å¤ªè¿‡æ™®é€šï¼Œè€Œä¸”ç¼ºå°‘å…³é”®ä¿¡æ¯
  * ä¸Captionä¸åŒçš„æ˜¯ï¼ŒVQGéœ€è¦ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜
* æ–¹æ³•

![image-20221015215029327](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221015215029327.png)

* ä¾‹å­
  * å¢åŠ å¯äº¤äº’æ€§

![image-20221015215356890](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221015215356890.png)

![æˆªå±2022-10-15 21.54.35](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-15%2021.54.35.png)

### :video_camera: Video QG

**Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning**, in ICASSP 2020. [[pdf](https://ieeexplore.ieee.org/document/9053476)]

* ä½¿ç”¨äº†**[TVQA](https://paperswithcode.com/dataset/tvqa)**æ•°æ®é›†ï¼Œis based on 6 popular TV shows and consists of **152,545 QA pairs** from **21,793 clips**.
* æ€»ä½“æ²¡ä»€ä¹ˆåˆ›æ–°çš„

**Multi-Turn Video Question Generation via Reinforced Multi-Choice Attention Network**, in T-CSVT 2021.[[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9161024)]

* Multi-Turnï¼ˆM-VQGï¼‰ï¼šç»“åˆå¤šè½®å¯¹è¯+è§†é¢‘ä¿¡æ¯
* ä¼˜ç‚¹ï¼š åˆ©ç”¨åŠ¨æ€åœºæ™¯ä¿¡æ¯ï¼Œé—®é¢˜å¯å›ç­”æ€§ï¼Œå¯¹è¯è®°å½•ä¿¡æ¯æŠ½å–
* æ–¹æ³•ï¼šbaselineæ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆçœ‹ä¸æ‡‚ï¼‰

**End-to-End Video Question-Answer Generation with Generator-Pretester Network**, in T-CSVT 2021. [[pdf](https://arxiv.org/pdf/2101.01447.pdf)]

* å¼•å…¥ä¸€é—®ä¸€ç­”çš„å½¢å¼ï¼Œç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œç„¶åæµ‹è¯•ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
* ç¡¬ä»¶å¹³å°ï¼šNVIDIA DGX-1ï¼ˆ8 * V100ï¼‰

### :sun_with_face: QG examples

:white_check_mark:  :hammer_and_wrench:  **Mixture Content Selection for Diverse Sequence Generation**, in EMNLP 2019.[[pdf](https://arxiv.org/abs/1909.01953)] [[torch](https://github.com/clovaai/FocusSeq2Seq)]

:hammer_and_wrench: **Radial Graph Convolutional Network for Visual Question Generation**, in IEEE Transactions on Neural Networks and Learning Systems 2020. [[pdf](https://ieeexplore.ieee.org/document/9079208)] [[torch](https://github.com/Wangt-CN/VQG-GCN)]

## :bookmark_tabs: Question Answering & Reasoning

---

### :sunflower: Visual

> åœ¨2022å¹´çš„ä»Šå¤©ï¼ŒVQAä»»åŠ¡ä¸å¤ªå¯èƒ½ä»åˆ·åˆ†çš„è§’åº¦æ¥å…¥æ‰‹äº† [[Blogé“¾æ¥](https://www.zhihu.com/question/419828408/answer/1595386400)]
>
> - VQAä»»åŠ¡æ˜¯ä»€ä¹ˆ
>
> - ä»‹ç»ä¹‹å‰çš„æ¨¡å‹å’Œæ–¹æ³•
>
> - æ¬¢è¿æ¥åˆ°Transformerçš„æ—¶ä»£
>
> - - 2019ï¼šå°è¯•å¤šæ¨¡æ€è¡¨å¾
>   - 2020ï¼šæ‹¥æŠ±å¤šæ¨¡æ€è¡¨å¾
>   - 2021ï¼šç»Ÿä¸€æ„æ¶çš„æ¢ç´¢

machine reading comprehension (**MRC**)å’Œquestion answering (QA)çš„å…³ç³»å…¶å®æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚Pure VQAä»»åŠ¡ä¸€èˆ¬æ˜¯æ²¡æœ‰å¼•å…¥é¢å¤–çš„**æ–‡æœ¬å†…å®¹**ï¼Œåªæ˜¯å•çº¯çš„æœ‰$\{å›¾ï¼Œ é—®å¥ï¼Œ å›ç­”\}$ã€‚è€ŒMultimodal MRCä»»åŠ¡ï¼Œå®é™…ä¸Šå°±åªæ˜¯å¼•å…¥äº†**é¢å¤–çš„context**ä½œä¸ºVQAä»»åŠ¡çš„çŸ¥è¯†ï¼Œå¹¶ä¸”æ›´åŠ æ³¨é‡äºè‡ªç„¶è¯­è¨€çš„ç†è§£ã€‚MRCçš„ä¸»è¦**ä»»åŠ¡ç±»å‹**ä¸€å…±æœ‰å››ç§ï¼Œåˆ†åˆ«ä¸º:

* å®Œå½¢å¡«ç©ºï¼ˆCloze Styleï¼‰
* å¤šé¡¹é€‰æ‹©ï¼ˆMultiple Choiceï¼‰
* ç‰‡æ®µæŠ½å–ï¼ˆSpan Predictionï¼‰
* è‡ªç”±ä½œç­”ï¼ˆFree-form Answerï¼‰

**[éæ·±åº¦å­¦ä¹ æ–¹æ³•] Answer-Type Prediction for Visual Question Answering**ï¼Œin CVPR 2016. [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780907)]

* é¢„æµ‹é—®é¢˜ç±»åˆ«ï¼ˆäººä¸ºæ ‡å®šï¼‰çš„æ¦‚ç‡å†å›ç­”é—®é¢˜
* åˆ©ç”¨**è´å¶æ–¯ç®—æ³•**å¯¹ç›®æ ‡çš„ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè®¡ç®—å‡ºæ¯ä¸ªç­”æ¡ˆçš„æ¦‚ç‡
* å…¶æœ‰æ•ˆæ€§ä¸å¦‚ç®€å•çš„åŸºçº¿æ¨¡å‹ï¼›éƒ¨åˆ†åŸå› åœ¨äºå…¶**ä¾èµ–è¯­ä¹‰åˆ†å‰²çš„ç»“æœ**



**Differential Attention for Visual Question Answering**, in CVPR 2018. [[pdf](https://arxiv.org/pdf/1804.00298.pdf)]

* è§£å†³ä¸ºäº†è®©æ¨¡å‹æ›´åŠ å…³æ³¨åˆ°**äººç±»æ‰€å…³æ³¨**çš„åŒºåŸŸ

![image-20220910151132747](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220910151132747.png)



:fire: :hammer_and_wrench: **[å› æœå…³ç³»] Visual Commonsense R-CNN**, in CVPR 2020. [[pdf](https://arxiv.org/abs/2002.12204)] [[torch](https://github.com/Wangt-CN/VC-R-CNN)] [[blog](https://zhuanlan.zhihu.com/p/111306353)]

> å‡ºè‡ª[MReal](https://mreallab.github.io/)ï¼Œ å¼ å«æœ›è€å¸ˆå›¢é˜Ÿçš„å·¥ä½œï¼Œéå¸¸Solidçš„ä¸€ç¯‡å·¥ä½œ
>
> * ç›®æ ‡æ˜¯è®­ç»ƒåŸºäº`Faster-RCNN`è®­ç»ƒä¸€ä¸ªæ›´å¼ºçš„`feature extractor`å¯ä»¥æ•è·è§†è§‰ä¸Šçš„å¸¸è¯†ä¿¡æ¯ã€‚
> * è¿™ç¯‡è®ºæ–‡å®åœ¨**å¤ªå¤šç»†èŠ‚å’Œæ¨ç†**äº†ï¼Œå»ºè®®çœ‹æˆ‘è‡ªå·±çš„**GoodNoteä¸Šçš„ç¬”è®°**ï¼

* åŠ¨æœº

  * ç°åœ¨çš„æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°è§†è§‰å¸¸è¯†ï¼ˆ**Commonsense**ï¼‰ï¼šäººå’Œæ¤…å­ -> äººå¯ä»¥ååœ¨æ¤…å­ä¸Šã€‚ä½†åœ¨NLPä¸­ï¼Œå¸¸è¯†çš„ä¿¡æ¯å·²ç»æ”¾åœ¨ç‰¹å¾é‡Œé¢äº†

    ![image-20220913155431514](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220913155431514.png)

  * æ•°æ®é›†çš„åå·®ä¼šå¯¼è‡´æ— æ³•æ•æ‰åˆ°å¸¸è¯†ä¿¡æ¯
    * çœŸæ­£çš„**è§†è§‰å…³ç³»**æ— æ³•æè¿°ï¼ˆå·¦å›¾ï¼‰
    * ç»™å‡ºçš„**è§£é‡Š**ä¸å¤Ÿæ­£ç¡®ï¼ˆå³å›¾ï¼‰

  **å› æœç†è®ºå°±æ˜¯ç”¨æ¥å‘ç°==ç°è±¡èƒŒåçš„ä¸å˜è§„å¾‹==çš„ï¼Œæ˜¯ä¸€ç§é²æ£’çš„é¢„æµ‹ã€‚è¿™ä¸å¸¸è¯†æœ¬èº«ä¸å°±å¾ˆç›¸ä¼¼å—ï¼Œæˆ‘ä»¬äººç±»ä¹Ÿæ˜¯ä»ç”Ÿæ´»ä¸­ä¸æ–­æ€»ç»“ç§¯ç´¯è¿™äº›ä¸å˜çš„ã€é²æ£’çš„ç»éªŒæˆ–è€…å› æœè§„å¾‹ï¼Œå¹¶æŠŠä»–ä»¬å«åšå¸¸è¯†ã€‚** æ¯”å¦‚ï¼Œçœ‹è§å‡³å­çŸ¥é“å¯ä»¥åï¼Œçœ‹è§pizzaçŸ¥é“å¯ä»¥åƒã€‚

**Association å’Œ Interventionï¼ˆåˆ†å±‚ï¼‰çš„è®¡ç®—**
$$
\begin{gathered}
P(Y \mid X)=\sum_z P(Y \mid X, z) P(z \mid X)=\frac{P(Y, X)}{P(X)} \\
P(Y \mid d o(X))=\sum_z P(Y \mid X, z) P(z)=\sum_z \frac{P(Y, X, z) P(z)}{P(X, z)}
\end{gathered}
$$
å…¶ä¸­ $X, Y, z$åˆ†åˆ«ä»£è¡¨äº†å›¾ç‰‡ä¸­çš„object labelï¼ŒåŒæ—¶è¿™é‡Œæˆ‘ä»¬ç”¨ç‰©ä½“å‡ºç°çš„é¢‘ç‡æ¥ä»£æ›¿æ¦‚ç‡ï¼Œæ¯”å¦‚ $P(Sink|Hair drier)$å°±æ˜¯ç”¨â€œå«æœ‰$Sink$å’Œ$Hair drier$ä¸¤è€…çš„å›¾ç‰‡æ•°â€æ¯”ä¸Šâ€œåªå«æœ‰Hair drierçš„å›¾ç‰‡æ•°â€è®¡ç®—å¾—åˆ°çš„ã€‚ç”»å‡ºä¸¤è€…è®¡ç®—ç»“æœå·®å¼‚çš„å¯¹æ¯”å›¾ï¼ˆåªæ ‡æ˜äº†20ç±»ï¼‰ï¼š

![img](https://pic2.zhimg.com/80/v2-ddffe6ddaaf70839faa1d62a9ef25291_720w.jpg)

* ä¸¤ä¸ªCaseçš„åˆ†æ
  * $Sink å’Œ drier$ï¼Œæƒ³è¦æ¢å¯»åœ¨**å·²çŸ¥å¹é£æœº**çš„æƒ…å†µä¸‹ï¼Œå»é¢„æµ‹æ°´æ± çš„å¯èƒ½æ€§å¤§å° $P(Sink|drier)$
    * **åœºæ™¯å› ç´ **è€ƒè™‘åœ¨å†…ï¼Œå¯¹ä¸åŒçš„åœºæ™¯è¿›è¡Œåˆ†å±‚ï¼ˆå› ä¸ºåœºæ™¯å°±æ˜¯ç”±objectç»„æˆçš„ï¼‰ï¼Œå¾—åˆ°å®é™…çš„å› æœæ•ˆåº”ï¼Œæ¯”å•çº¯Associationç®—çš„æ•°å€¼è¦ä½
  * äººå’Œé©¬æ¡¶ï¼Œæ¢å¯»â€œé©¬æ¡¶â€å’Œâ€œäººâ€ä¹‹é—´å¯èƒ½å­˜åœ¨çš„å› æœæ•ˆåº”
    * æ•°æ®é›†ä¸­äººå’Œé©¬æ¡¶ä¸€èµ·å‡ºç°çš„æ ·æœ¬å…¶å®ä¸å¤šï¼ˆä¹Ÿä¸ä¼šæœ‰å¾ˆå¤šäººåœ¨é©¬æ¡¶æ—è¾¹æ‹ç…§ï¼‰
    * å¦‚æœæƒ³è¦åšå‡ºæ›´robustçš„é¢„æµ‹ï¼Œæˆ‘ä»¬å°±éœ€è¦è€ƒè™‘æ··æ‚å› å­**confounder**ï¼Œ æ¯”å¦‚ç“¶å­ã€æ°´æ± ã€æ¯å­ç­‰ç­‰ã€‚æŒ‰ç…§confounder è¡Œ**åˆ†å±‚è®¡ç®—**ï¼Œæœ€åå†åŠ æƒæ±‚å’Œã€‚

* æ–¹æ³•ï¼ˆå› æœå¹²é¢„**Intervention**ï¼‰

  * ä»£ç†ä»»åŠ¡ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ï¼š**ç»™å®šRoI Xçš„featureå»é¢„æµ‹RoI Yçš„ç±»åˆ«**

  * åŒ…æ‹¬å¾ˆå¤šæ½œåœ¨çš„**æ··æ‚å› å­**ï¼Œå¦‚æœç›´æ¥é¢„æµ‹å‘¨å›´ç‰©ä½“Yå°±ä¸å¯é¿å…çš„ä¼šè¢«ä¸Šæ–‡æåˆ°çš„æ··æ‚å› å­**confounder**æ‰€å½±å“ã€‚æ ¹æ®æˆ‘ä»¬åˆšåˆšä»‹ç»çš„**â€œdoç®—å­â€**çš„ç†è®ºï¼Œè§£å†³çš„åŠæ³•ä¹Ÿä¸éš¾ï¼Œåªè¦èƒ½æ‰¾åˆ°confounderç„¶åå¯¹ä»–ä»¬ä½¿ç”¨**backdoorç†è®º**è¿›è¡Œæ§åˆ¶å³å¯ã€‚

  * æ··æ‚å› å­æ˜¯ä»€ä¹ˆï¼Ÿ æˆ‘ä»¬ç›´æ¥æŠŠæ•´ä¸ªæ•°æ®é›†ä¸Šçš„**object RoIç‰¹å¾ï¼ˆFaster RCNNä¸­æ¥ï¼‰åœ¨æ¯ä¸ªç±»åˆ«ä¸Šå–å¹³å‡**ï¼Œå½“ä½œè¿™ä¸ªç±»åˆ«çš„è¡¨ç¤ºï¼Œè¿›è€Œæ„å»ºå‡ºä¸€ä¸ª **ç±»åˆ«æ•°x1024** çš„confounderå­—å…¸ä½œä¸º$Z$ï¼ˆæ¯”å¦‚MSCOCOæœ‰80ç±»ï¼Œå°±æ˜¯ 80x1024ï¼‰ï¼Œå®ƒåŒ…å«ç€æ‰€æœ‰å¯èƒ½çš„æ··æ‚å› å­ã€‚

  * åé—¨è°ƒæ•´

    ![img](https://raw.githubusercontent.com/Gary-code/pic/main/img/v2-514061ff24e803c016324ead8bcf84b1_720w.jpg)

    * æˆ‘ä»¬æŠŠconfounder dictionary $Z$ä¸­çš„ç‰©ä½“$z_i$â€œborrowâ€åˆ°å½“å‰å›¾ç‰‡ä¸­ï¼Œæ³¨æ„è¿™é‡Œçš„ç‰©ä½“$z_i$ä¸éœ€è¦æ˜¯å½“å‰å›¾ç‰‡ä¸­å­˜åœ¨çš„ï¼Œæ‰€ä»¥æ˜¯ä¸€ç§globalå±‚é¢çš„å®šä¹‰ã€‚
    * ç„¶åæŠŠå€Ÿæ¥çš„$z_i$â€œputâ€åˆ°$X, Y$å‘¨å›´å’Œ$X, Y$å¯¹æ¯”ï¼Œä¾‹å¦‚ä¸Šå›¾ä¸­çš„æŠŠ sinkã€handbagã€chairç­‰ç­‰ç§»åˆ° toilet å’Œ person å‘¨å›´è¿›è¡Œbackdoorçš„è®¡ç®—ã€‚

  * æ¨¡å‹

    * æ•´ä¸ªinterventionæ•´åˆæˆä¸€è·¯context predictorã€‚
    * åŒæ—¶ä¸ºäº†ä¸è®©ç½‘ç»œå¿˜æ‰è¯†åˆ«RoIæœ¬èº«ç±»åˆ«çš„èƒ½åŠ›ï¼Œcontext predictorçš„åŸºç¡€ä¸Šåˆä¿ç•™äº†åŸå…ˆçš„è‡ªèº«ç±»åˆ«é¢„æµ‹**self predictor**ã€‚

  ![img](https://pic2.zhimg.com/80/v2-d3a05b26274f54a8bd785209f1b6a4c1_720w.jpg)

  æ³¨æ„ï¼šVC R-CNNçš„å®ç°å’ŒåŸå…ˆçš„Faster R-CNNç›¸æ¯”ï¼Œ**å»é™¤äº†RPNç½‘ç»œ**ï¼ˆRegion Proposal Networkï¼‰ï¼Œä¸å†è®­ç»ƒç½‘ç»œproposeè¾¹ç•Œæ¡†ï¼Œè€Œæ˜¯ç›´æ¥å°†æ•°æ®é›†**ground-truthçš„bounding boxåæ ‡è¾“å…¥åˆ°å…¶ä¸­**ï¼Œç›´æ¥æå–regionçš„ç‰¹å¾ã€‚è€Œåœ¨è®­ç»ƒå®Œæˆåçš„featureæå–é˜¶æ®µï¼Œç›¸å¯¹åº”çš„ï¼Œåªè¦ç»™å®šå›¾ç‰‡å’Œbounding boxåæ ‡ï¼Œéƒ½å¯ä»¥è·å¾—å¯¹åº”çš„VCç‰¹å¾ã€‚å°±è¿™æ ·ï¼Œæˆ‘ä»¬åˆ©ç”¨bottomupç‰¹å¾å·²æœ‰çš„è¾¹ç•Œæ¡†åæ ‡æå–VCç‰¹å¾åï¼Œå°†å…¶å¹¶åœ¨å…ˆå‰çš„bottomupç‰¹å¾ä¸Šä½œä¸ºæ–°çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¼ ç»Ÿçš„ Vision&Language ä¸‰å¤§ä»»åŠ¡ä¸ŠæŒ‘é€‰äº†ç»å…¸modelå’ŒSOTA modelè¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°åœ¨å„ä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜æ˜¾çš„æå‡ï¼Œå°¤å…¶æ˜¯åœ¨image captioningä¸Šçš„æå‡å°¤å…¶å¤§ã€‚åŒæ—¶ä¸ºäº†éªŒè¯æ€§èƒ½çš„æå‡ä¸æ˜¯ç”±äºå‚æ•°å¢å¤šå¸¦æ¥çš„ï¼Œæˆ‘ä»¬è¿˜åœ¨åŸæœ‰ç‰¹å¾ä¸Šå¹¶ä¸Šäº†ablativeçš„ç‰¹å¾ï¼ˆå•ç‹¬objectç‰¹å¾ï¼Œç”¨correlationè®¡ç®—çš„ç‰¹å¾ï¼‰ï¼Œå…·ä½“å¯ä»¥å‚è€ƒè®ºæ–‡çš„å®éªŒéƒ¨åˆ†ã€‚

:hammer_and_wrench: **MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering**, in CVPR 2022.  [[pdf](https://arxiv.org/pdf/2203.09138.pdf)] [[torch](https://github.com/AndersonStra/MuKEA.)]

* åŠ¨æœº

  * è¿‡å»åŸºäºçŸ¥è¯†çš„ï¼Œéƒ½åªæ˜¯è€ƒè™‘äº†æ–‡æœ¬ä¸Šçš„çŸ¥è¯†ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€çŸ¥è¯†çš„ç†è§£

  ![image-20220901165323764](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901165323764.png)

* ä¸»è¦è´¡çŒ®

  * ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€çŸ¥è¯†è¡¨ç¤º $(Entity, relation, answer)$
  * **pre-training and fine-tuning** strategy to accumulate both **out-domain and in-domain** knowledge

![image-20220901165520112](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901165520112.png)

* ç»†èŠ‚

  * ä¸‰ä¸ª**æŸå¤±å‡½æ•°**çš„è®¾è®¡

    * `Triplet TransE Loss`: ä¿æŒembeddingçš„ç»“æ„ï¼ˆé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼‰

    $$
    \mathcal{L}_{\text {TransE }}=\sum_{t^{+} \in \mathcal{A}^{+}} \sum_{t^{-} \in \mathcal{A}^{-}}\left[\gamma+\mathrm{d}\left(h+\boldsymbol{h}, \boldsymbol{t}^{+}\right)-\mathrm{d}\left(\boldsymbol{h}+\boldsymbol{r}, \boldsymbol{t}^{-}\right)\right]_{+}
    $$

    * `Triplet Consistency Loss`ï¼š ä¿è¯ä¸¥æ ¼çš„**æ‹“æ‰‘å…³ç³»**

    $$
    \mathcal{L}_{\mathrm{Tri}}=\operatorname{MSE}\left(h+r, t^{+}\right)
    $$

    * `Semantic Consistency Loss`: ä¿æŒåœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨è¾¾ä¸€è‡´æ€§

    $$
    {P\left(t^{+}\right)=\operatorname{softmax}\left((T)^{T}(h+r)\right)} \\{\mathcal{L}_{\mathrm{Sem}}=-\log \left(P\left(t^{+}\right)\right)}
    $$

    

  * é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥

    * å…ˆåœ¨`VQA 2.0`æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¥æ”¶é›†è§†è§‰ä¸»å¯¼çš„çŸ¥è¯†
    * åœ¨`KB-VQA`æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ

  * å…³äºå°¾éƒ¨`Entity`

    * è®­ç»ƒçš„æ—¶å€™ç›´æ¥åš`teacher-forcing`
    * æ¨ç†çš„æ—¶å€™è®¡ç®—$\mathbf{h}_{inf}+\mathbf{r}_{inf}$ ä¸ `look up` table $\mathbf{T}$çš„æœ€å°è·ç¦»

    $$
    \boldsymbol{t}_{\inf f}=\underset{\boldsymbol{t}_i \in T}{\arg \min } \mathrm{d}\left(\boldsymbol{h}_{\text {inf } f}+\boldsymbol{r}_{\text {inf } f}, \boldsymbol{t}_{\mathrm{i}}\right)
    $$

    

![image-20220901170039718](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901170039718.png)

**[VCR] Explicit Cross-Modal Representation Learning for Visual Commonsense Reasoning**, in TMM 2022. [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9465732)

* åŠ¨æœºï¼šä¸ºäº†åŠ å¼º`VCR`ä»»åŠ¡çš„**reasoning**è¿‡ç¨‹ï¼Œä¸å†é‚£ä¹ˆéšå¼
* æ–¹æ³•

![image-20220923223408003](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220923223408003.png)

* ä¾‹å­

![image-20220923223502035](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220923223502035.png)



:hammer_and_wrench:  **Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations**, in ICML 2022. [[pdf](https://arxiv.org/abs/2106.13876)] [[code (not released in 2022/11/20)](https://github.com/majumderb/rexc)]

> æ³¨æ„è¿™ç¯‡è®ºæ–‡æ—©äºğŸ‘‡ä¸‹é¢çš„ä¸¤ç¯‡è®ºæ–‡ï¼Œæ‰€ä»¥ç»“æœä¸Šå’Œä¸‹é¢ä¸¤ç¯‡è®ºæ–‡æœ‰æ˜æ˜¾çš„å·®è·

* åŠ¨æœº

  * å€ŸåŠ©**å…³é”®ä¿¡æ¯æŠ½å–**è¿˜æœ‰**è§£é‡Šç”Ÿæˆ**çš„æ–¹æ³•æ¥é¢„æµ‹ç­”æ¡ˆä¼šæ›´å¥½

* æ–¹æ³•ï¼ˆè¿™ç¯‡è®ºæ–‡æ–¹æ³•ä¸Šæ–‡ç« æ²¡æœ‰åšå¾ˆè¯¦ç»†çš„ï¼Œæè¿°ï¼Œå…·ä½“ä»£ç ä¹Ÿæ²¡æœ‰å…¬å¼€ï¼Œæ‰€ä»¥æ— æ³•å¾ˆè¯¦ç»†çš„è§£æï¼‰

  ![image-20221120100005850](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221120100005850.png)

  * æŠ½å–å…³é”®å…ƒç´ 
    * ä½¿ç”¨`HardKuma`å©å’çš„æ–¹æ³•æ¥å­¦ä¹ latent selectors é€‰æ‹©åˆé€‚çš„å…ƒç´ 
  * çŸ¥è¯†æŠ½å–
  * çŸ¥è¯†é€‰æ‹©ï¼ˆæ–¹æ³•å’Œç¬¬ä¸€ç‚¹ä¸€è‡´ï¼‰
  * ç”Ÿæˆè§£é‡Š+é¢„æµ‹ç­”æ¡ˆï¼ˆ**å…ˆ**ç”Ÿæˆè§£é‡Š**å**é¢„æµ‹ç­”æ¡ˆï¼‰

  ![image-20221120100330138](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221120100330138.png)

  



:hammer_and_wrench: **NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks**, in CVPR 2022. [[pdf](https://arxiv.org/abs/2203.05081)] [[pdf](https://github.com/fawazsammani/nlxgpt)]

* åŠ¨æœº
  * ä¹‹å‰æ–¹æ³•åˆ†ä¸ºå…ˆç”Ÿæˆ`A`å†ç”Ÿæˆ`E`ä¸¤æ­¥èµ°, ç¼ºä¹å¯¹reasoningè¿‡ç¨‹çš„è€ƒè™‘
  * ç¼ºå°‘ä¸€äº›å¯¹Eå’ŒAç›¸å…³æ€§è¿˜æœ‰å¯¹biasç¨‹åº¦ä¼°è®¡çš„è¯„ä»·æŒ‡æ ‡
  * è¿‡å»å’Œæ–‡ç« æ–¹æ³•ä¸Šçš„å¯¹æ¯”

![image-20221001182330655](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182330655.png)

* æ–¹æ³•ï¼ˆåŠå…¶ç®€å•ï¼‰
  * GPT-2æ¨¡å‹æ˜¯åœ¨å¤§è§„æ¨¡çš„image-captionæ•°æ®é›†ä¸­è’¸é¦å‡ºæ¥çš„
  * Vision Encoder ä½¿ç”¨äº†`CLIP`

![image-20221001182426390](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182426390.png)

* æ–°çš„è‡ªåŠ¨åŒ–è¯„ä»·æŒ‡æ ‡

  * è¯„ä¼°`E`å’Œ`A`çš„ç›¸å…³æ€§

  ![image-20221001182614962](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182614962.png)

  * è¯„ä¼°æ¨¡å‹Biasç¨‹åº¦

  ![image-20221001182654034](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221001182654034.png)

  çº¢è‰²ä¸ºå°äº0çš„å€¼ï¼Œéƒ½ç½®ä¸º0ï¼Œç„¶åè®¡ç®—distanceçš„å¹³å‡å€¼ï¼Œ**å€¼è¶Šå°ï¼Œæ¨¡å‹çš„Biasè¶Šå°**

**[Viisual Explanation]  Chunk-aware Alignment and Lexical Constraint for VisualEntailment with Natural Language Explanations**, in MM 2022. [[pdf](https://arxiv.org/abs/2207.11401)] [[Talk](https://www.youtube.com/watch?v=nAHIZOQSiXg)]

* æ˜¯ä¸Šé¢ä¸€ç¯‡å·¥ä½œçš„è¿›ä¸€æ­¥ç ”ç©¶
* ä»»åŠ¡ä»‹ç»
  * ç»™å®šä¸€ä¸ªå›¾ç‰‡æ–‡æœ¬å¯¹ ----> å…³ç³»ï¼ˆ**entailmentæˆ–è€…ç­”æ¡ˆ**ï¼‰+ è§£é‡Š

![image-20220930221347803](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220930221347803.png)

* åŠ¨æœº

  * ç¼ºå°‘å¯¹Chunkçº§åˆ«çš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯çš„ç†è§£
  * æ²¡æœ‰å†³ç­–çš„è¿‡ç¨‹ï¼Œåªæ˜¯ç®€å•çš„èåˆç‰¹å¾

* æ–¹æ³•ï¼ˆæ³¨æ„ï¼šä½¿ç”¨äº†**Oscar**è¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼‰

  ![image-20220930221438196](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220930221438196.png)

:hammer_and_wrench: **SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering**, in CVPR 2022. [[pdf](https://arxiv.org/abs/2204.02285)] [[torch](https://github.com/vipulgupta1011/swapmix/)]

* åŠ¨æœº

  * ä½œè€…ä»ä¸€ä¸ªæ–°çš„è§’åº¦æ¥ç ”ç©¶ VQA æ¨¡å‹çš„é²æ£’æ€§ï¼švisual contextã€‚å¹¶è¡¨ç¤ºï¼Œè¿™äº›æ¨¡å‹è¿‡åº¦ä¾èµ–visual contextï¼Œå³å›¾åƒä¸­ä¸ç›¸å…³çš„ç‰©ä½“ï¼Œæ¥è¿›è¡Œé¢„æµ‹ã€‚

  ![img](https://pic2.zhimg.com/80/v2-184656fad6e2bfbecb20726736d1283d_720w.webp)

* å‘ç°

  * è™½ç„¶ä¹‹å‰çš„å·¥ä½œä»**è¯­è¨€ä¸Šä¸‹æ–‡**çš„è§’åº¦ç ”ç©¶äº†VQAé²æ£’æ€§ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å¦ä¸€ä¸ªè§’åº¦ç ”ç©¶äº†VQAæ¨¡å‹çš„é²æ£’æ€§ï¼š**è§†è§‰ä¸Šä¸‹æ–‡**ã€‚
  * é€šè¿‡**æ‰°ä¹±ä¸ç›¸å…³çš„ä¸Šä¸‹æ–‡**ï¼Œè¶…è¿‡45%çš„é¢„æµ‹çš„æ­£ç¡®ç­”æ¡ˆéƒ½ä¼šè¢«æ”¹å˜ã€‚è¡¨æ˜VQAæ¨¡å‹é«˜åº¦ä¾èµ–å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ï¼Œå› æ­¤å®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡å¹²æ‰°çš„å½±å“ã€‚
  * è¿‡åº¦ä¾èµ–ä¸Šä¸‹æ–‡åœ¨**å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè§†è§‰è¡¨å¾**çš„è´¨é‡
    * ä¸ªå®Œç¾çš„è§†è§‰æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ä¾èµ–è¦å°å¾—å¤šã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨**groundtruthå¯¹è±¡å’Œå±æ€§ç¼–ç **æ›¿æ¢è§†è§‰è¡¨å¾æ¥å®ç°è¿™ä¸€ç‚¹

* è´¡çŒ®

  * æ˜¯ç¬¬ä¸€ä¸ªä»è§†è§‰ä¸Šä¸‹æ–‡çš„è§’åº¦ç ”ç©¶VQAç¨³å¥æ€§çš„äººï¼Œé€šè¿‡ç®€å•çš„**ä¸Šä¸‹æ–‡æ‰°åŠ¨ç­–ç•¥SwapMix**ï¼Œå¯¹ä¸¤ä¸ªæœ‰ä»£è¡¨æ€§çš„VQAæ¨¡å‹çš„å¥å£®æ€§è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å‘ç°å®ƒä»¬è¿‡åº¦ä¾èµ–è§†è§‰ä¸Šä¸‹æ–‡ã€‚
  * å‘ç°ä¸€ä¸ªå®Œç¾çš„è§†è§‰æ¨¡å‹å¯¹è§†è§‰ä¸Šä¸‹æ–‡çš„ä¾èµ–è¦å°å¾—å¤šï¼Œä¸ºæ¨¡å‹æä¾›**å®Œç¾çš„è§†è§‰ç¼–ç **ï¼Œå¹¶è§‚å¯Ÿæ¨¡å‹ç¨³å¥æ€§çš„æ”¹å–„ã€‚
  * å®šä¹‰äº†**ä¸¤ä¸ªæŒ‡æ ‡**ï¼Œ**ä¸Šä¸‹æ–‡ä¾èµ–æ€§å’Œæœ‰æ•ˆå‡†ç¡®æ€§**ï¼Œå¹¶é€šè¿‡ä½¿ç”¨SwapMixä½œä¸ºæ•°æ®å¢å¼ºæŠ€æœ¯æ˜¾ç¤ºäº†æ”¹è¿›ã€‚

* æ–¹æ³•

  ![image-20221103155743460](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221103155743460.png)

  

**Weakly Supervised Relative Spatial Reasoning for Visual Question Answering**, in ICCV 2021. [[pdf](https://arxiv.org/abs/2109.01934)]

> [åšå®¢é“¾æ¥](https://www.cnblogs.com/lhiker/articles/15630482.html)

* åŠ¨æœº

  * è§†è§‰æ¨ç†çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯**ç©ºé—´ç†è§£**ï¼Œå®ƒæ¶‰åŠåˆ°ç†è§£å¯¹è±¡çš„ç›¸å¯¹ä½ç½®ï¼Œå³éšå¼åœ°å­¦ä¹ åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ã€‚
  * è¿‡å»çš„é¢„è®­ç»ƒå¤§æ¨¡å‹åœ¨æ¨ç†æ—¶å€™ï¼Œéƒ½æ— æ³•ç†è§£2Då›¾åƒå½“ä¸­çš„**ç©ºé—´ä¿¡æ¯**ã€‚
  * ä¸ºæ­¤è®¾è®¡äº†ä¸¤ä¸ªç›®æ ‡ä½œä¸ºç©ºé—´æ¨ç†ï¼ˆSRï¼‰çš„ä»£ç†
    * å¯¹è±¡è´¨å¿ƒä¼°è®¡
    * ç›¸å¯¹ä½ç½®ä¼°è®¡

  ![image-20221106145708208](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221106145708208.png)

* è´¡çŒ®

  * æå‡ºä¸¤ä¸ªå­ä»»åŠ¡ï¼Œç†è§£2Då›¾åƒå½“ä¸­çš„å‡ ä½•ä¿¡æ¯
  * å±•ç°äº†å¼ºå¤§çš„`zero-shot`èƒ½åŠ›ï¼Œåªéœ€è¦10%çš„è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒ
  * OODï¼ˆOut of Distributionï¼‰çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¾ˆå¼º

* æ–¹æ³•

  * é¢„å¤„ç†å·¥ä½œ

    * æŠ½å–å›¾åƒçš„æ·±åº¦ï¼ˆé¢„è®­ç»ƒå¥½çš„`AdaBins`æ¨¡å‹ï¼‰

      * è´¨å¿ƒè¡¨ç¤º$(x_c, y_c, z_c),$ $z_cä»£è¡¨æ·±åº¦$
      * ç›¸å¯¹ä½ç½®è¡¨ç¤º: è´¨å¿ƒä¹‹é—´å‘é‡çš„å‡æ³•ï¼ŒåŒæ—¶$dist(Aï¼ŒB)=âˆ’dist(Bï¼ŒA)$

    * åœ¨ä¸Šè¿°ä¸¤ä¸ªä»»åŠ¡ä¸­ï¼Œé¢„æµ‹éƒ½æ˜¯**å®å€¼å‘é‡**ã€‚è¯„ä¼°äº†è¿™äº›ä»»åŠ¡çš„ä¸¤ä¸ªå˜ä½“ï¼š

      * ä¸€ä¸ª**å›å½’ä»»åŠ¡**ï¼Œå…¶ä¸­æ¨¡å‹é¢„æµ‹$\mathbb{R}^{3}_{[-1, 1]}$ä¸­çš„å®å€¼å‘é‡
      * **binåˆ†ç±»**ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ä¸‰ä¸ªç»´åº¦çš„å®å€¼èŒƒå›´åˆ’åˆ†ä¸ºCä¸ªlog-scale binsã€‚ç¬¬cä¸ªbinçš„binå®½ç”±ä¸‹å¼ï¼ˆä½¿ç”¨è¶…å‚æ•°$\lambda=1.5$)ç»™å‡ºï¼š

      $$
      b_c=\frac{1}{\lambda^{C-\left|c-\frac{C}{2}\right|+1}}-\frac{1}{\lambda^{C-\left|c-\frac{C}{2}\right|+2}} \forall c \in\{0 . . C-1\}
      $$

      * å¯¹æ•°å°ºåº¦çš„binså¯¹æ›´è¿‘çš„è·ç¦»æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡ï¼ˆæ›´å¤šçš„binsï¼‰ï¼Œå¯¹æ›´è¿œçš„è·ç¦»æœ‰æ›´ä½çš„åˆ†è¾¨ç‡ï¼ˆæ›´å°‘çš„binsï¼‰
      * æœ€ç®€å•çš„binåˆ†ç±»å½¢å¼æ˜¯ä¸€ä¸ªå…·æœ‰biné—´éš”çš„ä¸‰ç±»åˆ†ç±»ä»»åŠ¡$[âˆ’1,0)ã€[0]ã€(0,1]$

  * å¼±ç›‘ç£ä»£ç†ä»»åŠ¡ $SR$

    * è´¨å¿ƒé¢„æµ‹: $\mathcal{L}_{S R-r e g}=\mathcal{L}_{M S E}\left(f_{r e g}(v), y_{r e g}\right)$
      * å°†è§†è§‰ä¿¡æ¯å‹ç¼©ä¸º$36 \times 3$æ¥å’Œ`gt`(é¢„å¤„ç†å¾—åˆ°çš„) è¿›è¡Œè®¡ç®—
    * ç›¸å¯¹ä½ç½®è¯„ä¼°: $\mathcal{L}_{S R \text {-bin }}=\mathcal{L}_{C E}\left(f_{\text {bin }}(V), y_{b i n}\right)$
      * è®­ç»ƒä¸€ä¸ªä¸¤å±‚å‰é¦ˆç½‘ç»œ$f_bin$æ¥é¢„æµ‹æ¯ä¸ªç»´åº¦ä¸Šæ¯ä¸ªå¯¹è±¡çš„$36\times C \times D$ä¸ª$bin$ç±»ï¼Œå…¶ä¸­$C$æ˜¯ç±»çš„æ•°é‡ï¼Œ$D$ä¸º3

  * Patchesè§†è§‰ä¿¡æ¯

    * èƒ½æ›´å¥½åˆ©ç”¨ç©ºé—´ä¿¡æ¯(è¿™é‡Œè¯´çš„æ˜¯å¹³é¢ç©ºé—´ä¿¡æ¯ï¼Œæœ‰ä½ç½®ç¼–ç )

    ![image-20221106152759382](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221106152759382.png)

:fire: :hammer_and_wrench: **An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2109.05014)] [[torch](https://github.com/microsoft/PICa)] [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/433110834)]

![image-20221108163040036](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221108163040036.png)

* æ¢ç´¢`PLM`å¯¹çŸ¥è¯†çš„ç†è§£èƒ½åŠ›åšVQAï¼Œä¸éœ€è¦æ ¹æ®çŸ¥è¯†åº“è¿›è¡Œæ£€ç´¢
* `few-shot`çš„å½¢å¼



:fire: :hammer_and_wrench: **A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge**, in ECCV 2022. [[home page](https://allenai.org/project/a-okvqa/home)]

> OK-VQAçš„å‡çº§ç‰ˆæœ¬

* åŠ¨æœº

  * è¿‡å¾€çš„æ•°æ®é›†é¡¶å¤šå°±æ˜¯æ£€ç´¢æ•°æ®åº“é‡Œé¢çš„çŸ¥è¯†ï¼Œæ²¡æœ‰åšåˆ°å¯¹å›¾ç‰‡å¸¸è¯†çš„æ¨ç†ï¼ˆæ›´åŠ ä¸°å¯Œçš„çŸ¥è¯†ï¼‰
  * æˆ‘ä»¬å®é™…ä¸Šéœ€è¦ï¼Œè¯†åˆ«å›¾ç‰‡ï¼Œç†è§£é—®é¢˜ï¼Œæ‰¾åˆ°çŸ¥è¯†ï¼Œ**æ¨ç†**å‡ºå¯¹åº”çš„ç­”æ¡ˆ
  * è¿‡å»çš„æ•°æ®é›†
    * FVQAï¼šç¼ºä¹æ¨ç†çš„è¿‡ç¨‹ï¼Œè€Œä¸”å’Œå›¾ç‰‡ä¸æ€ä¹ˆç›¸å…³
    * KVQAï¼šé€šå¸¸æ˜¯å®ä½“çš„çŸ¥è¯†ï¼Œåœ¨ç»´åŸºç™¾ç§‘ä¸Šé¢çš„çŸ¥è¯†ï¼Œè€Œä¸”ä¸»è¦æ˜¯é—®ä»»åŠ¡æ–¹é¢çš„ï¼Œæ²¡æœ‰å¸¸è¯†çš„çŸ¥è¯†
    * OK- VQAï¼šæœ‰biasï¼ŒçŸ¥è¯†å¤ªè¿‡ç®€å•ï¼ˆe.g., What is the capital of this country?ï¼‰, è€Œä¸”ç¼ºä¹æ¨ç†
    * VCRï¼šé‡ç‚¹æ˜¯ç”µå½±åœºæ™¯ä¸­äººçš„æ„å›¾

* æ•°æ®é›†çŸ¥è¯†ç±»å‹

  ![image-20221110165948489](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221110165948489.png)

  * **Commonsense** - learn from social behavior, æœ‰ç‚¹**æ¨ç†**çš„æ„æ€ (e.g., many donuts being made in a cart implies they are for sale rather than for personal consumption).
  * **Visual** - Knowledge of concepts represented visuallyï¼Œ å’Œ**å›¾ç‰‡ç›¸å…³** (e.g., muted color pallets are associated with the 1950s).
  * **Knowledge bases** - **æ£€ç´¢çŸ¥è¯†åº“çš„çŸ¥è¯†**, Knowledge obtained from textbooks, Wikipedia and other textual sources (e.g., hot dogs were invented in Austria).
  * **Physical** - **æ—¥å¸¸ç”Ÿæ´»çš„çŸ¥è¯†ï¼ŒFact**, Knowledge about the physics of the world (e.g., shaded areas have a lower temperature than other areas)

:hammer_and_wrench: **[VCR] Heterogeneous Graph Learning for Visual Commonsense Reasoning**, in NIPS 2019. [[pdf](https://arxiv.org/abs/1910.11475)] [[torch](https://github.com/yuweijiang/HGL-pytorch)]

* ä¸ä¼ ç»Ÿçš„`VQA`ä¸å¤ªä¸€æ ·ï¼ŒR: è§£é‡Šï¼ˆReasonï¼‰
  * ä¸‰ä¸ªå­ä»»åŠ¡åˆ†åˆ«æ˜¯: $Q \rightarrow A$, $QA \rightarrow R$, $Q \rightarrow AR$
* æ–¹æ³•ï¼šæ„å»ºå¼‚æ„å›¾

![image-20220921151513456](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921151513456.png)

![image-20220921151544188](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921151544188.png)



:hammer_and_wrench: **[VCR] Connective Cognition Network for Directional Visual Commonsense Reasoning**ï¼Œin NIPS 2019.  [[pdf](https://proceedings.neurips.cc/paper/2019/file/8a56257ea05c74018291954fc56fc448-Paper.pdf)] [[torch](https://github.com/AmingWu/CCN)]

* ä¸ä¸Šä¸€ç¯‡è®ºæ–‡æ€æƒ³æ¯”è¾ƒç±»ä¼¼ï¼Œå‚è€ƒç¥ç»ç§‘å­¦å½“ä¸­å°†ç¥ç»å…ƒæ•´åˆèµ·æ¥çš„æ€æƒ³

* åšæ³•

![image-20220921171105320](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171105320.png)

* æ¨¡å‹ç›¸å…³ç»†èŠ‚

![image-20220921171210400](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171210400.png)

* ç¬¬ä¸€partä¸­**è¿æ¥**çš„æ„å»º

![image-20220921171342266](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171342266.png)



:fire: **[Knowledge-Based] KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2012.11014)] [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/392431083)]

* åŠ¨æœº

  * éªŒè¯åˆ©ç”¨**å¤–éƒ¨çŸ¥è¯†+éšå¼çŸ¥è¯†**ç»“åˆçš„åš`QA`çš„èƒ½åŠ›

  * **éšå¼çŸ¥è¯†**å¯ä»¥ä»åŸºäºå¤§è§„æ¨¡è¯­æ–™é¢„è®­ç»ƒçš„æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ã€‚

  * è€Œ**æ˜¾ç¤ºçš„çŸ¥è¯†**å¯ä»¥ä»çŸ¥è¯†åº“ä¸­çš„æ˜ç¡®çš„ã€ç¬¦å·åŒ–çš„çŸ¥è¯†ä¸­å­¦ä¹ ã€‚

  * å°†ä¸¤ç§æ¨¡å‹è¿›è¡Œé›†æˆï¼Œå³å¯åŒæ—¶ç»“åˆéšå¼çŸ¥è¯†ä¸æ˜¾å¼çŸ¥è¯†è¿›è¡Œæ¨ç†ã€‚

    ![image-20221112091608964](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221112091608964.png)

* æ–¹æ³•

  * æ„å»ºå¤–éƒ¨çŸ¥è¯†åº“ï¼Œç”±äºåŸæ¥çš„çŸ¥è¯†å¤ªå¤šäº†ï¼Œæ ¹æ®è§„åˆ™å¯¹çŸ¥è¯†è¿›è¡Œä¸€å®šçš„ç­›é€‰

    * DBPedia, ConceptNet , VisualGenome and hasPart KB ï¼Œè¿›è¡ŒèŠ‚ç‚¹è¿‡æ»¤ï¼Œåªä¿ç•™åŒ…å«å›¾åƒç›®æ ‡æ£€æµ‹labelçš„èŠ‚ç‚¹ï¼Œæœ€åçš„çŸ¥è¯†å›¾åŒ…å«8000èŠ‚ç‚¹å’Œ36000æ¡è¾¹ï¼Œé‡‡ç”¨RGCNä½œä¸ºå·ç§¯æ¨¡å‹

      ![img](https://pic4.zhimg.com/80/v2-4299f57ff66a828d0cf5e5eb869e5f17_1440w.webp)

  * æ¨¡å‹è®¾è®¡ï¼ˆå¾ˆç®€å•ï¼‰

  ![æˆªå±2022-11-12 09.18.44](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-11-12%2009.18.44.png)





### :sunny: Textual

:fire:  :hammer_and_wrench: **[Question Answering] Commonsense for Generative Multi-Hop Question Answering Tasks**, in EMNLP 2018. [[pdf]](https://arxiv.org/abs/1809.06309) [[tensorflow]](https://github.com/yicheng-w/CommonSenseMultiHopQA)

:hammer_and_wrench: **[Dialogue System] Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering**, in AAAI 2020. [[pdf]](https://arxiv.org/abs/1912.07491) [[torch]](https://github.com/siat-nlp/TransDG)

**[Question Answering] Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs**, in EMNLP 2019. [[pdf\]](https://arxiv.org/abs/1910.08435)

:fire: :hammer_and_wrench: **[Question Answering] ** **Improving Multi-hop Question Answering over Knowledge Graphs usingKnowledge Base Embeddings**, in ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.412/)] [[torch](https://github.com/malllabiisc/EmbedKGQA)]

:hammer_and_wrench: **Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Urooj_Found_a_Reason_for_me_Weakly-supervised_Grounded_Visual_Question_Answering_CVPR_2021_paper.pdf)] [[torch](https://github.com/aurooj/ WeakGroundedVQA_Capsules.git)]

* ä¸ç”¨faster-rcnn
* è®­ç»ƒè¾“å…¥æ˜¯é—®é¢˜å’Œç­”æ¡ˆï¼Œè¾“å‡ºæ˜¯é¢„æµ‹ç­”æ¡ˆå¯¹åº”çš„**grouding area**ã€‚

**KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.422.pdf)] [[project](https://github.com/shijx12/ KQAPro_Baselines)]

* æ›´åŠ å¤æ‚çš„æ•°æ®é‡æ›´å¤§çš„å¼•å…¥çŸ¥è¯†çš„æ•°æ®é›†
  * å¹¶ä¸”ç»™å‡ºäº†ä¸¤ç§reasoningçš„è¿‡ç¨‹
  * å¯ä»¥åšQAå’Œ**è¯­ä¹‰è§£æ**æœåŠ¡
  * åˆ©ç”¨æ›´åŠ å¤æ‚çš„æ¨¡ç‰ˆå’ŒçŸ¥è¯†ç”Ÿæˆé—®é¢˜

![image-20220912160811327](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912160811327.png)

* [è¯¦ç»†ä»‹ç»çš„blog](https://blog.csdn.net/weixin_47903246/article/details/124649493)

**[è‡ªç„¶è¯­è¨€æ¨ç†] Generated Knowledge Prompting for Commonsense Reasoning**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.225.pdf)] [[torch](https://github.com/liujch1998/GKP)

> è¿™ç¯‡è®ºæ–‡çš„æ€æƒ³å’Œ`few-shot` COTå¾ˆåƒ 

* åŠ¨æœº

  * æ¢ç´¢LMå¯¹çŸ¥è¯†çš„ç†è§£èƒ½åŠ›æ¥å›ç­”é€»è¾‘æ¨ç†çš„é—®é¢˜
  * **ä¸å†éœ€è¦**å¤–éƒ¨çŸ¥è¯†åº“è¿›è¡Œæ£€ç´¢ï¼Œæˆ–è€…æ ¹æ®ç‰¹å®šä»»åŠ¡çš„ç›‘ç£ä¿¡å·è¿›è¡ŒçŸ¥è¯†çš„èåˆ

* æ–¹æ³•

  * è®©LMåœ¨few-shotæƒ…å†µä¸‹æ¥ç”Ÿæˆè§£é‡Šçš„`prompt`

    * é¦–å…ˆæ ¹æ®ä»»åŠ¡è§„å®šä¸€ä¸‹promptæ¨¡ç‰ˆï¼Œæ¯ä¸ªä»»åŠ¡äº”ä¸ªæ¨¡æ¿ï¼ˆfew-shotï¼‰

    ![image-20221106225100256](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221106225100256.png)

    * ç»§ç»­è¾“å…¥é—®é¢˜åˆ°é‚£ä¸ªå ä½ç¬¦å½“ä¸­ï¼ŒLMè‡ªåŠ¨ç”ŸæˆçŸ¥è¯†ï¼Œæ‰”å›å»ä½œä¸ºæ ·æœ¬é›†åˆ$K_q=\{k_1, k2, ..., k_M\}$

  * çŸ¥è¯†èåˆ

    * é—®é¢˜å’Œæ¯ä¸ªæ ·æœ¬é›†åˆä¸­çš„çŸ¥è¯†$k$è¿›è¡Œèåˆ, `concat`æ“ä½œ

    $$
    q_0=q, q_1=\left[k_1 \| q\right], \ldots, q_M=\left[k_M \| q\right]
    $$

  * æœ€åé€‰æ‹©æœ€åˆé€‚çš„å¯¹ï¼Œæ‰”è¿›å»ä¸‹ä¸€ä¸ªLMè¿›è¡Œæ¨ç†ï¼Œæœ€åç”Ÿæˆç­”æ¡ˆ

  

## :book: Paraphrase

---



:hammer_and_wrench: **[Sentence Discrimination] Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator**,in COLING 2018. [[pdf](https://aclanthology.org/C18-1230/)] [[torch](https://github.com/badripatro/PQG)]

:hammer_and_wrench: **[Hierarchical Sketch&Paraphrase Generation] Hierarchical Sketch Induction for Paraphrase Generation**, in ACL 2022.[[pdf](https://aclanthology.org/2022.acl-long.178.pdf)] [[torch](https://github.com/tomhosking/hrq-vae)]

### :whale2: Related Big Model

:fire: :hammer_and_wrench: **[Cross-Modal&Contrastive Learning] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning**, in ACL(long paper) 2021. [[pdf](https://aclanthology.org/2021.acl-long.202/)] [[project from Baidu](https://unimo-ptm.github.io/)]

:hammer_and_wrench: **[MultiModal] UniT: Multimodal Multitask Learning with a Unified Transformer**, ICCV 2021. [[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.pdf)] [[project from Fair](https://mmf.sh/)]

## :framed_picture: Image Caption

---



:white_check_mark: :hammer_and_wrench: **[Image Caption] Generating Diverse and Descriptive Image Captions Using Visual Paraphrases**, in ICCV 2019. [[pdf](https://ieeexplore.ieee.org/document/9010984)] [[torch](https://github.com/pkuliu/visual-paraphrases-captioning)]

* è¯¥è®ºæ–‡ç ”ç©¶äº†ç›®å‰å›¾åƒçš„æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å¤è¿°çš„ä¸¤é˜¶æ®µè§£ç çš„æ¨¡å‹ã€‚
  * ç»™å®šå›¾åƒè¾“å…¥ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç”Ÿæˆåˆæ­¥çš„å¥å­ï¼Œå†å°†å…¶æ”¹å†™ä¸ºå†…å®¹æ›´åŠ å¤šæ ·å’Œä¸°å¯Œçš„æè¿°ã€‚åœ¨MS COCOå›¾åƒæè¿°æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ã€‚

  * é‡ç‚¹æ¢ç´¢**visual paraphrases** è§’è‰² + **scoring function**
  
    * ```mermaid
      graph LR
      ä¸äººç±»ç›¸æ¯” --æ–‡ç« ä¸­æœ‰example--> ç¼ºå°‘å¤šæ ·æ€§å’Œå…·ä½“æ€§ --> ä¸¤é˜¶æ®µè§†è§‰å¤è¿°æ–¹æ³• --> MSCOCOæ•°æ®é›†
      ```
  
  
  * æ•…äº‹å±•å¼€:
  
    * ```mermaid
      graph LR
      æ ‡å‡† -->æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> å½¢å®¹è¯
      æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> ç»†èŠ‚,with
      å½¢å®¹è¯ --> Pa((Paraphrase))
      ç»†èŠ‚,with --> Pa
      Pa --> visual-paraphrase
      visual-paraphrase --> sentence_pairs --> ä¸¤é˜¶æ®µç¼–ç 
      ```
  
    * ```mermaid
      graph LR
      ç›¸å…³å·¥ä½œ --caption--> å¤šcaption.vs.å•caption --paraphrases--> æœªå¤„ç†ç‰¹å¾å’Œè§†è§‰ä¿¡æ¯ --ä¸¤é˜¶æ®µç¼–ç --> ä¸­é—´seq.vs.2captions 
      ```
  
  
  * æ¨¡å‹æ–¹æ³•ï¼š
  
    * ```mermaid
      graph LR
      é€‰æ‹©è§†è§‰å¤è¿°captionå¯¹ --> è¯„åˆ†å‡½æ•° --> è®¾è®¡ä¸‰ä¸ªAttentionæ“ä½œ,å­¦ä¹ åˆ°å¤šæ¨¡æ€çŸ¥è¯† --> æœ€åsoftmaxè¾“å‡º
      ```

* æ›´å¤šç»†èŠ‚å¯è§æˆ‘ä¸ªäººçš„[slide](https://kdocs.cn/l/conDzdschwAn)

:white_check_mark: ::fire: :hammer_and_wrench: **[Text Generation & Image Caption] Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions**, in CVPR 2019. [[pdf](https://openaccess.thecvf.com/content_CVPR_2019/html/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.html)] [[torch](https://github.com/aimagelab/show-control-and-tell)]

![](https://s2.loli.net/2022/04/09/COnvomETrl6GRf2.png)

![](https://s2.loli.net/2022/04/09/7ASmXcCazOh9GsU.png)

```mermaid
  graph LR
  å¤–éƒ¨ä¿¡å·æ§åˆ¶ --> å›¾åƒä¸­çš„ä¸€ç»„åŒºåŸŸå— --> core((æ ¸å¿ƒ))
  core --> æ”¹å˜chunkçš„é¡ºåº
  core --> æ”¹å˜å›¾åƒçš„åŒºåŸŸ
  model((æ¨¡å‹)) --åŸºäºåŒºåŸŸçš„ç‰¹å¾ä¸çŠ¶æ€--> LSTM((LanguageModel,ä¸¤å±‚LSTM)) --ç¬¬ä¸€å±‚--> è®¡ç®—attention --æ³¨æ„--> æ‰€æœ‰åŒºåŸŸçš„ç‰¹å¾å‘é‡è¿›è¡Œmean-poolingä½œä¸ºå›¾åƒçš„æ€»ä½“ç‰¹å¾I
  LSTM --ç¬¬äºŒå±‚--> é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯
  model --ä½•æ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå›¾åƒåŒºåŸŸ--> å—è½¬ç§»é—¨ --è®¡ç®—gt--> åŸºäºç¬¬ä¸€å±‚LSTMçš„çŠ¶æ€è®¾ç«‹ä¸€ä¸ªchunk-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sc_t-rtçš„attention
  model --è§†è§‰è¯oræ–‡æœ¬æ¬¡--> AdaptiveAttention --> è®¾ç½®ä¸€ä¸ªvisual-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sv_t-rtçš„attention --> attentionçš„ç»“æœ,å¯ä»¥è®¡ç®—å‡ºå½“å‰æ—¶åˆ»æ¨¡å‹æ­£åœ¨å…³æ³¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ct
  model --æ— åºé›†åˆæ’åº--> æ’åºç½‘ç»œ --> Rä¸­åŒ…å«Nä¸ªåŒºåŸŸé›† --å…¨è¿æ¥å±‚--> æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --Sinkhornç®—å­--> è½¯ç½®æ¢çŸ©é˜µ 
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --> æœ€å°åŒ–è½¯ç½®æ¢ä¸çœŸå®ç»“æœä¹‹é—´çš„å‡æ–¹è¯¯å·®
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --æµ‹è¯•åŒˆç‰™åˆ©ç®—æ³•è¿›è¡ŒåŒ¹é…--> è½¯ç½®æ¢çŸ©é˜µè½¬åŒ–ä¸ºæœ€ç»ˆçš„ç½®æ¢,ä»¥æ­¤æ¥å¯¹Rè¿›è¡Œæ’åº
```

 [è¯¦ç»†è®²è§£](https://zhuanlan.zhihu.com/p/150667499)

:hammer_and_wrench: **Length-Controllable Image Captioning**, in ECCV 2020 by [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q) and  Mingkui Tan.  [[pdf](https://arxiv.org/abs/2007.09580)] [[torch](https://github.com/bearcatt/LaBERT)]

* åŠ¨æœº
  * ä¸ºäº†è®©å¥å­æ›´åŠ ç²—ç•¥æˆ–è€…ç»†èŠ‚ï¼Œæå‡º**é•¿åº¦å¯æ§**çš„captionç”Ÿæˆ
  * è¿‡å»ç”±äºæ–¹æ³•æ˜¯è‡ªå›å½’çš„ï¼Œæ‰€ä»¥è®¡ç®—å¤æ‚åº¦ä¼šéšç€å¥å­é•¿åº¦ä¸Šå‡è€Œä¸Šå‡ã€‚ï¼ˆæ¨¡å‹ä¸Šçš„åˆ›æ–°ï¼‰

![image-20220831210727673](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831210727673.png)

ä¹‹å‰çš„SOTAæ–¹æ³•å¯èƒ½ä¼šé—æ¼ä¸€äº›å…³é”®çš„ä¿¡æ¯ï¼Œå¦‚æœæˆ‘æƒ³è¦æ›´åŠ ç»†èŠ‚ç‚¹çš„æè¿°ï¼Œä»–ä»¬æ— æ³•ç”Ÿæˆã€‚

* æ–¹æ³•

  > è¿‡å»ç”±äºæ–¹æ³•æ˜¯è‡ªå›å½’çš„ï¼Œæ‰€ä»¥è®¡ç®—å¤æ‚åº¦ä¼šéšç€å¥å­é•¿åº¦ä¸Šå‡è€Œä¸Šå‡ã€‚åœ¨è¿™é‡Œæå‡ºäº† non-autoregressiveçš„æ–¹æ³•ã€‚

  * è·å–å¥å­é•¿åº¦ä¿¡æ¯ï¼ˆlevel -> $[L_{low}, L_{high}]$ï¼‰åšembedding

  * æå‡ºDecode é˜¶æ®µ (non-autoregressive) **LaBERT**

    * ä½¿ç”¨ä½ç½®ä¿¡æ¯æ¥é¢„æµ‹mask

    * ä½¿ç”¨é•¿åº¦ä¿¡æ¯æ¥é¢„æµ‹unmask

    * æ¨ç†çš„æ—¶å€™é¼“åŠ±ç”Ÿæˆ**æ›´é•¿çš„å¥å­**

      * exponentially decay: $p_i\left(s_i=[\mathrm{EOS}]\right) \leftarrow \gamma^{L_{\text {high }}-i} p_i\left(s_i=[\mathrm{EOS}]\right), \forall i \in\left[L_{\text {low }}, L_{\text {high }}\right]$

        ![image-20220831212334860](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831212334860.png)

      * æ¯ä¸€æ­¥éƒ½ä¼šå¯¹æœ€ä½ç½®ä¿¡åº¦çš„å•è¯è¿›è¡Œmask

:hammer_and_wrench: **Human-like Controllable Image Captioning with Verb-specific Semantic Roles**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Human-Like_Controllable_Image_Captioning_With_Verb-Specific_Semantic_Roles_CVPR_2021_paper.pdf)] [[torch](https://github.com/mad-red/VSR-guided-CIC)]

* ä¸ä¸Šé¢ä¸¤ç¯‡å·¥ä½œå¯æ§æ€§çš„å¯¹æ¯”

![image-20220831213129731](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831213129731.png)

* åŠ¨æœº

  * äº‹ä»¶å…¼å®¹æ€§ï¼Œä¸¤ä¸ªä¸å…¼å®¹çš„äº‹ä»¶ä¸åº”è¯¥åˆåœ¨ä¸€èµ·

  * é‡‡æ ·çš„å…¼å®¹æ€§ï¼Œä¸åˆç†çš„é‡‡æ ·ä¸åº”è¯¥å‡ºç°åœ¨å¥å­å½“ä¸­

  * å¯¹äºä¸Šé¢çš„caseï¼š 

    ```python
    verb=sit, Arg1="thing sitting", Arg2="sitting position" 
    verb=read, Arg0="reader", Arg1="thing read"
    ```

* æ–¹æ³•ä¸Šæ˜¯å…ˆæŠ½å–å‡ºæ¥çº¦æŸçš„æ ‡ç­¾ï¼Œå†decoder

![image-20220831213747447](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831213747447.png)

:star: **MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning**, in AAAI 2022.  [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/20243)]

* åŠ¨æœº
  * ä¸ºäº†captionçš„ç”Ÿæˆæ›´åŠ ä¸°å¯Œå¤šæ ·ï¼Œå¹¶ä¸”**æ— éœ€è¿‡å¤šçš„æ ‡æ³¨æ•°æ®**ï¼
  * captionç›´æ¥åšåˆ°**åœºæ™¯æ–‡æœ¬**çº§åˆ«
* æ–¹æ³•
  * Unpired Captioningçš„æ–¹æ³•ï¼ˆå…¶å®å°±æ˜¯`GAN`çš„æ€æƒ³ï¼‰
  * å­¦åˆ°äº†æ¨¡æ€å†…éƒ¨ï¼Œè·¨æ¨¡æ€ä¹‹é—´çš„å…³ç³»
  * Unpaired å­¦ä¹ çš„èŒƒå¼ï¼Œæ— éœ€è¿‡å¤šç›‘ç£ä¿¡å·

![image-20220904163643769](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220904163643769.png)





:hammer_and_wrench: :fire: **Show, Edit and Tell: A Framework for Editing Image Captions**, in CVPR 2020.  [[pdf](https://arxiv.org/abs/2003.03107)] [[torch](https://github.com/fawazsammani/show-edit-tell)]

* ç›´æ¥å¯¹ç”Ÿæˆçš„captionè¿›è¡Œç¼–è¾‘ä¿®æ”¹

:hammer_and_wrench: **Towards Accurate Text-based Image Captioning with Content Diversity Exploration**, in CVPR 2021. [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Towards_Accurate_Text-Based_Image_Captioning_With_Content_Diversity_Exploration_CVPR_2021_paper.pdf)]  [[torch](https://github.com/guanghuixu/AnchorCaptioner)]

* åŠ¨æœº
  * Captionç”Ÿæˆçš„å¤šæ ·æ€§
  * æŒ‘æˆ˜
    * ä¸çŸ¥é“åº”è¯¥å¦‚ä½•é€‰æ‹©æ–‡æœ¬ä¿¡æ¯
    * æ–‡æœ¬å’Œå›¾ç‰‡ä¹‹é—´çš„å…³ç³»
    * å¤šæ ·æ€§captionçš„ç”Ÿæˆ

![image-20220831221056898](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831221056898.png)



* æ¨¡å‹æ–¹æ³•

![image-20220831221253743](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831221253743.png)



**Improving OCR-based Image Captioning by Incorporating Geometrical Relationship**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf)]

* åŠ¨æœº
  * æ— æ³•å»ºç«‹OCRæŠ½å‡ºæ¥ä¸œè¥¿ä¹‹é—´çš„å…³ç³»
* æ–¹æ³•
  * é€šè¿‡é«˜åº¦ï¼Œå®½åº¦ã€è·ç¦»ã€IoUå’Œæ–¹å‘æ„å»ºç›¸åº”çš„OCR

![image-20220831223252058](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831223252058.png)



 :hammer_and_wrench: **Towards Unique and Informative Captioning of Images**, in ECCV 2020.  [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-58571-6_37.pdf)] [[torch](https://github.com/princetonvisualai/SPICE-U)]

* ç›®å‰é—®é¢˜ï¼š

![image-20220901103845451](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901103845451.png)

* å…³é”®è´¡çŒ®ï¼Œåšäº†ä¸€ä¸ª**æ–°çš„è¯„ä»·æŒ‡æ ‡**

:hammer_and_wrench: **Comprehensive Image Captioning via Scene Graph Decomposition**, in ECCV 2020.  [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-58568-6_13.pdf)] [[torch](https://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html)]

* åœºæ™¯å›¾åˆ†è§£æ¥å®ç°å¤šæ ·æ€§

![image-20220901104357419](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104357419.png)

![image-20220901104406842](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104406842.png)

:hammer_and_wrench: **In Defense of Scene Graphs for Image Captioning**, in ICCV 2021.  [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9710596)]  [[torch](https://github.com/ Kien085/SG2Caps)]

* åŠ¨æœº
  * å¼¥è¡¥æ–‡æœ¬åœºæ™¯å›¾è¿˜æœ‰è§†è§‰åœºæ™¯å›¾ç›´æ¥çš„Gap
  * ä»¥å¾€çš„å·¥ä½œåœ¨è®­ç»ƒcaptioneræ—¶ï¼Œå¾€å¾€ç”¨**TSGä½œä¸ºè¾“å…¥**ï¼Œæµ‹è¯•æ—¶å†æ¢æˆVSG
  * VGæ•°æ®é›†ä¸Šå­¦å¾—çš„åœºæ™¯å›¾ä¸­relationshipå¤šæ˜¯has, onè¿™ç±»**æ— æ„ä¹‰çš„å…³ç³»**
  * VSGä¸TSGå¹¶ä¸å…¼å®¹  ï¼ˆä¸¤ä¸ªåœºæ™¯å›¾ä¹‹é—´ï¼‰

![image-20220831224945278](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831224945278.png)

* åŸºæœ¬æ€æƒ³
  *  close the **semantic gap** between the two scene graphs
  * ä½¿ç”¨**HOIä¿¡æ¯å¢å¼ºVSG**ï¼Œå¹¶å¼•å…¥object locationä¿¡æ¯æå‡VSGçš„è¡¨è¾¾èƒ½åŠ›

![image-20220831224458911](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831224458911.png)

* å…·ä½“æ–¹æ³•

  * VSGæ„å»º
    * VGæ•°æ®é›†è®­ç»ƒä¸€ä¸ªVSG generatorï¼ŒåŒä»¥å¾€å·¥ä½œä¸€æ ·å¯¹MSCOCOä¸­çš„å›¾ç‰‡ç”ŸæˆVSGã€‚ä¸æ­¤åŒæ—¶ï¼Œä½œè€…åˆåœ¨MSCOCOä¸Šè®­ç»ƒäº†ä¸€ä¸ªobject detectorï¼Œå¯¹å›¾ç‰‡æ£€æµ‹å‡ºä¸€ç³»åˆ—çš„ç‰©ä½“ã€‚
  * VSGç¼–ç 
    * éšåä½¿ç”¨HOI inferenceå¯¹ä¸äººç›¸å…³çš„ç‰©ä½“è¿›è¡Œå…³ç³»åŠå±æ€§çš„é¢„æµ‹ã€‚æœ€åå–åŸå§‹VSGä¸HOI (æ£€æµ‹åˆ°çš„ç‰©ä½“) graphçš„å¹¶é›†ä½œä¸ºæœ€ç»ˆVSGã€‚
    * ä½¿ç”¨å¤šä¸ªGCNå¯¹å…¶è¿›è¡Œç¼–ç ï¼Œä¸åŒç±»å‹çš„èŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„GCNå‚æ•°ã€‚
  * decodeé˜¶æ®µ (Up-down)
    * ä»…ä»…ä½¿ç”¨scene graphï¼Œä¸ä½¿ç”¨ä»»ä½•è§†è§‰ç‰¹å¾ï¼ŒSG2Capsæ¨¡å‹ä¾¿å¯ä»¥å–å¾—æœ‰ç«äº‰åŠ›çš„æè¿°ç”Ÿæˆç»“æœã€‚
  * caseå±•ç¤º

  ![image-20220831230157943](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831230157943.png)



:hammer_and_wrench: **Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning**, in CVPR 2022. [[pdf]()] [[torch](https://github.com/GT-RIPL/Xmodal-Ctx)]

* å…³æ³¨åˆ°æ›´å¤šçº§åˆ«çš„ä¿¡æ¯

![image-20220901105828069](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901105828069.png)

* æ–¹æ³•ä¸Šä¸»è¦åŠ å…¥äº†Crop

![image-20220901105902348](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901105902348.png)

:hammer_and_wrench: **Comprehending and Ordering Semantics for Image Captioning**, in CVPR. [[pdf](https://arxiv.org/pdf/2206.06930.pdf)] [[torch](https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet)]

* **è¯­ä¹‰çš„è¯­è¨€æ’åº**ï¼ˆä¸å•å•æ˜¯å¯¹è±¡ï¼‰åŒæ ·å¾ˆé‡è¦

![image-20220901111757395](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111757395.png)

* æ–¹æ³•

![image-20220901111817536](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111817536.png)

:hammer_and_wrench: **DIFNet: Boosting Visual Information Flow for Image Captioning**, in CVPR 2022.  [[pdf](DIFNet: Boosting Visual Information Flow for Image Captioning)] [[torch](https://github.com/mrwu-mac/DIFNet)]

* è€ƒè™‘äº†ä¿¡æ¯æµçš„ä¿¡æ¯

![image-20220901111002940](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111002940.png)

![image-20220901111014355](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111014355.png)

:hammer_and_wrench: **Injecting Semantic Concepts into End-to-End Image Captioning**, in CVPR 2022.  [[pdf](https://arxiv.org/abs/2112.05230)]  [[torch](https://github.com/jacobswan1/ViTCAP)]

* ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œdetector-free å’ŒåŠ å…¥è¯­ä¹‰concept
* è¿‡å»çš„å·¥ä½œ

![image-20220901104826507](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104826507.png)

* åŠ å…¥Concept
  * é€šè¿‡æŠ½å–captionä¸­çš„åŠ¨åè¯æˆ–è€…é€šè¿‡çŸ¥è¯†è’¸é¦å¾—åˆ°ä¸€äº›conceptä½œä¸º**ä¼ªæ ‡ç­¾**åšåˆ†ç±»

![image-20220901104847149](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104847149.png)



:hammer_and_wrench: **Show, Deconfound and Tell: Image Captioning with Causal Inference**, in CVPR 2022.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Show_Deconfound_and_Tell_Image_Captioning_With_Causal_Inference_CVPR_2022_paper.pdf)] [[torch](https: //github.com/CUMTGG/CIIC)]

* è§£å†³æ•°æ®é›†ä¸­å¤§é‡å‡ºç°äº†ï¼Œæ¨¡å‹**short-cut path** çš„é—®é¢˜

![image-20220901110327977](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901110327977.png)

* ä¸»è¦ä¸ºäº†è§£å†³ä¸¤ä¸ªCaptionå­˜åœ¨çš„é—®é¢˜
  * è¯†åˆ«**å¯¹è±¡é”™è¯¯**ï¼ˆé•¿å¤´å‘çš„ç”·äººè¯†åˆ«æˆäº†å¥³äººï¼‰
  * æè¿°å¾—**ä¸å¤Ÿå…³é”®å’Œè¯¦ç»†**

* Encoderé˜¶æ®µï¼ˆè§£å†³åˆ†ç±»å‡†ç¡®æ€§çš„é—®é¢˜ï¼‰
  * åŸºäºFaster-RCNNå¾—åˆ°æ— åçš„ç‰©ä½“åˆ†ç±»

![image-20220901110456322](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901110456322.png)

![image-20220915113730411](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915113730411.png)

![image-20220915113654808](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915113654808.png)

* decoderé˜¶æ®µè€ƒè™‘ç”Ÿæˆå•è¯çš„bias

![image-20220915114452498](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915114452498.png)



![image-20220915114520789](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915114520789.png)



**[å› æœå…³ç³» + å¼ºåŒ–å­¦ä¹ ] Dependent Multi-Task Learning with Causal Intervention for Image Captioning**, in IJCAI 2021.  [[pdf](https://www.ijcai.org/proceedings/2021/0312.pdf)] 

> è¯´å®è¯è¿™ç¯‡è®ºæ–‡å†™ä½œ**æœ‰ç‚¹å¤ªå¤æ‚äº†ï¼Œå¾ˆéš¾çœ‹æ‡‚**

* è§£å†³captionç”Ÿæˆ**åäº‹å®**ä¸**ä¸å¤Ÿè¯¦ç»†**çš„é—®é¢˜

![image-20220914112625963](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220914112625963.png)

* **å› æœå¹²é¢„åˆ†æè¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œè¯¦è§è®ºæ–‡**

![image-20220914112637246](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220914112637246.png)

**NOC-REK: Novel Object Captioning with Retrieved Vocabulary from External Knowledge**, in CVPR 2022. [[pdf](https://arxiv.org/pdf/2203.14499.pdf)]

* ä»»åŠ¡æè¿°

  * æ–°ç‰©ä½“æè¿°(novel object captioning)ï¼Œå³è®©æ¨¡å‹æè¿°å‡ºè®­ç»ƒ**æè¿°è¯­æ–™ä¸­æ²¡æœ‰å‡ºç°è¿‡çš„ç‰©ä½“**ã€‚
  * å¦‚å›¾ï¼Œä¼ ç»Ÿçš„captionè®­ç»ƒå¦‚å·¦ä¸‹æ¡†æ‰€ç¤ºï¼Œè®­ç»ƒè¯­æ–™é‡Œæ²¡æœ‰rampè¿™ä¸ªè¯ï¼Œæµ‹è¯•æ—¶è‡ªç„¶ä¹Ÿä¸ä¼šç”Ÿæˆå«æœ‰**ramp**çš„å¥å­ã€‚

  ![img](https://raw.githubusercontent.com/Gary-code/pic/main/img/v2-f66020b9a4dad5f7a4ef1108c57468f5_720w.webp)

  * åœ¨å®é™…åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¾€å¾€ä¼šè§åˆ°åœ¨è®­ç»ƒæ•°æ®ä¸­æ²¡è§è¿‡çš„æ–°ç‰©ä½“ï¼Œæ­¤æ—¶ä¼ ç»Ÿçš„æ–¹æ³•ä¸èƒ½åšåˆ°å¯¹æ–°ç‰©ä½“ç”Ÿæˆæè¿°ï¼Œä¸ç¬¦åˆæˆ‘ä»¬å¯¹æ¨¡å‹åº”ç”¨çš„æœŸå¾…ã€‚

* å½“å‰çš„æ–¹æ³•

  * **é‡‡é›†æ›´å¤šçš„æ•°æ®**ï¼Œè®©è®­ç»ƒè¯­æ–™åŒ…å«å°½å¯èƒ½å¤šçš„ç‰©ä½“å¹¶é‡æ–°è®­ç»ƒã€‚ç„¶è€Œæ•°æ®çš„é‡‡é›†å’Œæ ‡æ³¨æ˜¯éº»çƒ¦ä¸”æ˜‚è´µçš„ï¼Œå¹¶ä¸”æ¯æ¬¡åŠ å…¥æ–°ç‰©ä½“ï¼Œæ¨¡å‹éœ€éƒ½éœ€è¦**é‡æ–°è®­ç»ƒ**ï¼Œå¾ˆä¸ä¼˜é›…ã€‚
  * å¦‚å…ˆå¯¹å›¾ç‰‡åšç‰©ä½“æ£€æµ‹ï¼Œè·å¾—ROIå’Œobject tagsï¼Œå†ç»“åˆå…¶è®­ç»ƒcaptionæ¨¡å‹ã€‚ï¼ˆå½“å‰**ä¸»æµæ–¹æ³•**ï¼‰
  * ç„¶è€Œï¼Œæ‰€ç”¨ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„èƒ½åŠ›ä¹Ÿæ˜¯æœ‰é™çš„ï¼Œè§åˆ°çš„**æ–°ç‰©ä½“ææœ‰å¯èƒ½è¶…å‡ºæ£€æµ‹æ¨¡å‹çš„èƒ½åŠ›èŒƒå›´**ã€‚

* åŠ¨æœº

  * äººç±»è®¤çŸ¥ç‰©ä½“æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€æ˜¯é **å¤–è§‚çš„åŒ¹é…**ï¼Œè§è¿‡å³è®¤è¯†ï¼›äºŒæ˜¯é å®šä¹‰ï¼Œå“ªæ€•æ²¡è§è¿‡ä¸€ä¸ªç‰©ä½“ï¼Œ**å‡­å€Ÿç‰©ä½“çš„å®šä¹‰**ï¼Œæˆ‘ä»¬å¾€å¾€ä¹Ÿèƒ½ç†è§£è¯†åˆ«ã€‚
  * æœ¬æ–‡è®¾è®¡äº†ä¸€ä¸ªè¯è¯­æ£€ç´¢æ¨¡å—ï¼Œå°†**==æ–°ç‰©ä½“å®šä¹‰ä½œä¸ºå¤–éƒ¨çŸ¥è¯†==**å¼•å…¥captionæ¨¡å‹ï¼Œä¸captionæ¨¡å—ä¸€èµ·ç«¯åˆ°ç«¯åœ°è®­ç»ƒã€‚

* æ–¹æ³•

  ![image-20221005102539630](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221005102539630.png)

  * åŒ¹é…`loss`çš„è®¡ç®—

    * ä¸ºäº†é¼“åŠ±æ¨¡å‹å¼•å…¥æ–°çš„ç±»ï¼Œæˆ‘ä»¬å°†paddingï¼ˆå’ŒRegion featureæ•°é‡ä¸€è‡´ï¼‰çš„ç©ºç±»çš„15%éšæœºæ›¿æ¢æˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­éšæœºæŒ‘é€‰çš„è¯ä½œä¸ºGT

    * è®¡ç®—$Hungarian$ loss (åŒˆç‰™åˆ©æŸå¤±)
      $$
      \mathcal{L}_{\mathrm{H}}(\mathcal{Y}, \mathcal{V})=\sum_{i=1}^K-\log \operatorname{sim}\left(\mathbf{y}_i, \hat{\mathbf{v}}_{\hat{\sigma}(i)}\right)
      $$
      

**Image Captioning with Novel Topics Guidance and Retrieval-based Topics Re-weighting**, in TMM 2022. [[pdf](https://ieeexplore.ieee.org/document/9869686)]

* åˆ©ç”¨ä¸»é¢˜(**Topic**)æ¨¡å‹æ¥åšimage caption

  * æ¢ç´¢å›¾ç‰‡å¯¹è±¡å’ŒTopicä¹‹é—´çš„å…³ç³»

* ä»€ä¹ˆæ˜¯ä¸»é¢˜ [é“¾æ¥](https://zhuanlan.zhihu.com/p/41683009)

  * ä¸‰ä¸ªlistï¼šã€é¸¡èƒ¸è‚‰ï¼Œè›‹ç™½ç²‰ï¼Œé¥®é£Ÿæ§åˆ¶ã€‘ã€ã€è·‘æ­¥æœºï¼Œæ¤­åœ†æœºï¼Œé¾™é—¨æ¶ã€‘ã€ã€å‡è‚¥ï¼Œä½“é‡ä¸‹é™ï¼Œç²¾ç¥çŠ¶æ€ã€‘ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬ä¹Ÿèƒ½å¤§æ¦‚çŸ¥é“æ¯ä¸ªè¯èƒŒåçš„ä¸»é¢˜æ˜¯ä»€ä¹ˆäº†ã€‚
  * æˆ‘ä»¬å¯ä»¥å°†Topic Model çœ‹å¾…ä¸ºä¸€ä¸ª**Clusteré—®é¢˜**ã€‚è€Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å°†ä¸€äº›ç‰¹å¾æ˜æ˜¾çš„è¯æŠ“å–å‡ºæ¥ã€‚

* æ–¹æ³•

  ![æˆªå±2022-10-08 19.14.43](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-08%2019.14.43.png)

  * Topicæ˜¯é€šè¿‡`NMF`(NLTKä¸­æœ‰)æ¥ä»captionä¸­æå‰è·å–çš„ï¼Œæ•°é‡ä¸º200ã€‚
  * RTRæ˜¯æ¨ç†æ—¶å€™æ‰ä½¿ç”¨çš„ï¼Œä¸ºäº†è®©**ä¸»é¢˜å’Œå›¾åƒä¿¡æ¯æ›´åŠ ç›¸å…³**
  * ETPå­¦ä¹ å¯¹è±¡å’Œä¸»é¢˜ä¹‹é—´çš„ç›¸ä¼¼åº¦
  * STPæ¨¡å—åµŒå…¥åœ¨æ¯ä¸ªLSTMCellä¸Šé¢ï¼Œä¸ºäº†åœ¨å¯¹åº”çš„æ—¶é—´æ­¥é€‰æ‹©åˆé€‚çš„ä¸»é¢˜è¿›è¡Œcaption



**[ä¸»é¢˜ + Image Caption] Show, Rethink, And Tell: Image Caption Generation With Hierarchical Topic Cues**, in ICME 2021. [[pdf](https://ieeexplore.ieee.org/document/9428353)]

* åŠ¨æœºï¼ˆcaptionæœ¬èº«å°±å…·å¤‡å¤šä¸ªæˆ–è€…ä¸€ä¸ªä¸»é¢˜çš„ç‰¹å¾ï¼‰ã€ç›®å‰è§‰å¾—topicçš„ä½œç”¨å°±æ˜¯ç”Ÿæˆæ›´åŠ å¤šè¯­ä¹‰ä¿¡æ¯+ç”Ÿæˆæ›´åŠ ç²¾ç¡®ã€‘

  * ä¹‹å‰æ–¹æ³•æ²¡æœ‰å»ºç«‹ä¸åŒç±»å‹å›¾ç‰‡ç‰¹å¾çš„è”ç³»ï¼Œcaptionä¸å¤Ÿå‡†ç¡®
  * decoderæ—¶å€™æ³¨æ„åŠ›æœºåˆ¶å¯¹äºæ¯ä¸ªå•è¯åªä¼šæ“ä½œä¸€æ¬¡

* æ–¹æ³•

  * æ ¹æ®Faster-RCNNç‰¹å¾æ„å»ºå‡ºä¸»é¢˜->å…³é”®è¯çº§åˆ«çš„ç‰¹å¾ï¼ˆæ¯ä¸ªæ£€æµ‹åŒºåŸŸéƒ½ä¼šæ£€ç´¢å‡ºå¯¹åº”çš„ä¸»é¢˜ï¼‰
  * decoderæ—¶å€™åŠ å…¥å¯¹ä¸»é¢˜çš„attentionæ“ä½œï¼ˆå¯ä»¥ç”Ÿæˆä¸€äº›ä¸å¸¸ç”¨çš„æè¿°ï¼Œå¦‚little boyä¸­çš„littleï¼‰

  ![æˆªå±2022-10-17 16.08.26](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-17%2016.08.26.png)

:hammer_and_wrench: **Explicit Image Caption Editing**, in ECCV 2022. [[pdf](https://arxiv.org/pdf/2207.09625.pdf)] [[torch](https://github.com/baaaad/ECE)]

* ä»»åŠ¡ï¼šImage Captionç¼–è¾‘ï¼Œä¿®æ”¹å¾—æ›´åŠ åˆç†

  * è¿‡å»çš„æ–¹æ³•éƒ½æ˜¯éšå¼çš„ä¿®æ”¹ï¼Œç¼ºå°‘å¯è§£é‡Šæ€§ï¼Œæ›´åƒæ˜¯re-writingçš„ä»»åŠ¡ï¼Œå¥å­çš„ç»“æ„ä¹Ÿå®¹æ˜“è¢«ç ´å

  ![image-20221012105504301](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012105504301.png)

* æ•°æ®é›†æ„å»º

  * Ref-Capå’ŒGT-Capéƒ½éœ€è¦æ˜¯äººå·¥æ ‡è®°çš„ï¼ˆä»¥å‰çš„æ–¹æ³•Ref-Capæ˜¯æ¨¡å‹ç”Ÿæˆçš„ï¼‰
  * COCOæ•°æ®é›†
    * æ¯å¼ å›¾çš„5ä¸ªcapéƒ½ä½œä¸ºGTï¼Œç„¶åæ ¹æ®é€šè¿‡å›¾åƒå’Œcapè®¡ç®—å’Œå…¶ä»–ï¼ˆä¸æ˜¯è¿™å¼ å›¾ï¼‰captionç›¸ä¼¼åº¦è®¡ç®—ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„
  * e-SNLI-VEï¼ˆFlicker30Kï¼‰æ•°æ®é›†
    * contradictionå’Œentailmentåˆ†åˆ«ä½œä¸ºRefå’ŒGTå³å¯

* æ–¹æ³• (æ¨¡å‹æ˜¯åŸºäº**BERT**)

  * å¼•å…¥ç¼–è¾‘ç®—å­`<ADD>, <DELETE>, <KEEP>`

    ![image-20221012110107268](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012110107268.png)

    ![](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012110107268.png)

* Case Study

![image-20221012110327880](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221012110327880.png)

:hammer_and_wrench: **Object-Centric Unsupervised Image Captioning**, in ECCV 2022.  [[pdf](https://arxiv.org/abs/2112.00969)] [[torch](https://github.com/zihangm/obj-centric-unsup-caption)]

* ä¸»è¦åŠ¨æœº
  * äººå·¥æ ‡æ³¨çš„image caption annotationå¤ªæµªè´¹æ—¶é—´äº†

* è´¡çŒ®
  * **æ— ç›‘ç£**çš„Image Caption + **å¤šè¯­è¨€**

* å¦‚ä½•æ— ç›‘ç£

  * ä½¿ç”¨ç½‘ç»œçˆ¬ä¸‹æ¥çš„è¯­æ–™åº“ï¼Œè¿˜æœ‰æ•°æ®é›†ä¸­å›ºæœ‰çš„å›¾åƒä¿¡æ¯æ¥åš
  * ä½†`BLEU4`å¾ˆä½ï¼Œåªæœ‰6å·¦å³

  ![image-20221105204258603](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221105204258603.png)

:hammer_and_wrench: **Human-Centric Image Captioning**, in PR 2022. [[pdf](https://pdf.sciencedirectassets.com/272206/1-s2.0-S0031320322X00034/1-s2.0-S0031320322000267/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGoaCXVzLWVhc3QtMSJIMEYCIQDGI9Pe2Vg29Uf127g8bgoLH2in5NivB%2FYNbpJFB7t6pwIhAIJI6BJ3FO8IVZf7anhzx1JlrQnoA%2BvNPnTnFXynNyuLKswECEMQBRoMMDU5MDAzNTQ2ODY1IgyuR0sHCTJ7ZSggWKQqqQRnRT7Gv%2F7BVa9ChvOv%2Bv5L7LGF8GiKDTAVH%2FlK624uvf7X%2B8Rb0sn9tn7aGPgMwDLI8wG3Xu0uZKtLTtWkTE8zzNxzxDCTkNqLc7ePo3gd5kKdrjOCgXRyPF3K%2FKu%2FHiy5b%2FkKhvaXoSIv49WpU4Mw3jduHux0yg74CmV3I087VDVR6y0Z%2Fn1MGartEpeGdbwBoy%2F0Z59JHQF7rjquT4cG5isYwRK0nnr%2B79L%2BtWkL6uIaFyYk%2Bi%2B9GtknfOyGpuOQ4aF%2FVoFmdAzCsqwJAGjqS30nAaK7SiiiFtIoYpRPT5woODRU8SJ68UI4nHt0Es%2FEQJWTF59qx%2Fpl8SlN2sI%2FDIfN4aBDgwtF7ZPC3vgpwmcxBg67LjXS5Lwsha0Ui1PlDfo4EyrzzBHnE2wimBvn%2BU5L4xokqI246LjOALNUEUH8fPRm3ZnN9b2IWKqDFkJWwQ76T7FN2hnGPu%2F3JYf%2FX6mOJXaSVsumevPlqTn4oE1gQHZ%2FmP8uNc2feI36D4y17pFycY3HBZp5COJ3XlrSmGIBTFQFmXwt7JxQ14peTqI1GFR%2FTNZqTP9Uv6UkfQCdhYkK7dkemhKJpKtALjJz%2F0J1RIwaetYxcSMo%2F4kQDowiRJpQY7Y6OsRqgz7aqxCxPp9PNVIR64T9p%2Bj4VJdzsLs7B6%2BDWHoq0Sz99UQhVWupnVDXjH%2F8HLIaWQq5l6soAeVRmG8Z8t28pXM1pzgaoPzY%2Fdvl36r0MNqIv5oGOqgB8Mx5pBXQYDZWHBBnVJkdllGOlXSy8W7DgEygt929CiDsF%2F%2Fz4aCPqCmaN8WERin7vVZgIAwGD8dyHFgfCKJ0wY8%2BxnqhtHVeRq4FHknBpQJlBc50wlk9Jd4H4SIl%2FqASs5J7ssdt7UqhLhim%2FEosHKxTPOhVlHkd7HlgjOYebUvlAnxsOuqyy9LoMtdnpRwnCaCzqdOahqTFZ0bY1KoGgjLvfX306cS0&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20221019T105552Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYV2BRT6MM%2F20221019%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=cdb598601cea787f991b427a00715b9977c06eb5ca33f744d087db73f1e3f9d3&hash=4a2e7e1603daf30647f492f3181b0a0e2260dc043d979436c01ef5d645e16465&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0031320322000267&tid=spdf-176b5542-edc3-49a9-b2fd-3d5bf73304c5&sid=19abb8f59650524d941be66636a7de94bc37gxrqb&type=client&ua=51570d5e09030508575a&rr=75c8fbdf599b8b41)] [[code & dataset](https://github.com/JohnDreamer/HCCM/)]

* åŠ¨æœºã€åŠ¨æœºä¸æ˜¯å¾ˆå¼ºï¼Œè¯´å¾—ä¸å¤Ÿæœ‰é“ç†ã€‘

  * å½“å‰æ–¹æ³•ç¼ºä¹å¯¹äººç±»è¡Œä¸ºåŠ¨ä½œçš„æè¿°ï¼Œåªæ˜¯ç®€å•çš„æ¢ç´¢å‰æ™¯ç‰©ä½“
  * æ— è§†äº†è¿™ä¸ªå…³é”®ç‰©ä½“çš„ä¸€äº›ç»†èŠ‚è¿˜æœ‰å’Œå…¶ä»–ç‰©ä½“ä¹‹é—´çš„å…³ç³»

* æ–¹æ³•

  * æ ‡æ³¨**æ•°æ®é›†**ï¼Œå·²å…¬å¼€

    * äººå·¥æ ‡æ³¨äº†äººçš„èº«ä½“éƒ¨ä½çš„boxes

      ![æˆªå±2022-10-20 16.08.38](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-20%2016.08.38.png)

    * èƒŒæ™¯ç‰©ä½“å’Œæ´»åŠ¨ç‰©ä½“çš„ç‰¹å¾åŒºåˆ†æ˜¯é€šè¿‡ç®—å’Œäººä¹‹é—´çš„similarityæ¥å¾—åˆ°çš„

:hammer_and_wrench: **Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning**, in IJCV 2022. [[pdf](https://arxiv.org/abs/2105.04143)] [[torch](https://github.com/dandanguo1993/vtcm-based-image-paragraph-caption)]

> å¼•å…¥Topicæ¥åšparagraph Captionçš„ç”Ÿæˆ

* åŠ¨æœº

  * é™ä½ç”ŸæˆCaptionçš„éš¾åº¦
  * ä»¥å¾€çš„Topicæ¨¡å‹åšCaptionå­˜åœ¨å†—ä½™ä¿¡æ¯
  * åªæœ‰å•å±‚Topicä¸å¤Ÿä¸°å¯Œ
  * Captionçš„å¤šæ ·æ€§ï¼Œå¤šå±‚Topicçš„Keywordæ˜¯ç”¨æˆ·æƒ³è¦å¼ºè°ƒçš„ä¸œè¥¿

* æ–¹æ³•

  * ä½¿ç”¨**`end-to-end`çš„æ–¹æ³•**ï¼Œå³è®­ç»ƒä¸»é¢˜æ¨¡å‹ï¼Œåˆç”ŸæˆCaptionï¼Œå’Œä»¥å¾€çš„Two-stageä¸ä¸€æ ·ã€‚
    * ä»¥å¾€éƒ½æ˜¯å•å±‚çš„Topicæ¨¡å‹
    * ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆç”¨å¯¹è±¡åœ¨é¢„è®­ç»ƒå¥½çš„Topicæ¨¡å‹é‡Œé¢æ£€ç´¢å‡ºæ¥Topicï¼Œç¼ºä¹å¯¹å›¾åƒä¿¡æ¯çš„è€ƒè™‘æ¥ä¸¢å¼ƒTopicä¸­çš„å†—ä½™ä¿¡æ¯ã€‚
  * å­¦ä¹ åˆ°äº†**å±‚æ¬¡åŒ–**çš„Topicè¡¨ç¤º
  * å¯æ’æ‹”çš„

  ![image-20221023213041474](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221023213041474.png)

* Case Study

![image-20221023213130865](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221023213130865.png)





## :sunglasses: Video Understanding

### :video_camera: Features Fusion

:white_check_mark: :fire: :hammer_and_wrench: **[TSN] Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**, in ECCV 2016.  [[pdf](https://arxiv.org/abs/1608.00859)]  [[torch](https://github.com/yjxiong/temporal-segment-networks)]

* æŠ½å–æ‰€æœ‰å¸§æ˜¯ä¸ç°å®çš„ï¼ŒTSNå°†å…¶**ç­‰é—´éš”åˆ†**ä¸º$K$ä¸ªç‰‡æ®µï¼ˆi.e., $K=16$),åœ¨æ¯ä¸ªç‰‡æ®µä¸­è°å¯„æŠ½å–ä¸€å¸§ä½œä¸ºè¾“å…¥

* æä¾›äº†éå¸¸å¸¸ç”¨çš„æ•°æ®äº‰å¼ºæ–¹å¼å’Œä¸€äº›è®­ç»ƒæ—¶å€™çš„trickï¼ˆä¸»è¦åŒ…æ‹¬location jittering, horizontal flipping, corner cropping, and scale jitteringï¼‰

* ä»ç„¶åˆ©ç”¨åŒæµçš„æ€è·¯ï¼Œè®©æ¯ä¸ªç‰‡æ®µä¿¡æ¯æœ€åé€šè¿‡ä¸€ä¸ªå…±è¯†ç½‘ç»œå†Fusion

  ![](https://pic4.zhimg.com/80/v2-67b66b3618606af8b81d1f77b1f92a3b_1440w.jpg)

:white_check_mark: :fire: :hammer_and_wrench: **[TRN] Temporal Relation Reasoning in Videos**, in ECCV 2018.  [[pdf]()] [[torch](https://github.com/zhoubolei/TRN-pytorch)]

![img](https://pic4.zhimg.com/80/v2-86fa6c271c9d2dfad07d4603ed457a83_1440w.jpg)

* èåˆå°ºåº¦ç¡®å®š (éœ€è¦å¤šå°‘ä¸ªè§†é¢‘å¸§æ¥èåˆ)ã€å¦‚å›¾æ‰€ç¤ºã€‘æœ‰2ï¼Œ3ï¼Œ4è¿™ä¸‰ç§å°ºåº¦
* æ¯ä¸ªå°ºåº¦ä¸‹éœ€è¦å¤šå°‘ç»„è§†é¢‘å¸§
* åœ¨åº”ç”¨å¤šå°ºåº¦TRNçš„æ—¶å€™ï¼Œä¸€èˆ¬ä¼šé¢å¤–å¢åŠ ä¸€ä¸ªå…¨å¸§çš„å°ºåº¦ï¼Œå³12å¸§ç‰¹å¾å…¨éƒ¨concatåˆ°ä¸€èµ·ï¼Œä»¥å……åˆ†åˆ©ç”¨æœ‰æ•ˆä¿¡æ¯ã€‚
* **å¹³è¡¡**æ•ˆæœå’Œè®¡ç®—é€Ÿåº¦ï¼Œ**ç®€å•å¥½ç”¨**

:white_check_mark: :fire: :hammer_and_wrench: **[TSM] TSM: Temporal Shift Module for Efficient Video Understanding**, in ICCV 2019.  [[pdf](https://arxiv.org/abs/1811.08383)] [[torch](https://github.com/mit-han-lab/temporal-shift-module)]

* å¯¹æŸäº›é€šé“shiftï¼Œå¾—åˆ°å‰ä¸€å¸§æˆ–è€…åä¸€å¸§çš„ç‰¹å¾

  ![image-20220709174802714](https://s2.loli.net/2022/07/09/cb1f8LWpV9JotO3.png)

* ç”±äºshiftæ˜¯æœ‰æŸå¤±çš„ï¼Œä¸ºæ­¤è®¾è®¡æ®‹å·®æ¥è¿›è¡Œå¼¥è¡¥ï¼ˆåŸæ¥çš„ä¸æ®‹å·®çš„å¯¹æ¯”ï¼‰

![image-20220709174842935](https://s2.loli.net/2022/07/09/2dTjLBJwQ5FUber.png)

:white_check_mark: :fire: :hammer_and_wrench: **[LRCN] Long-term Recurrent Convolutional Networks for Visual Recognition and Description**, in CVPR 2015.  [[pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf)] [[torch](https://github.com/garythung/torch-lrcn)]

* CNNæŠ½å‡ºæ¥çš„å¸§ç‰¹å¾å†æ”¾è¿›å»`LSTM`å¾—åˆ°æ¯å¸§çš„æ—¶åºç‰¹å¾

> å…³äºè§†é¢‘ç‰¹å¾æŠ½å–ï¼Œä¸‹é¢è®²ä¸€ä¸‹`netvlad`ç³»åˆ—çš„ç»“æ„,NextVladå°±æ˜¯ä¸“é—¨é’ˆå¯¹è§†é¢‘å¸§èåˆæ¥åšçš„ä¼˜åŒ–ã€‚
>
> [ç›¸å…³åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/385512915)

:fire: :hammer_and_wrench: **[NetVLAD] NetVLAD: CNN architecture for weakly supervised place recognition**, in CVPR 2016.  [[pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)] [[torch (simple)](https://github.com/lyakaap/NetVLAD-pytorch)]

* VLADç®—æ³•ï¼ˆå®é™…ä¸Šå°±æ˜¯Kmeansï¼‰ï¼š
  $$
  V(j, k)=\sum_{i=1}^{N} a_{k}\left(x_{i}\right)\left(x_{i}(j)-c_{k}(j)\right), \quad k \in K, j \in D
  $$

* æœ¬æ–‡ä½¿ç”¨`CNN`æ¨¡æ‹Ÿè¯¥VLADç®—æ³•çš„è¿‡ç¨‹

  * å¹³æ»‘åŒ–$\alpha$ ä½¿å…¶å˜æˆä¸€ä¸ª0-1åˆ†å¸ƒçš„æƒé‡å‚æ•°ï¼Œä½¿ç”¨$1 \times 1$å·ç§¯+softmax è¿›è¡Œè¯¥è¿‡ç¨‹ï¼Œå¹³æ»‘æ¨å¯¼å…¬å¼$\bar{a}_{k}\left(\mathbf{x}_{i}\right)=\frac{e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k}\right\|^{2}}}{\sum_{k^{\prime}} e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k^{\prime}}\right\|^{2}}}$

  

  ![image-20220712104829439](https://s2.loli.net/2022/07/12/aFlJSCw8dvBnzEH.png)

   

:fire: :hammer_and_wrench: **[NextVLAD] NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**, in ECCV workshop 2018.  [[pdf](https://arxiv.org/pdf/1811.05014.pdf)] [[tensorflow](https://github.com/linrongc/youtube-8m)]

> åŒæ—¶ï¼Œè¿˜æœ‰[å…³äºå¤šæ¨¡æ€ï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰Transformeræ¨¡å‹çš„åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/388361095)





### :timer_clock: Temporal Grounding

> æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªååˆ†ç»å…¸çš„ä»»åŠ¡ï¼ˆTemporal Groundingï¼‰æ¥çœ‹çœ‹è§†é¢‘çš„ç‰¹å¾æ˜¯å¦‚ä½•åˆ©ç”¨çš„

:fire::hammer_and_wrench: **[Video-NLP] Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, in AAAI 2020.  [[pdf](https://arxiv.org/pdf/1912.03590.pdf)] [[torch](https://github.com/microsoft/VideoX)]

> extend version: **MS-2D-TAN**, in TPAMI 2021.  [[pdf](https://arxiv.org/pdf/2012.02646.pdf)]  [[torch](https://github.com/microsoft/VideoX)]

* 2D: start time & end time æ„æˆçš„é‚»æ¥çŸ©é˜µ

![image-20220723153302775](https://s2.loli.net/2022/07/23/lUBL6uPHRFCzvxt.png)

* **æ ¸å¿ƒæ€æƒ³**ï¼š

  * ä½¿ç”¨max pooling ï¼ˆæœ¬æ–‡ä½¿ç”¨ï¼‰ æˆ–è€… stack convolutionçš„è·å–moment featureï¼Œå¦‚ä¸Šå›¾`2D Temporal Feature Map Extraction`æ‰€ç¤º
  * ç”±äºè¿™æ ·å­çš„è®¡ç®—å¼€é”€å¤ªå¤§äº†ï¼Œä½¿ç”¨**ç‰¹å®šçš„é‡‡æ ·æ–¹å¼**è¿›è¡Œè°ƒæ•´ï¼Œè¯¦è§è®ºæ–‡ï¼ï¼ˆè·ç¦»è¿‘çš„é‡‡æ ·å¤šä¸€ç‚¹ï¼Œè¿œçš„é‡‡æ ·å°‘ä¸€ç‚¹ï¼‰
  * å¤šæ¨¡æ€èåˆï¼ˆ`Hadamard product`ï¼‰

  $$
  \mathbf{F}=\left\|\left(\mathbf{w}^{S} \cdot \mathbf{f}^{S} \cdot \mathbb{1}^{T}\right) \odot\left(\mathbf{W}^{M} \cdot \mathbf{F}^{M}\right)\right\|_{F}
  $$

  * æŸå¤±è®¡ç®—æ—¶å€™ï¼Œå¯¹`IoU`è¿›è¡Œä¸€ä¸ªscaleå˜æˆç›‘ç£ä¿¡å·

  $$
  y_{i}= \begin{cases}0 & o_{i} \leq t_{\min } \\ \frac{o_{i}-t_{\min }}{t_{\max }-t_{\min }} & t_{\min }<o_{i}<t_{\max } \\ 1 & o_{i} \geq t_{\max }\end{cases}
  $$

  * BCE loss:

  $$
  L o s s=\frac{1}{C} \sum_{i=1}^{C} y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)
  $$

  

:fire: :hammer_and_wrench: **Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2109.04872)] [[torch](https://github.com/MCG-NJU/MMN)] [[blog](https://zhuanlan.zhihu.com/p/446203594)]

* ä¸»å¹²ç½‘ç»œæ˜¯æ²¿ç”¨[TDN](https://arxiv.org/abs/2012.10071)

* ä½¿ç”¨äº†**metric learning**çš„æ–¹æ³•å¹¶ä¸”å¼•å…¥**è´Ÿæ ·æœ¬**æ¥åšTemporal Groundingçš„ä»»åŠ¡

  * è§†é¢‘é—´çš„è´Ÿæ ·æœ¬ï¼ˆ`IoU`æ¥æ ‡å®šç›‘ç£ä¿¡å·`yi`ï¼Œä¸`2D-TAN`ä¸€æ ·å¤„ç†å¾—æ¥çš„ï¼Œè®°å¾—`scale`ä¸€ä¸‹ï¼‰
  * æ–‡æœ¬ä¸­çš„è´Ÿæ ·æœ¬ï¼Œä»å…¶ä»–è§†é¢‘çš„æ–‡æœ¬è¯­å¥å½“ä¸­é€‰å–å‡ºæ¥

* è´¡çŒ®

  * æ„é€ äº†æ–°çš„ç›‘ç£ä¿¡å·ï¼šè§†é¢‘é—´çš„æ­£è´Ÿæ ·æœ¬(`IoU`æ¥é‡‡æ ·)ï¼Œ å¥å­å’Œè§†é¢‘å¯¹åº”çš„æ­£è´Ÿæ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬å¥å­ä»åˆ«çš„è§†é¢‘æŠ½å–è¿‡æ¥ï¼‰
  * ä¸€ä¸ªè§†é¢‘åªéœ€è¦å»ºæ¨¡ä¸€æ¬¡ï¼Œå¤§å¤§èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä»¥å¾€çš„fusionæ–¹æ³•éƒ½æ˜¯è¦æ–‡æœ¬-è§†é¢‘å¸§å¯¹æ¥å»ºæ¨¡

* Trick

  * ä¸ºäº†ç¼–ç å…¬å¹³ï¼Œä½¿ç”¨é¢„è®­ç»ƒå¥½çš„`DistilBERT`æ¥è¿›è¡Œç¼–ç å¥å­

* æŸå¤±å‡½æ•°è®¡ç®—

  * å’Œ`2D-TDN`ä¸€æ ·çš„`BCE_loss`
  * ç±»ä¼¼äº`InfoNCE loss`çš„è®¾è®¡å¯¹æ¯”æŸå¤±

  $$
  \begin{aligned}
  &p\left(i_{s} \mid v\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)}{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)+\sum_{j \neq i}^{N_{s}} \exp \left(\mathbf{f}_{j}^{S T} \mathbf{f}^{V} / \tau_{v}\right)} \\
  &p\left(i_{v} \mid s\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)}{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)+\sum_{j \neq i}^{N_{v}} \exp \left(\mathbf{f}_{j}^{V T} \mathbf{f}^{S} / \tau_{s}\right)} \\
  &L_{m m}=-\left(\sum_{i=1}^{N} \log p\left(i_{v} \mid s_{i}\right)+\sum_{i=1}^{N} \log p\left(i_{s} \mid v_{i}\right)\right)
  \end{aligned}
  $$

  

### :man_student: Video Question Answer

**Invariant Grounding for Video Question Answering**, in CVPR 2022 oral.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)] [[torch](https://github.com/yl3800/IGV)]

> è¿™ç¯‡æ–‡ç« æ„Ÿè§‰æ˜¯ä¸€ç¯‡å¾ˆæ ‡å‡†çš„`CVPR`çš„ä¸­è§„ä¸­çŸ©æ–‡ç« ï¼Œå†™ä½œç”¨è¯ä¸Šéå¸¸å‡ºè‰²çš„

* å…ˆåšäº†Groundingçš„æ£€æµ‹ï¼Œæ£€æµ‹å‡ºé—®é¢˜ç›¸å…³å¸§ï¼ˆæœ‰å› æœå…³ç³»`Casual`ï¼‰è¿˜æœ‰æ— å…³å¸§ï¼ˆè¡¥å¿å¸§`Complement`ï¼‰
* æ„å»ºè´Ÿæ ·æœ¬åˆ°æ— å…³å¸§å½“ä¸­ï¼Œä½¿ç”¨`memory bank`æ¥å­˜å‚¨æ‰€æœ‰æ ·æœ¬ (å› æ­¤è¦æ³¨æ„å­˜å‚¨çš„ç‰¹å¾ç»´åº¦ä¸èƒ½å¤ªå¤§)

**Video as Conditional Graph Hierarchy for Multi-Granular Question Answering**ï¼Œin AAAI 2022. [[pdf](https://arxiv.org/abs/2112.06197)] [[torch](https://arxiv.org/abs/2112.06197)]

* ç°æœ‰çš„æ–¹æ³•å¯¹è§†é¢‘é—®é¢˜çš„å›ç­”ç¼ºä¹**å¯è§£é‡Šæ€§**
* æ„å»ºäº†ä¸¤ç§è§†è§’æ¥çœ‹é—®é¢˜
  * bottom-upï¼Œ ä¸åŒçš„è§†é¢‘ç‰¹å¾å†³å®šäº†ä¸åŒçš„å±æ€§levelï¼ˆå®ä½“ï¼ŒåŸå­ï¼ŒåŠ¨ä½œï¼Œäº‹ä»¶ï¼‰
  * up-bottomï¼Œé—®é¢˜ä¸­çš„ä¸åŒå•è¯ï¼Œå…³è”äº†ä¸åŒçš„level
* æ„å»º**å›¾ç¥ç»ç½‘ç»œ**æ¥æ¨¡æ‹Ÿè¿™äº›levelæ€è€ƒçš„è¿‡ç¨‹

![image-20220910231620242](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220910231620242.png)

:fire: :hammer_and_wrench: **[äº¤é€šäº‹æ•…QAæ•°æ®é›†] SUTD-TraffificQA: A Question Answering Benchmark and an Effificient Network for Video Reasoning over Traffific Events**, in CVPR 2021. [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_SUTD-TrafficQA_A_Question_Answering_Benchmark_and_an_Efficient_Network_for_CVPR_2021_paper.pdf)] [[project](https://github.com/SUTDCV/SUTD-TrafficQA[)]

* å¯¹æ¯”èµ·å…¶ä»–QAæ•°æ®é›†
  * éœ€è¦æ¨¡å‹æœ‰å› æœæ¨ç†å’Œè®¤çŸ¥å‘å±•ï¼ˆcognitive developmentï¼‰
* å¤„ç†æ–¹æ³•
  * å¯¹ç²—ç»†ä¸¤ç§ç²’åº¦è¿›è¡Œè¯†åˆ«å’Œè®¡ç®—ï¼ˆä¸åŒçš„CNNç½‘ç»œï¼‰ï¼Œå¤§å¤§åŠ å¿«äº†æ¨¡å‹çš„è¿ç®—æ—¶é—´

![image-20220912152326067](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912152326067.png)

**Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering**, in TPAMI 2022. [[pdf](https://arxiv.org/pdf/2207.12647.pdf)]

> è¿™ç¯‡è®ºæ–‡æ¨¡å‹è¾ƒä¸ºå¤æ‚ï¼Œæ‰€ä»¥è¿™é‡Œåªè®²è¯‰å…¶æ ¸å¿ƒæ€æƒ³

* åŠ¨æœº

  * ç°æœ‰æ–¹æ³•åªå…³æ³¨äº†å¾ˆç®€å•çš„äº‹ä»¶ï¼Œæ¯”å¦‚è¯´çœ‹ç”µå½±ï¼Œæ— æ³•å…³æ³¨çœŸæ­£äº‹ä»¶çº§çš„å› æœå…³ç³»

    ![image-20220912224156198](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224156198.png)

  * è¯­è¨€å’Œå›¾åƒå½“ä¸­çš„å¹²æ‰°å› ç´ ï¼ˆConfounderï¼‰

    * è¿‡äºå…³æ³¨ä¸€äº›æ˜¾å¼çš„ä¸œè¥¿ï¼Œå¿½ç•¥äº†ä¸€äº›å¾ˆé‡è¦çš„ä¸œè¥¿

  ![image-20220912224356724](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224356724.png)

* æ–¹æ³•

![image-20220912224635966](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224635966.png)

* è¯¦ç»†ç»†èŠ‚è§è®ºæ–‡ï¼



### :writing_hand: Video Caption

**[Video Caption] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2101.12059)]

:hammer_and_wrench: :fire: **[Video Caption] Robust Change Captioning**, in ICCV 2019. [[pdf](https://arxiv.org/pdf/1901.02527.pdf)] [[torch](https://github.com/Seth-Park/RobustChangeCaptioning)]

* è¾“å…¥ä¸ºå‰åå›¾åƒå¯¹ï¼Œäº”ç§å˜åŒ–ç±»å‹ï¼ˆcolor/material change,adding/dropping/moving an objectï¼‰
* æå‡ºä¸€ä¸ªæœ‰è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[CLEVR-Change](https://cs.stanford.edu/people/jcjohns/clevr/)ï¼ˆ80Kå›¾ç‰‡å¯¹ï¼‰ï¼Œå¹¶åœ¨æ— è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[Spot-the-Diff](https://github.com/harsh19/spot-the-diff)å–å¾—SOTAæ•ˆæœã€‚
* æ¨¡å‹ï¼šDual æ³¨æ„åŠ›ï¼Œ åˆ†è¾¨**è§†ç‚¹å˜åŒ–**ï¼Œ å…¶å®æ˜¯é€šè¿‡è¾“å…¥ä¸¤å¼ å·®ä¸å¤šçš„å›¾ç‰‡ï¼Œæå‰æ ‡å®šå¥½æ•°æ®é›†è·å¾—çš„ï¼Œæœ‰ç‚¹è¢«å‘çš„æ„æ€![image-20220522213419579](https://s2.loli.net/2022/05/22/fiUArgZIjlzw4p1.png)

:hammer_and_wrench: :fire: **[Video Caption] Semantic Grouping Network for Video Captioning**, in AAAI 2021. [[pdf](https://arxiv.org/pdf/2102.00831.pdf)] [[torch](https://github.com/hobincar/SGN)]

* ![image-20220621204108736](https://s2.loli.net/2022/06/21/DMmzxs7dKwyU6BE.png)

:hammer_and_wrench: :fire: **Hierarchical Context-aware Network for Dense Video Event Captioning**, in ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.156/)] [[torch](https://github.com/KirkGuo/HCN)]

* **å±€éƒ¨ä¿¡æ¯**+**å…¨å±€ä¿¡æ¯**ç»“åˆç”Ÿæˆdense caption ï¼ˆè¾“å…¥åŒ…æ‹¬**video** å’Œ **transcript**ï¼‰
* ä¸ºæ­¤è®¾è®¡äº†ä¸¤å¥—`Attention`æœºåˆ¶
  * falt attention + cross attention

![image-20220820000708776](https://s2.loli.net/2022/08/20/wdO5eWZcxgqsItT.png)

* è®¾è®¡äº†é—¨æœºåˆ¶æ¥decodeï¼ˆä¹‹å‰çš„æ–‡æœ¬ä¿¡æ¯ä¸æœªæ¥çš„æ–‡æœ¬ä¿¡æ¯ï¼‰



```mermaid
  graph LR
  SG(Semantic-Grouping) --å»æ‰å†—ä½™phrase--> ç›¸ä¼¼åº¦è®¡ç®—
  SG --attentionæœºåˆ¶ --> å¯¹å…¶phraseå’Œframe --> åŠ å…¥å¯¹æ¯”æŸå¤±,è®¡ç®—æ²¡æœ‰åŒ…å«negativeçš„æ¦‚ç‡
```

**å¯¹æ¯”æŸå¤±**$\mathcal{L}_{c a}=\sum_{(V, Y) \in \mathcal{D}} \sum_{t} \sum_{i}^{M_{t}}\left(-\log p_{c a}\left(s_{i, t}\right)\right)$, $p_{c a}\left(s_{i, t}\right)=\sum_{j=1}^{N} \alpha_{i, j, t}^{p o s}$    ($\alpha^{pos}$ ä¸ºæ­£æ ·æœ¬æ—¶å€™å¯¹é½æ³¨æ„åŠ›çš„æƒé‡) 



##  :apple: Causality Learning

:fire: :hammer_and_wrench:  **Unbiased Scene Graph Generation from Biased Training**, in CVPR 2020. [[pdf](https://arxiv.org/abs/2002.11949)] [[torch](https://github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch)] [[zhihu](https://zhuanlan.zhihu.com/p/109657521)]

* åŠ¨æœº

  * æœŸæœ›ä»**æœ‰åè§çš„è®­ç»ƒä¸­ï¼ˆBiased Trainingï¼‰åˆ©ç”¨æ— åé¢„æµ‹è·å¾—æ— åè§çš„åœºæ™¯å›¾ï¼ˆUnbiased Scene Graphï¼‰**
  * ç°æœ‰æ–¹æ³•é—®é¢˜
    * human **walk on/ sit on/ lay on** beachç­‰åŒ…å«ä¸°å¯Œä¿¡æ¯çš„è°“è¯­ç®€å•â€œæ¦‚æ‹¬â€ä¸ºhuman **on** beachæˆ–å°†**behind/ in front of**â€œæ¦‚æ‹¬â€ä¸º**near**
    * æ— æ³•å¾ˆå¥½åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸Šé¢

* è´¡çŒ®

  * è®¾è®¡äº†ä¸€ä¸ªæ— åé¢„æµ‹çš„æ¨ç†ç®—æ³•**Causal TDE Inference**ï¼ˆéè®­ç»ƒæ–¹æ³•ï¼Œæ¨¡å‹â€œä¸å¯è§â€ï¼Œé€‚ç”¨äºä»»ä½•SGGæ¨¡å‹ï¼‰
  * è®¾è®¡äº†ä¸€ä¸ªæ–°çš„é€šç”¨SGGæ¡†æ¶**[Scene-Graph-Benchmark.pytorch](https://link.zhihu.com/?target=https%3A//github.com/KaihuaTang/Scene-Graph-Benchmark.pytorch)**ï¼Œå…¶ä½¿ç”¨[maskrcnn-benchmark](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/maskrcnn-benchmark)è¿›è¡Œåº•å±‚ç›®æ ‡æ£€æµ‹ï¼Œé›†æˆäº†ç›®å‰æœ€å…¨çš„SG metricsï¼ˆåŒ…æ‹¬Recallã€Mean Recallã€No Graph Constraint Recallã€Zero Shot Recallç­‰ï¼‰ã€‚è¯¥æ¡†æ¶æä¾›é‡å†™çš„å„ç§SGG baselineæ¨¡å‹ï¼ˆå¦‚MOTIFSã€VTranEã€VCTreeï¼‰ï¼Œæœ‰ç€å½“ä¹‹æ— æ„§çš„State-of-The-Art SGClså’ŒSGGenç»“æœã€‚

  ![img](https://pic4.zhimg.com/80/v2-f0657ac8afb0b8fbcbe0fcf65e4944f3_720w.webp)

* é—®é¢˜å®šä¹‰ï¼ˆ**æœ‰åè§çš„æ•°æ®æ ‡æ³¨çš„åŸå› **ï¼‰

  * **the long-tail theoryï¼šperson** carry bagç¡®å®æ¯”**dog** carry bagçš„æ•°é‡å¤š
  * **bounded rationalityï¼ˆæœ‰é™ç†æ€§ï¼‰ï¼š**åœ¨äººç±»æ ‡æ³¨å…³ç³»æ—¶ï¼Œæ›´å€¾å‘äºæ ‡æ³¨ç®€å•çš„å…³ç³»ï¼Œå³æ ‡æ³¨person **beside** tableè€Œä¸æ˜¯person **eating on** tableï¼ˆ**å…³ç³»ä¸å¤Ÿå‡†ç¡®å±‚é¢**ï¼‰
  * **language or reporting biasï¼š**æˆ‘ä»¬æ›´å–œæ¬¢è¯´person **on** bikeï¼Œè€Œä¸æ˜¯person **ride on** bikeï¼ˆ**åªæ˜¯ç®€å•æè¿°äº†çœŸå®äº‹ä»¶çš„ä¸€éƒ¨åˆ†**ï¼‰

* æ— åçš„æ€æƒ³ï¼ˆ**contentï¼šå†…å› ï¼Œcontextï¼šå¤–å› **ï¼‰

  * **äººç±»**åœ¨æœ‰åè§çš„å¤§è‡ªç„¶ä¸­ç”Ÿé•¿ï¼Œåœ¨æ‹¥æŠ±å¥½çš„contextçš„åŒæ—¶ï¼Œé¿å…ä¸å¥½çš„contextï¼Œå¹¶ä¸contentä¸€èµ·åšå‡ºæ— åè§çš„å†³å®šã€‚

  * å…¶æ½œåœ¨çš„æœºåˆ¶æ˜¯**åŸºäºå› æœå…³ç³»çš„ï¼ˆcausality-basedï¼‰ï¼š**å†³ç­–æ˜¯é€šè¿‡è¿½æ±‚ç”±**contentå¼•èµ·çš„ä¸»è¦å› æœæ•ˆåº”**ï¼Œè€Œä¸æ˜¯è¿½æ±‚ç”±**contextå¼•èµ·çš„å‰¯ä½œç”¨**æ¥åšå‡ºçš„ã€‚ç„¶è€Œï¼Œ**æœºå™¨**æ˜¯åŸºäºå¯èƒ½æ€§çš„ï¼ˆlikelihood-basedï¼‰ï¼Œä¼šäº§ç”Ÿæœ‰åç»“æœã€‚

  * æ•…è®ºæ–‡è®¤ä¸ºï¼Œæ— åé¢„æµ‹çš„å…³é”®æ˜¯æ•™ä¼šæœºå™¨å¦‚ä½•åŒºåˆ†ä¸»è¦ä½œç”¨ï¼ˆmain effectï¼‰å’Œå‰¯ä½œç”¨

    * **content**ï¼šobjectå’Œsubjectçš„**visual features**
    * **context**ï¼šobject-subject union regionsçš„**visual features**ä»¥åŠobjectã€subjectçš„**ç±»åˆ«æ ‡ç­¾**

  * ä¸ºäº†åœ¨æ— åé¢„æµ‹ä¸­è¿½æ±‚ä¸»è¦ä½œç”¨ï¼Œè®ºæ–‡æå‡ºèµ‹äºˆæœºå™¨**åäº‹å®æ€ç»´ï¼ˆcounterfactual thinkingï¼‰:**
    ***If i had not seen** the content, would I still make the same prediction?*

  * **åäº‹å®æ€ç»´**ï¼š**äº‹å®ä¸åäº‹å®ä¹‹é—´çš„æ¯”è¾ƒï¼Œå°†ä¼šè‡ªç„¶åœ°æ¶ˆé™¤contextåå·®çš„å½±å“ï¼Œå› ä¸ºcontextæ˜¯ä¸¤è€…ä¹‹é—´å”¯ä¸€ä¸å˜çš„ä¸œè¥¿**ã€‚

  * å¦‚å›¾ï¼Œå·¦ä¾§å›¾ç‰‡æ˜¯æ‰€è°“çš„**äº‹å®åœºæ™¯**ï¼Œä¹Ÿå¯ä»¥è¯´æ˜¯**åŸå§‹åœºæ™¯**ï¼›å³ä¾§å›¾ç‰‡æ˜¯**åäº‹å®åœºæ™¯**ï¼Œå°±æ˜¯**å°†åŸå§‹åœºæ™¯ä¸­contentï¼ˆç‹—å’Œå†²æµªæ¿çš„è§†è§‰ç‰¹å¾ï¼‰å»é™¤ï¼Œå…¶ä»–éƒ¨åˆ†ï¼ˆå¦‚sceneå’Œobject classesï¼‰ä¿æŒä¸å˜ï¼Œå°±åƒobjectçš„è§†è§‰ç‰¹å¾ä»æœªå‡ºç°è¿‡ã€‚**é€šè¿‡è¿™ä¸¤è€…çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºå…³ç³»çš„ä¸»è¦è§†è§‰å½±å“ï¼ŒåŒæ—¶ä¹Ÿä¸ä¸¢å¤±contextã€‚

     ![img](https://pic2.zhimg.com/80/v2-d811612b212103ed4852016136302139_720w.webp)

* æ–¹æ³•

  * **æœ‰å**è®­ç»ƒæ¶æ„

  ![image-20221009153731849](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221009153731849.png)

  * æ— åè§+åäº‹å®çš„å› æœå›¾æ„å»º

  ![image-20221009153828669](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221009153828669.png)

TDEï¼ˆTotal Direct Effectï¼‰æ–¹æ³•**æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„å‚æ•°**ï¼Œä¹Ÿå¯ä»¥è¯´æ²¡æœ‰é’ˆå¯¹æ¨¡å‹çš„æœ‰åè®­ç»ƒè¿›è¡Œä»»ä½•æ”¹åŠ¨ï¼Œå…¶ä½¿ç”¨åŸå§‹SGGæ¨¡å‹è¿›è¡Œäº†ä¸¤æ¬¡é¢„æµ‹ï¼Œå°†ä¸¤æ¬¡é¢„æµ‹çš„ç»“æœè¿›è¡Œ**å·®å€¼è¿ç®—**ï¼Œæœ€ç»ˆå¾—åˆ°æ— åè§çš„é¢„æµ‹ã€‚æ‰€ä»¥**TDEæ–¹æ³•æ˜¯æ¨¡å‹â€œä¸å¯è§â€çš„ï¼Œå¹¿æ³›é€‚ç”¨äºå„ç§SGGæ¨¡å‹**ã€‚

:fire: :hammer_and_wrench: **Distilling Causal Effect of Data in Class-Incremental Learning**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2103.01737)] [[torch](https://github.com/JoyHuYY1412/DDE_CIL)]

* [[æ¨¡å‹å…¬å¼è§£é‡Š](https://zhuanlan.zhihu.com/p/358340627)]  [[è®ºæ–‡ä»‹ç»](https://www.163.com/dy/article/G4OHT10U0511DPVD.html)]

* åŠ¨æœº

  * å¯¹æ’èŠ‚ç‚¹çš„å­˜åœ¨ä½¿å¾—æ¨¡å‹å¯¹æ–°æ•°æ®ä¼šäº§ç”Ÿç¾éš¾æ€§é—å¿˜

  * è¿‡å»çš„æ–¹æ³•å½“ä¸­

    ![image-20220918101920897](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220918101920897.png)

    ![img](https://pic1.zhimg.com/80/v2-0a7452df24b00f8e7d186f0dcf0c0680_720w.jpg)

    

    * **data reply**: éœ€è¦è¾ƒå¤§çš„**å­˜å‚¨**ç©ºé—´ï¼›**distillation**: ä¸æ˜¯**ç«¯åˆ°ç«¯**çš„è¡¨ç¤ºå­¦ä¹ ã€‚å› æ­¤ä½œè€…è€ƒè™‘å§æ˜¯å¦å­˜åœ¨ä¸€ç§ç«¯åˆ°ç«¯å½±å“çš„è’¸é¦æ–¹æ³•ã€‚

* æ€è·¯

  1. æ„å»º**å› æœè§’åº¦ä¸‹çš„ç±»åˆ«å¢é‡å­¦ä¹ **è¿‡ç¨‹
  2. åˆ†æ**ç¾éš¾æ€§é—å¿˜å‘ç”Ÿçš„åŸå› **ï¼ˆ**causal effect** lostï¼‰
  3. åˆ†æç°æœ‰å·¥ä½œå¦‚ä½•å®ç°æœ‰æ•ˆçš„æŠ—é—å¿˜ã€‚ åœ¨è¿™äº›åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‘ç°**æ§åˆ¶å¯¹æ’èŠ‚ç‚¹**æ˜¯ä¸€ç§å°šæœªåˆ©ç”¨ã€ä½†éå¸¸æœ‰æ•ˆçš„æŠ—é—å¿˜æ–¹æ³•ï¼Œåœ¨å„ç§ç±»åˆ«å¢é‡å­¦ä¹ çš„è®¾å®šä¸Šå–å¾—äº†ç¨³å®šçš„æå‡ã€‚
  4. åŒæ—¶è§£å†³äº†æ•°æ®ï¼ˆæ–°æ—§ç±»åˆ«ï¼‰é‡‡æ ·åˆ†å¸ƒ**ä¸å‡åŒ€**å¯¼è‡´çš„**bias**é—®é¢˜ã€‚

* **æ–‡ç« ç»†èŠ‚è¯¦è§å¼€å¤´çš„åšå®¢é“¾æ¥**

:fire: :hammer_and_wrench: **Causal Attention for Vision-Language Tasks**, in CVPR 2021. [[pdf](https://arxiv.org/pdf/2103.03493.pdf)] [[torch](https://github.com/yangxuntu/catt)]

* åŠ¨æœºï¼šè§£å†³å½“å‰`VL`æ¨¡å‹å½“ä¸­è®­ç»ƒæ•°æ®é›†å½“ä¸­bias

  ![æˆªå±2022-10-11 11.09.42](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.09.42.png)

* æ€æƒ³

  * æ„å»ºå› æœå›¾

  ![image-20221011111108239](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221011111108239.png)

  * æ–¹æ³•ï¼ˆ**å‰é—¨è°ƒæ•´æœºåˆ¶**åš**Causal Attention**ï¼Œæ­¤å¤„ä¸åšå±•å¼€è®¨è®ºï¼Œè¯¦è§è®ºæ–‡ï¼‰

    ![æˆªå±2022-10-11 11.12.27](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.12.27.png)

    ![æˆªå±2022-10-11 11.12.56](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.12.56.png)

* ç»“æœcase

  ![æˆªå±2022-10-11 11.13.42](https://raw.githubusercontent.com/Gary-code/pic/main/img/%E6%88%AA%E5%B1%8F2022-10-11%2011.13.42.png)

**Learning Causal Effects on Hypergraphs**, Best Paper of KDD 2022. [[pdf](https://arxiv.org/pdf/2207.04049.pdf)]

* **è¶…å›¾**ä¸Šè¿›è¡Œå› æœå½±å“åˆ†æï¼Œå¯¹æ¯”ä¼ ç»Ÿçš„ä»¥**ä¸¤ä¸¤èŠ‚ç‚¹**ä¸ºå›¾çš„æ›´åŠ æœ‰æ„ä¹‰ã€‚
* [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/564481108)] [[æ–¹æ³•ç»†èŠ‚](https://zhuanlan.zhihu.com/p/567996036)]

**Entropic Causal Inference: Graph Identifiability**, in ICML 2022. [[pdf](https://proceedings.mlr.press/v162/compton22a.html)]

> æ–°çš„**å› æœæ¨æ–­æ¶æ„**ï¼Œå¯ä»¥**ä¸å€ŸåŠ©intervention**
>
> ç†è®ºæ€§å¾ˆå¼ºçš„ä¸€ç¯‡æ–‡ç« 

* ç†µå› æœæ¨æ–­ï¼š

  * é€šè¿‡å¯»æ‰¾æ•°æ®çš„ä¿¡æ¯-ç†è®ºä¸Šæœ€ç®€å•çš„ç»“æ„è§£é‡Šï¼Œå³**æœ€å°ç†µæ¨¡å‹**ï¼Œä»è§‚æµ‹æ•°æ®ä¸­å­¦ä¹ ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å› æœå›¾ã€‚

  * **åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œé¦–å…ˆæ¨å¹¿äº†æ¾å¼›å‡è®¾ä¸‹çš„å› æœå›¾å¯è¾¨è¯†æ€§ç»“æœ**ã€‚

  * ç„¶åï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç¬¬ä¸€ä¸ªå¯è¯†åˆ«çš„ç»“æœï¼Œä½¿ç”¨ç†µçš„æ–¹æ³•å­¦ä¹ **è¶…è¿‡ä¸¤ä¸ªèŠ‚ç‚¹çš„å› æœå›¾**ã€‚

    * è¯¥æ–¹æ³•åˆ©ç”¨äº†ä¸€ä¸ªå±æ€§ï¼Œå³ä¸€ä¸ªæºèŠ‚ç‚¹å’Œå®ƒçš„åä»£èŠ‚ç‚¹ä¹‹é—´çš„ç¥–å…ˆå…³ç³»å¯ä»¥ç”¨äºŒå…ƒç†µæµ‹è¯•æ¥ç¡®å®šã€‚
    * æä¾›äº†ä¸€ç§åŸºäºæ­¤ç‰¹æ€§çš„æ™®é€šå›¾çš„**æœ‰åºå‰¥ç¦»ç®—æ³•**ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ª**å¯å‘å¼ç®—æ³•**ï¼Œå°å›¾æ˜¾ç¤ºäº†è¾ƒå¼ºçš„ç»éªŒæ€§èƒ½ã€‚
    * æœ€åï¼Œåœ¨çœŸå®çš„æ•°æ®é›†ä¸Šæµ‹è¯•çš„ç®—æ³•ã€‚

    ![img](https://pic4.zhimg.com/80/v2-05352bd30cf8d5f8373c2ae67c3203c3_720w.webp)

## :abc: Scene Text Recognization

:hammer_and_wrench: **From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network**, in ICCV 2021. [[pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2108.09661)] [[torch](https://link.zhihu.com/?target=https%3A//github.com/wangyuxin87/VisionLAN)]

* è¿‡å»çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«éœ€è¦ï¼šè§†è§‰ç‰¹å¾æŠ½å–å™¨ + è¯­è¨€æ¨¡å‹

* æœ¬æ–‡ç›´æ¥åœ¨è§†è§‰ç©ºé—´è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼ˆç±»ä¼¼äººç±»ï¼Œè¯­è¨€ä¿¡æ¯æ˜¯å¯ä»¥å­¦ä¹ çš„ï¼‰
  * å¯¹å­—ç¬¦çº§åˆ«çš„Maskæ“ä½œ![image-20220701212346925](https://s2.loli.net/2022/07/01/ZLFUIkb41S782GD.png)
    * è®­ç»ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨å¼±ç›‘ç£äº’è¡¥å­¦ä¹ ![image-20220701212430601](https://s2.loli.net/2022/07/01/kc3K7XxAN6SfRut.png)

:hammer_and_wrench: **Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition**, in AAAI 2022.  [[pdf]([https://arxiv.org/abs/2112.12916](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2112.12916))] [[torch]([https://github.com/adeline-cs/GTR](https://link.zhihu.com/?target=https%3A//github.com/adeline-cs/GTR))]
* åŠ å…¥ä¸€ä¸ªGCNå¼ºåŒ–äº†è§†è§‰å­¦ä¹ çš„è¿‡ç¨‹ï¼Œå¹¶ä¸”åšäº†ä¸€ä¸ªfusion



## :label: NER

>  Named Entity Recognition

**å½“å‰ç«èµ›NERä»»åŠ¡çš„baselineï¼š**

- BERT + BILSTM + CRF
- [åšå®¢è¿æ¥ NERé“æ‰“çš„baseline](https://zhuanlan.zhihu.com/p/166496466)

:fire: **Bidirectional LSTM-CRF Models for Sequence Tagging**, in 2015. [[pdf](https://arxiv.org/pdf/1508.01991v1.pdf)] [[code](https://paperswithcode.com/paper/bidirectional-lstm-crf-models-for-sequence)

* ä½¿ç”¨BiLSTM+CRFåšNERçš„å¼€å±±ä¹‹ä½œ
* [ç›¸å…³åšå®¢è¿æ¥](https://zhuanlan.zhihu.com/p/166496466)

:hammer_and_wrench: :fire: **Fast and Accurate Entity Recognition with Iterated Dilated Convolutions**, in EMNLP 2017.  [[pdf](https://arxiv.org/pdf/1702.02098.pdf)] [[tensorflow](https://github.com/iesl/dilated-cnn-ner)]

* Iterated Dilated Convolutions ç©ºæ´å·ç§¯ 


![image-20220825002628063](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825002628063.png)

* æ ¸å¿ƒæ€æƒ³

  * ä¼ ç»Ÿå·ç§¯çš„é—®é¢˜ï¼špoolingå±‚ä¼š**æŸå¤±ä¿¡æ¯**ï¼Œé™ä½ç²¾åº¦ã€‚é‚£ä¹ˆä¸åŠ poolingå±‚ä¼šä½¿**æ„Ÿå—é‡å˜å°**ï¼Œå­¦ä¸åˆ°å…¨å±€çš„ç‰¹å¾ã€‚å¦‚æœå•çº¯çš„å»æ‰poolingå±‚ã€æ‰©å¤§å·ç§¯æ ¸çš„è¯ï¼Œè¿™æ ·çº¯ç²¹çš„æ‰©å¤§å·ç§¯æ ¸åŠ¿å¿…å¯¼è‡´**è®¡ç®—é‡çš„å¢å¤§**ã€‚
  * CNNä¹Ÿå¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜

![image-20220825004251541](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825004251541.png)

* é€Ÿåº¦æ¯”ä»¥å‰çš„`Bi-LSTM-CRF`å¿«äº†éå¸¸å¤šï¼Œè€Œä¸”ç²¾åº¦æ²¡æœ‰ä¸‹æ»‘

:hammer_and_wrench: :fire: **BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision** , in KDD 2020.  [[pdf](https://arxiv.org/pdf/2006.15509.pdf)] [[torch](https://github.com/cliang1453/BOND)]

* è¿œè·ç¦»ç›‘ç£çš„é—®é¢˜

  * å™ªå£°ï¼Œè´Ÿæ ·æœ¬ä¸å¥½ç”Ÿæˆ
  * å®Œæ•´æ€§ä¸è¶³
  * `trade-off` åœ¨æ ‡æ³¨å‡†ç¡®åº¦å’Œè¦†ç›–èŒƒå›´ä¹‹é—´

* æƒ³æ³•

  * ç¬¬ä¸€é˜¶æ®µä½¿ç”¨`RoBERTa`å¾®è°ƒï¼Œé€‚åº”`NER`ä»»åŠ¡

    * ä½¿ç”¨`Early stopping` æ–¹æ³•é˜²æ­¢æ•°æ®è¿‡æ‹Ÿåˆè¿˜æœ‰å¯¹**æœªçŸ¥æ•°æ®å¢å¼ºæ³›åŒ–èƒ½åŠ›**
    * é¦–å…ˆé€šè¿‡`POS`è¯†åˆ«æ½œåœ¨å®ä½“ï¼Œç„¶åé€šè¿‡è¯­æ–™åº“è®¡ç®—æœ€å°æŸå¤±ç¡®å®šå®ä½“

    ![img](https://pic2.zhimg.com/80/v2-0951f0afefa53b2efc269f92136a3ae9_720w.jpg)

  * ç¬¬äºŒé˜¶æ®µè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ ï¼ˆteacher-studentæ¨¡å‹2ï¼‰

    * åº”å¯¹**å˜ˆæ‚å’Œä¸å®Œæ•´æ ‡æ³¨**çš„æŒ‘æˆ˜
    * teacherç”Ÿæˆä¼ªæ ‡ç­¾äº¤ç»™studentå»é¢„æµ‹
    * **é‡æ–°åŠ æƒçš„é«˜ç½®ä¿¡åº¦è½¯æ ‡ç­¾**
    * ==ç¬¬äºŒé˜¶æ®µçš„æ ¸å¿ƒå°±æ˜¯å¢å¼ºæ•°æ®çš„ç½®ä¿¡ç¨‹åº¦ï¼==

![img](https://pic4.zhimg.com/80/v2-8788aa61ea19b041e763b597d844ebcb_720w.jpg)

ä¸¤é˜¶æ®µçš„BONDæ¡†æ¶

* åœ¨é˜¶æ®µIä¸­ï¼Œç»è¿‡é¢„è®­ç»ƒçš„BERTç”¨äºæ—©åœçš„è¿œè·ç¦»NERä»»åŠ¡
* åœ¨é˜¶æ®µIIä¸­ï¼Œé¦–å…ˆä»é˜¶æ®µIä¸­å­¦ä¹ çš„æ¨¡å‹åˆå§‹åŒ–studentæ¨¡å‹å’Œteacheræ¨¡å‹ã€‚ç„¶åä½¿ç”¨teacheræ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹studentæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åŒæ—¶ï¼Œç”±æ—©åœçš„studentè¿­ä»£æ›´æ–°teacheræ¨¡å‹ã€‚



:hammer_and_wrench: :fire: **[A Boundary-aware Neural Model for Nested Named Entity Recognition](https://aclanthology.org/D19-1034.pdf)** , in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1034.pdf)] [[torch](https://github.com/thecharm/boundary-aware-nested-ner)]

* è§£å†³`Nested NER`çš„é—®é¢˜
* ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•
  * é¢„æµ‹è¾¹ç•Œ
  * é¢„æµ‹å®ä½“åˆ†ç±»

:hammer_and_wrench: :fire: **Cross-Domain NER using Cross-Domain Language** , in ACL 2020.  [[pdf](https://aclanthology.org/P19-1236/)] [[torch](https://github.com/jiachenwestlake/Cross-Domain_NER)]

* è§£å†³**è·¨é¢†åŸŸæ— ç›‘ç£**çš„æ ‡æ³¨é—®é¢˜
* æ ¸å¿ƒæ€æƒ³

![img](https://pic4.zhimg.com/80/v2-52cb3242e0a708e48b29ab2f8d81e027_720w.jpg)

æœ€åº•ä¸‹çš„ä¸€å±‚æ˜¯æ•°æ®å±‚ï¼Œæ ‡å‡†æƒ…å†µä¸€ä¸‹æ€»å…±æœ‰å››ä»½è¯­æ–™ï¼Œåˆ†åˆ«å¯¹åº”**ä¸¤ä¸ªdomainä¸‹çš„ä¸¤ä¸ªtaskï¼ˆNERå’Œè¯­è¨€å»ºæ¨¡ï¼‰**ã€‚å…¶ä¸­Source Domainï¼ˆå³ä¿è¯æœ‰æ ‡è®°æ•°æ®ç”¨äºNERçš„domainï¼‰å¯¹åº”ä¹‹å‰æåˆ°çš„News Domainï¼Œå› ä¸ºè®ºæ–‡ä¸­Source Domainä½¿ç”¨çš„æ˜¯æ–°é—»æ•°æ®ã€‚å¦å¤–å¦‚æœæ˜¯æ— ç›‘ç£æŠ½å–Target Domainæ•°æ®åˆ™åªæœ‰ä¸‰ä»½è¯­æ–™ã€‚

ç”±åº•å‘ä¸Šç¬¬äºŒå±‚æ˜¯Word Embeddingå±‚ï¼Œè®ºæ–‡ä¸­çš„Word Embeddingç»“åˆäº†è¯çº§åˆ«å’Œå­—ç¬¦çº§åˆ«çš„å‘é‡è¡¨ç¤ºã€‚å³æŠŠè¯å‘é‡å’Œä¸€ä¸ªè¯çš„å­—ç¬¦åºåˆ—å½¢æˆçš„çŸ©é˜µç»è¿‡CNNå¤„ç†åçš„**å‘é‡concatenateèµ·æ¥**ã€‚

ç¬¬ä¸‰å±‚æ˜¯åŒå‘LSTMï¼Œç”¨äºåºåˆ—å¤„ç†ç¬¬äºŒå±‚çš„æ•°æ®ï¼Œç”Ÿæˆå‰åå‘hidden stateã€‚

ç¬¬å››å±‚LMå’ŒCRFï¼Œå³task modelå±‚ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬å››å±‚æœ‰ä¸‰ä¸ªæ¨¡å—ï¼Œä¸¤ä¸ªæ˜¯ç”¨äºNERçš„CRFæ¨¡å‹ï¼Œåˆ†åˆ«å¯¹åº”Source Domainå’ŒTarget Domainã€‚å¦ä¸€ä¸ªæ˜¯åŸºäºç¬¬ä¸‰å±‚BiLSTMçš„è¯­è¨€æ¨¡å‹ï¼ŒNSSoftmaxæ˜¯æŒ‡è¿™ä¸ªè¯­è¨€æ¨¡å‹åˆ©ç”¨**Negative Sampling Softmaxçš„æ–¹å¼è¿›è¡Œè®­ç»ƒ**ã€‚



* æœ€å…³é”®çš„åœ°æ–¹å°±æ˜¯`Bi-LSTM`çš„å‚æ•°æ˜¯ç”Ÿæˆçš„ï¼Œä¸åŒdomainçš„ä¸åŒtaskéœ€è¦çš„LSTMçš„å‚æ•°å’Œ$I$æœ‰å…³

$$
\begin{equation}
\theta_{\mathrm{LSTM}}^{d, t}=\mathbf{W} \otimes \mathbf{I}_{d}^{D} \otimes \mathbf{I}_{t}^{T},
\end{equation}
$$





**NERçš„æœªæ¥**

æ—¢ç„¶æ¨¡å‹æ‰“ä¸åŠ¨äº†ï¼Œç„¶åæˆ‘æ‰¾äº†æ‰¾ ACL2020åšNERçš„è®ºæ–‡ï¼Œçœ‹çœ‹ç°åœ¨çš„NERè¿˜åœ¨åšå“ªäº›äº‹æƒ…ï¼Œä¸»è¦åˆ†å‡ ä¸ªæ–¹é¢

1. **å¤šç‰¹å¾**ï¼šå®ä½“è¯†åˆ«ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤æ‚çš„ä»»åŠ¡ï¼Œä¸éœ€è¦å¤ªæ·±å…¥çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå°±æ˜¯åŠ ç‰¹å¾ï¼Œç‰¹å¾è¶Šå¤šæ•ˆæœè¶Šå¥½ï¼Œæ‰€ä»¥å­—ç‰¹å¾ã€è¯ç‰¹å¾ã€è¯æ€§ç‰¹å¾ã€å¥æ³•ç‰¹å¾ã€KGè¡¨å¾ç­‰ç­‰çš„å°±ä¸€ä¸ªä¸ªåŠ å§ï¼Œç”šè‡³æœ‰äº›ä¸­æ–‡ NER ä»»åŠ¡é‡Œè¿˜åŠ å…¥äº†æ‹¼éŸ³ç‰¹å¾ã€ç¬”ç”»ç‰¹å¾...... å¿ƒæœ‰å¤šå¤§ï¼Œç‰¹å¾å°±æœ‰å¤šå¤š
2. **å¤šä»»åŠ¡**ï¼šå¾ˆå¤šæ—¶å€™åš NER çš„ç›®çš„å¹¶ä¸ä»…æ˜¯ä¸ºäº† NERï¼Œè€Œæ˜¯æœåŠ¡äºä¸€ä¸ªæ›´å¤§çš„ç›®æ ‡æˆ–ç³»ç»Ÿï¼Œæ¯”å¦‚ä¿¡æ¯æŠ½å–ã€é—®ç­”ç³»ç»Ÿç­‰ç­‰ã€‚å¦‚æœæŠŠæ•´ä¸ªå¤§ä»»åŠ¡åšä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œå°±éœ€è¦åšæˆä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼ŒæŠŠ NER ä½œä¸ºå…¶ä¸­ä¸€ä¸ªå­ä»»åŠ¡ï¼›å¦å¤–ï¼Œå•çº¯çš„ NER ä¹Ÿå¯ä»¥åšæˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚å®ä½“ç±»å‹è¿‡å¤šæ—¶ï¼Œä»…ç”¨ä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡æ¥åŒæ—¶æŠ½å–å®ä½“ä¸åˆ¤æ–­å®ä½“ç±»å‹ï¼Œä¼šæœ‰äº›åŠ›ä¸ä»å¿ƒï¼Œå°±å¯ä»¥æ‹†æˆä¸¤ä¸ªå­ä»»åŠ¡æ¥åš
3. **æ—¶ä»¤å¤§æ‚çƒ©**ï¼šæŠŠå½“ä¸‹æ¯”è¾ƒæµè¡Œçš„æ·±åº¦å­¦ä¹ è¯é¢˜æˆ–æ–¹æ³•è·Ÿ NER ç»“åˆä¸€ä¸‹ï¼Œæ¯”å¦‚ç»“åˆå¼ºåŒ–å­¦ä¹ çš„ NERã€ç»“åˆ **few-shot learning** çš„ NERã€ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯çš„ NERã€ç»“åˆè·¨è¯­ç§å­¦ä¹ çš„ NER ç­‰ç­‰çš„ï¼Œå…·ä½“å°±ä¸æäº† (Few-shot + Cross-domainæ˜¯ä¸ªä¸é”™çš„é€‰é¡¹ï¼)

ä½œè€…ï¼šç‹å²³ç‹é™¢é•¿
é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/166496466
æ¥æºï¼šçŸ¥ä¹
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚



## :world_map: Knowledge Graph

:fire: **[å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ç»¼è¿°] Multi-Modal Knowledge Graph Construction and Application: A Survey**, in 2022. [[pdf](https://arxiv.org/pdf/2202.05786.pdf)] [[zhihu](https://zhuanlan.zhihu.com/p/484096631)]



## :framed_picture: Topic 

:fire: **[å±‚æ¬¡åŒ–ä¸»é¢˜] Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC**, in ICML 2017. [[pdf](https://proceedings.mlr.press/v70/cong17a.html)]

> ç†è®ºæ€§éå¸¸å¼ºçš„è®ºæ–‡
>
> * è¯¦ç»†çš„ç†è®ºæ¨å¯¼è¯·è§åŸæ–‡ã€‚

* caseå±•ç¤º

![image-20221031220028391](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20221031220028391.png)
