# Knowledge-enriched Text Generation paper reading

ğŸ˜ Awesome list of papers about knowledge-enhanced Question generation with notes.

:white_check_mark: : **already reading carefully**

:fire:: **high citation in recent years**

:hammer_and_wrench:: **available code**

> Content

[TOC]



---

## :grey_question: Question Generation

---

### :mountain_snow: **Textual Question Generating Crosstalk**

**ä¸€ã€åˆ©ç”¨ç­”æ¡ˆå’Œè¯­è¨€ç‰¹å¾**

1. **ä¸¤ç¯‡Ground Breaking Work**

:white_check_mark: :fire: **Neural question generation from text: A preliminary study**, in EMNLP 2017. [[pdf](https://arxiv.org/abs/1704.01792)] 

* åœ¨ç¼–ç æ—¶é¢å¤–è€ƒè™‘äº†ç­”æ¡ˆä½ç½®ä¸è¯­æ³•ä¿¡æ¯ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚(ç°åœ¨æ¥çœ‹éå¸¸**basic**é‡è¦çš„ä¿¡æ¯ï¼)
  * word case åšè®­ç»ƒæ—¶å€™çš„teacher forcing
  * answer position feature
  * lexical features
    * **POS**
    * **NER**

```mermaid
graph LR
en((encoder)) --bi-GRU--> fe((feature-Rich)) --> word-vecotr
fe --> lexcial-feature-embedding-vectors --> POS+NER
fe --> answer-position-embedding --> BIO-tagging

word-vecotr --> åŒå‘çš„éšè—å±‚
POS+NER --> åŒå‘çš„éšè—å±‚
BIO-tagging --> åŒå‘çš„éšè—å±‚

de((decoder)) --å¸¦æ³¨æ„åŠ›æœºåˆ¶,ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›--> maxout-hidden+å…·ä½“éœ€è¦çœ‹referenceè®ºæ–‡
de --> GRU

de --> Copy-Mechanism,ä¸€æ ·ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ› --> è®¡ç®—å‡ºæ¦‚ç‡ä»sourceå¥å­ä¸­ç›´æ¥copyå•è¯
```



:white_check_mark: :fire: :hammer_and_wrench: **Learning to Ask: Neural Question Generation for Reading Comprehension**, in ACL 2017. [[pdf]](https://arxiv.org/abs/1705.00106) [[official code (torch)](https://github.com/xinyadu/nqg)]
* å°†ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œåº”ç”¨äºé—®é¢˜ç”Ÿæˆ
* é‡‡ç”¨seq2seq+attentionæ¨¡å‹æ¶æ„
* æ‘†è„±äº†è½¬æ¢è§„åˆ™ä¸æ¨¡ç‰ˆçš„å±€é™ï¼Œå–å¾—äº†ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„æ€§èƒ½
* åŠ å…¥äº†paragraph-level


```mermaid
graph LR
ä»»åŠ¡éš¾ç‚¹ --æ›´åŠ æ¥è¿‘äºäººç±»--> åŒä¹‰è¯æ›¿æ¢+çŸ¥è¯†å¼•å…¥ --> ç›¸å…³å·¥ä½œ --> è¿‡å»:rule-based 
ç›¸å…³å·¥ä½œ --> å…¶ä»–æ•°æ®æ˜ å°„è‡ªç„¶è¯­è¨€

Seq2Seq --> en((encoder)) --bidirectional--> softè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° --> lstm((LSTM))  --> only-sentence
lstm --> sentence+paragraph --> truncateæˆªæ–­,å½“ç„¶æ›´å¥½çš„æ–¹æ³•æ˜¯åˆ‡ç‰‡

Seq2Seq --> de((decoder)) --word-level-prediction--> LSTM((LSTM)) --> éšè—å±‚åˆå§‹åŒ– --basic-model --> å¥å­encoderçš„æœ€åéšè—å±‚
LSTM --oours--> å¥å­+æ®µè½çš„encoderè¾“å‡º


```

2. **ç­”æ¡ˆç¼–ç **

:white_check_mark: :fire: **Improving Neural Question Generation using Answer Separation**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1809.02393)] 
* å¾ˆå¤šåŸºç¡€æ“ä½œ
* åœ¨ç­”æ¡ˆä¸Šåšäº†ç®€å•é«˜æ•ˆçš„é¢„å¤„ç†
  * Mask åŸæ–‡ä¸­çš„ç­”æ¡ˆ
  * å¯¹ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯åšæŠ½å–ï¼Œè®¡ç®—attention

3. **è¯­è¨€ç‰¹å¾å¼ºåŒ–**

> ä¼ ç»Ÿçš„æœ‰**POS**ï¼ˆè¯æ€§æ ‡æ³¨ï¼‰å’Œ**NER**ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚åç»­è¿˜æœ‰ä¸€äº›æ›´åŠ ç»†å¾®çš„å¤„ç†

:fire: **Learning to Generate Questions by Learning What not to Generate**, in WWW 2019.  [[pdf](https://arxiv.org/pdf/1902.10418.pdf)] 

* clue å’Œ copyçš„æœºåˆ¶
* ![image-20220703112845472](https://s2.loli.net/2022/07/03/hoa1kTVzIpXGeQK.png)
* ![image-20220703113749291](https://s2.loli.net/2022/07/03/bdToKkIADvemV6B.png)
* æ–‡ç« è´¡çŒ®
  * å¸®åŠ©æ¨¡å‹å†³ç­–ä»€ä¹ˆæ—¶å€™ç”Ÿæˆï¼Œä»€ä¹ˆæ—¶å€™copy
  * ç”Ÿæˆå¤šä¸ªé—®é¢˜

4. ç–‘é—®è¯ç±»å‹ï¼ˆquestion typeï¼‰

**Question-type Driven Question Generation**, in EMNLP 2019.  [[pdf](https://arxiv.org/pdf/1909.00140.pdf)]

* å¼•å…¥å¯¹ç–‘é—®è¯çš„é¢„æµ‹æ¨¡å—ï¼Œå¹¶ä¸”åŠ å…¥å¯¹åº”çš„æŸå¤±å‡½æ•°
* ![image-20220703121312077](https://s2.loli.net/2022/07/03/uhcs1erWpQ9UFKL.png)
* ![image-20220703121503229](https://s2.loli.net/2022/07/03/GTO7j5c1AkXNnqY.png)
* ![image-20220703121520488](https://s2.loli.net/2022/07/03/seIfghRSbvpO68a.png)
* [æŸå¤±å‡½æ•°å¼•æ–‡](https://aclanthology.org/P17-1099.pdf)ï¼š <img src="https://s2.loli.net/2022/07/03/UZv17XkMyLRPOgd.png" alt="image-20220703122045296" style="zoom: 50%;" />

**äºŒã€æ®µè½çº§åˆ«ç‰¹å¾**

1. **å¼ºåŒ–æ®µè½çº§åˆ«æ–‡æœ¬çš„ç‰¹å¾**

:fire: :hammer_and_wrench: **Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks**, in EMNLP 2018. [[pdf](https://github.com/seanie12/neural-question-generation)] [[torch](https://github.com/seanie12/neural-question-generation)]

* ä¸»è¦è´¡çŒ®éƒ½åœ¨æ¨¡å‹ä¸Šé¢ï¼ŒåŸºäºseq2seqè®¾è®¡ï¼š![image-20220704150006013](https://s2.loli.net/2022/07/04/trhe5uTRkUqyfsx.png)

  * gate self-attention: ä¸ªäººè§‰å¾—æ˜¯ä¸€å¥—å¾ˆå¸¸ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸‹ï¼Œä¹Ÿéå¸¸ç®€å•

  * Maxout ==**Pointer**== & Decoding **å…¨æ–°çš„å¤„ç† copy æœºåˆ¶**  (æœ‰ç©ºå¯ä»¥è‡ªè¡Œå»çœ‹çœ‹ä»£ç ï¼)

    * ä¹‹å‰copyå¾—åˆ†ï¼š$\operatorname{sc}^{\text {copy }}\left(y_{t}\right)=\left\{\begin{array}{l}\sum_{k, \text { where } x_{k}=y_{t}} r_{t, k}, \quad y_{t} \in \chi \\ -i n f, \text{otherwise}\end{array} \quad\right.$ , é—®é¢˜åœ¨äºè‹¥æ–‡ç« ä¸­æŸä¸ªå•è¯é‡å¤å‡ºç°å¤šæ¬¡ï¼Œåˆ™å¯¹è¯¥å•è¯copyä¹Ÿä¼šå¤šï¼Œå½±å“è¯­å¥é€šé¡ºã€‚

    * ä¸ºæ­¤æ”¹è¿›ä¸ºMaxout Pointerï¼š
      $$
      \operatorname{sc}^{\text {copy }}\left(y_{t}\right)= \begin{cases}\max _{k, \text { where } x_{k}=y_{t}} r_{t, k}, & y_{t} \in \chi \\ -i n f, & \text { otherwise }\end{cases}
      $$

:fire: **Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model**,in ICLR 2020. [[pdf](https://arxiv.org/abs/1908.04942)] [[torch](https://github.com/hugochan/RL-based-Graph2Seq-for-NQG.)]

* å°†passageå’Œanswerçš„è¡¨ç¤ºï¼ˆåŒ…å«bertå‘é‡ï¼Œgloveå‘é‡ï¼Œè¯æ±‡ç‰¹å¾ç­‰ï¼‰è¿›è¡Œå¤šæ¬¡åå¤çš„äº¤äº’è¿›è¡Œç¼–ç ï¼ˆ**éå¸¸ç»†èŠ‚**çš„deep alignment networkï¼‰
* åˆ©ç”¨GNNæ¥ç¼–ç ï¼ˆä½¿ç”¨äº†ä¸¤ç§æ–¹å¼ï¼‰ï¼š
  * å¯¹sentenceåš**dependency parsing**ï¼Œç„¶åç›¸é‚»çš„å¥å­é“¾æ¥å¾—åˆ°passageçš„å›¾
  * é€šè¿‡self attentionçš„æ–¹å¼å¾—åˆ°passage çš„å›¾ï¼ˆæƒå€¼çŸ©é˜µï¼‰

:fire: **Improving Question Generation With to the Point Context**, in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1317.pdf)]

* è”åˆå»ºæ¨¡éç»“æ„åŒ–å¥å­ï¼ˆåŸæ–‡ï¼‰å’Œç»“æ„åŒ–ç­”æ¡ˆç›¸å…³å…³ç³»ï¼ˆ answer-relevant relation é¢„å…ˆä»å¥å­ä¸­æå–ï¼‰ä»¥ç”Ÿæˆé—®é¢˜(**æŠ“å–é‡ç‚¹ä¸Šä¸‹æ–‡**)
* ä½œè€…å‘ç°ä¸Šä¸‹æ–‡ä¸­ï¼Œè·ç¦»ansewræ¯”è¾ƒè¿œçš„è¯å¹¶ä¸ä¸€å®šä¸é‡è¦ï¼Œç›¸å¯¹çš„è·Ÿanswerç´§è´´çš„è¯ä¹Ÿæœ‰å¾ˆå¤šæ— å…³çš„ï¼Œä¸ºäº†æ•æ‰è¿™ç§å…³ç³»ï¼Œä½œè€…ä½¿ç”¨**OpenIE**è¿™ä¸ªå·¥å…·æŠ½å–ä¸Šä¸‹æ–‡ä¸­å­˜åœ¨çš„**å…³ç³»ä¸‰å…ƒç»„**ã€‚

**ä¸‰ã€å¤šä»»åŠ¡è®­ç»ƒ**

**Multi-Task Learning with Language Modeling for Question Generation**, in EMNLP 2019. [[pdf](http://aclanthology.lst.uni-saarland.de/D19-1337.pdf)]

* æŠŠè¯­è¨€æ¨¡å‹ï¼ˆé¢„æµ‹å‰åè¯ï¼‰å’ŒQGä½œä¸ºmulti-taskä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚
* ä¸¤ä¸ªä»»åŠ¡æ˜¯å±‚çº§çš„å…³ç³»ï¼Œå…ˆè¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„æµ‹ï¼Œç„¶åå°†è¯­è¨€æ¨¡å‹çš„hiddenä½œä¸ºç‰¹å¾æä¾›ç»™åé¢seq2seq

:fire: **Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1912.00879)]

* å‡ºå‘ç‚¹æ˜¯æ˜¯**è§£å†³ç”Ÿæˆé”™è¯¯çš„ç–‘é—®è¯**å’Œ**copyåŸæ–‡ä¸­æ— å…³è¯**çš„é—®é¢˜

* ä½œè€…è®¤ä¸ºç”Ÿæˆé”™è¯¯è¯çš„åŸå› æ˜¯æ²¡æœ‰æ­£ç¡®çš„åˆ©ç”¨**answer position**ä¿¡æ¯ï¼Œcopyæ— å…³è¯çš„åŸå› æ˜¯ç¼ºä¹**å±€éƒ¨è¯­ä¹‰ä¿¡æ¯**ã€‚

* ä¸ºäº†åˆ†åˆ«ç¼“è§£è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½œè€…ä¹Ÿæ˜¯è®¾è®¡äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼š

  * è¯­ä¹‰åŒ¹é…åˆ†ç±»ï¼šè¿™ä¸ªä»»åŠ¡çš„è®¾è®¡å‡ºå‘ç‚¹ä¹Ÿæ˜¯SQuADçš„æ•°æ®ç‰¹ç‚¹ï¼Œå¯¹äºä¸€ä¸ªpassageå­˜åœ¨å¤šä¸ªanswer-questionè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å¯¹è¿™æ ·çš„æ•°æ®å®¹æ˜“äº§ç”Ÿä¸€äº›å®½æ³›ä¸å…·ä½“çš„é—®é¢˜ã€‚æ‰€ä»¥ä½œè€…æŠŠpassage-questionä½œä¸ºæ­£æ ·æœ¬ï¼Œpassage-random selected questionï¼Œ random selected passage-questionä½œä¸ºè´Ÿæ ·æœ¬è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚

  ![img](https://pic3.zhimg.com/80/v2-6e137d01fd833693fcc6cf526a39fb8e_720w.jpg)

  * answer-postionä½ç½®é¢„æµ‹ï¼šäº†è®©æ¨¡å‹æ›´å¥½çš„åˆ©ç”¨answerä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªé¢„æµ‹answeråœ¨ä¸Šä¸‹æ–‡ä¸­startå’Œendä½ç½®çš„æ¨¡å‹ï¼ˆpointer networkï¼‰ï¼Œå…¶ä¸­åŸºç¡€çš„ç¼–ç éƒ¨åˆ†é‡‡ç”¨BiDAFçš„æ–¹å¼ã€‚
  * ç„¶å**QGå’Œè¿™ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ä¸€èµ·è®­ç»ƒ**ï¼Œæ•ˆæœå¯ã€‚


****

### :sunrise: Visual QG

:hammer_and_wrench: **[No Visual] Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing**, in AAAI 2021. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17544)] [[torch](https://github.com/VISLANG-Lab/EGSS)]

* Multi-feature Encoder: ä½¿ç”¨äº†POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰+ NERï¼ˆå…³ç³»æŠ½å–ï¼‰

:hammer_and_wrench: **Multiple Objects-Aware Visual Question Generation**, in ACM MM 2021. [[pdf](https://dl.acm.org/doi/abs/10.1145/3474085.3476969)]
* **å†™ä½œä¸Šå†™å¾—å¾ˆå®åœ¨ï¼Œå¾ˆå®¹æ˜“æ‡‚**ï¼Œæœ‰å¾ˆå¤šæ‰¿ä¸Šå¯ä¸‹çš„å¥å­ã€‚
* é¦–æ¬¡å°†**å¯¹è±¡**èå…¥åˆ°é—®é¢˜ç”Ÿæˆä»»åŠ¡å½“ä¸­

![image-20220629230122528](https://s2.loli.net/2022/06/29/6Mn4HPG9ZiCjOqv.png)

:hammer_and_wrench:  **Difficulty-Controllable Visual Question Generation**, in APWeb-WAIM 2021. [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-85896-4_26.pdf)]

* **éš¾åº¦å¯æ§**çš„é—®é¢˜ç”Ÿæˆï¼šé‡‡ç”¨äº†æ•™è‚²å­¦é¢†åŸŸæ”¶é›†å¥½çš„é—®é¢˜éš¾åº¦æ ‡ç­¾(DIF), è¯¦è§[é“¾æ¥](https://www.apims.net/index.php/apims/article/view/9)
* åœ¨VQA2.0æ•°æ®é›†çš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŒºåˆ†ä¸ºå®¹æ˜“å’Œéš¾çš„é—®é¢˜æ•°æ®é›†
  * å¼•å…¥ä¸¤ä¸ªVQAçš„æ¨¡å‹æ¥è¿›è¡Œå›ç­”ï¼Œéƒ½å›ç­”å¯¹çš„ä¸ºå®¹æ˜“ï¼Œéƒ½å›ç­”é”™è¯¯å°±æ˜¯éš¾çš„

* ![image-20220629231350190](https://s2.loli.net/2022/06/29/POftb69si7hnINX.png)
  * å…¶ä¸­Difficulty Variableå°±æ˜¯$\{0, 1\}$



:hammer_and_wrench: **Learning to Caption Images Through a Lifetime by Asking Questions**, in ICCV 2019.  [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9009050)] [[torch](https://github.com/fidler-lab/Caption-Lifetime-by-Asking-Questions)]

* å°†Caption å’Œ VQG ä¸€èµ·æ¥åšï¼Œæå‡ç”Ÿæˆçš„æ€§èƒ½


### :video_camera: Video QG

**Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning**, in ICASSP 2020. [[pdf](https://ieeexplore.ieee.org/document/9053476)]

* ä½¿ç”¨äº†**[TVQA](https://paperswithcode.com/dataset/tvqa)**æ•°æ®é›†ï¼Œis based on 6 popular TV shows and consists of **152,545 QA pairs** from **21,793 clips**.
* æ€»ä½“æ²¡ä»€ä¹ˆåˆ›æ–°çš„

**Multi-Turn Video Question Generation via Reinforced Multi-Choice Attention Network**, in T-CSVT 2021.[[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9161024)]

* Multi-Turnï¼ˆM-VQGï¼‰ï¼šç»“åˆå¤šè½®å¯¹è¯+è§†é¢‘ä¿¡æ¯
* ä¼˜ç‚¹ï¼š åˆ©ç”¨åŠ¨æ€åœºæ™¯ä¿¡æ¯ï¼Œé—®é¢˜å¯å›ç­”æ€§ï¼Œå¯¹è¯è®°å½•ä¿¡æ¯æŠ½å–
* æ–¹æ³•ï¼šbaselineæ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆçœ‹ä¸æ‡‚ï¼‰

**End-to-End Video Question-Answer Generation with Generator-Pretester Network**, in T-CSVT 2021. [[pdf](https://arxiv.org/pdf/2101.01447.pdf)]

* å¼•å…¥ä¸€é—®ä¸€ç­”çš„å½¢å¼ï¼Œç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œç„¶åæµ‹è¯•ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
* ç¡¬ä»¶å¹³å°ï¼šNVIDIA DGX-1ï¼ˆ8 * V100ï¼‰

### :sun_with_face: QG examples

:white_check_mark:  :hammer_and_wrench:  **Mixture Content Selection for Diverse Sequence Generation**, in EMNLP 2019.[[pdf](https://arxiv.org/abs/1909.01953)] [[torch](https://github.com/clovaai/FocusSeq2Seq)]

:hammer_and_wrench: **Radial Graph Convolutional Network for Visual Question Generation**, in IEEE Transactions on Neural Networks and Learning Systems 2020. [[pdf](https://ieeexplore.ieee.org/document/9079208)] [[torch](https://github.com/Wangt-CN/VQG-GCN)]

## :bookmark_tabs: Question Answering

---

### :sunflower: Visual

> åœ¨2022å¹´çš„ä»Šå¤©ï¼ŒVQAä»»åŠ¡ä¸å¤ªå¯èƒ½ä»åˆ·åˆ†çš„è§’åº¦æ¥å…¥æ‰‹äº† [[Blogé“¾æ¥](https://www.zhihu.com/question/419828408/answer/1595386400)]
>
> - VQAä»»åŠ¡æ˜¯ä»€ä¹ˆ
>
> - ä»‹ç»ä¹‹å‰çš„æ¨¡å‹å’Œæ–¹æ³•
>
> - æ¬¢è¿æ¥åˆ°Transformerçš„æ—¶ä»£
>
> - - 2019ï¼šå°è¯•å¤šæ¨¡æ€è¡¨å¾
>   - 2020ï¼šæ‹¥æŠ±å¤šæ¨¡æ€è¡¨å¾
>   - 2021ï¼šç»Ÿä¸€æ„æ¶çš„æ¢ç´¢

machine reading comprehension (**MRC**)å’Œquestion answering (QA)çš„å…³ç³»å…¶å®æ˜¯ç›¸å¯¹ç‹¬ç«‹çš„ã€‚Pure VQAä»»åŠ¡ä¸€èˆ¬æ˜¯æ²¡æœ‰å¼•å…¥é¢å¤–çš„**æ–‡æœ¬å†…å®¹**ï¼Œåªæ˜¯å•çº¯çš„æœ‰$\{å›¾ï¼Œ é—®å¥ï¼Œ å›ç­”\}$ã€‚è€ŒMultimodal MRCä»»åŠ¡ï¼Œå®é™…ä¸Šå°±åªæ˜¯å¼•å…¥äº†**é¢å¤–çš„context**ä½œä¸ºVQAä»»åŠ¡çš„çŸ¥è¯†ï¼Œå¹¶ä¸”æ›´åŠ æ³¨é‡äºè‡ªç„¶è¯­è¨€çš„ç†è§£ã€‚MRCçš„ä¸»è¦**ä»»åŠ¡ç±»å‹**ä¸€å…±æœ‰å››ç§ï¼Œåˆ†åˆ«ä¸º:

* å®Œå½¢å¡«ç©ºï¼ˆCloze Styleï¼‰
* å¤šé¡¹é€‰æ‹©ï¼ˆMultiple Choiceï¼‰
* ç‰‡æ®µæŠ½å–ï¼ˆSpan Predictionï¼‰
* è‡ªç”±ä½œç­”ï¼ˆFree-form Answerï¼‰

**[éæ·±åº¦å­¦ä¹ æ–¹æ³•] Answer-Type Prediction for Visual Question Answering**ï¼Œin CVPR 2016. [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7780907)]

* é¢„æµ‹é—®é¢˜ç±»åˆ«ï¼ˆäººä¸ºæ ‡å®šï¼‰çš„æ¦‚ç‡å†å›ç­”é—®é¢˜
* åˆ©ç”¨**è´å¶æ–¯ç®—æ³•**å¯¹ç›®æ ‡çš„ç©ºé—´å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè®¡ç®—å‡ºæ¯ä¸ªç­”æ¡ˆçš„æ¦‚ç‡
* å…¶æœ‰æ•ˆæ€§ä¸å¦‚ç®€å•çš„åŸºçº¿æ¨¡å‹ï¼›éƒ¨åˆ†åŸå› åœ¨äºå…¶**ä¾èµ–è¯­ä¹‰åˆ†å‰²çš„ç»“æœ**



**Differential Attention for Visual Question Answering**, in CVPR 2018. [[pdf](https://arxiv.org/pdf/1804.00298.pdf)]

* è§£å†³ä¸ºäº†è®©æ¨¡å‹æ›´åŠ å…³æ³¨åˆ°**äººç±»æ‰€å…³æ³¨**çš„åŒºåŸŸ

![image-20220910151132747](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220910151132747.png)



:fire: :hammer_and_wrench: **[å› æœå…³ç³»] Visual Commonsense R-CNN**, in CVPR 2020. [[pdf](https://arxiv.org/abs/2002.12204)] [[torch](https://github.com/Wangt-CN/VC-R-CNN)] [[blog](https://zhuanlan.zhihu.com/p/111306353)]

> å‡ºè‡ª[MReal](https://mreallab.github.io/)ï¼Œ å¼ å«æœ›è€å¸ˆå›¢é˜Ÿçš„å·¥ä½œï¼Œéå¸¸Solidçš„ä¸€ç¯‡å·¥ä½œ
>
> * ç›®æ ‡æ˜¯è®­ç»ƒåŸºäº`Faster-RCNN`è®­ç»ƒä¸€ä¸ªæ›´å¼ºçš„`feature extractor`å¯ä»¥æ•è·è§†è§‰ä¸Šçš„å¸¸è¯†ä¿¡æ¯ã€‚
> * è¿™ç¯‡è®ºæ–‡å®åœ¨**å¤ªå¤šç»†èŠ‚å’Œæ¨ç†**äº†ï¼Œå»ºè®®çœ‹æˆ‘è‡ªå·±çš„**GoodNoteä¸Šçš„ç¬”è®°**ï¼

* åŠ¨æœº

  * ç°åœ¨çš„æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°è§†è§‰å¸¸è¯†ï¼ˆ**Commonsense**ï¼‰ï¼šäººå’Œæ¤…å­ -> äººå¯ä»¥ååœ¨æ¤…å­ä¸Šã€‚ä½†åœ¨NLPä¸­ï¼Œå¸¸è¯†çš„ä¿¡æ¯å·²ç»æ”¾åœ¨ç‰¹å¾é‡Œé¢äº†

    ![image-20220913155431514](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220913155431514.png)

  * æ•°æ®é›†çš„åå·®ä¼šå¯¼è‡´æ— æ³•æ•æ‰åˆ°å¸¸è¯†ä¿¡æ¯
    * çœŸæ­£çš„**è§†è§‰å…³ç³»**æ— æ³•æè¿°ï¼ˆå·¦å›¾ï¼‰
    * ç»™å‡ºçš„**è§£é‡Š**ä¸å¤Ÿæ­£ç¡®ï¼ˆå³å›¾ï¼‰

  **å› æœç†è®ºå°±æ˜¯ç”¨æ¥å‘ç°==ç°è±¡èƒŒåçš„ä¸å˜è§„å¾‹==çš„ï¼Œæ˜¯ä¸€ç§é²æ£’çš„é¢„æµ‹ã€‚è¿™ä¸å¸¸è¯†æœ¬èº«ä¸å°±å¾ˆç›¸ä¼¼å—ï¼Œæˆ‘ä»¬äººç±»ä¹Ÿæ˜¯ä»ç”Ÿæ´»ä¸­ä¸æ–­æ€»ç»“ç§¯ç´¯è¿™äº›ä¸å˜çš„ã€é²æ£’çš„ç»éªŒæˆ–è€…å› æœè§„å¾‹ï¼Œå¹¶æŠŠä»–ä»¬å«åšå¸¸è¯†ã€‚** æ¯”å¦‚ï¼Œçœ‹è§å‡³å­çŸ¥é“å¯ä»¥åï¼Œçœ‹è§pizzaçŸ¥é“å¯ä»¥åƒã€‚

**Association å’Œ Interventionï¼ˆåˆ†å±‚ï¼‰çš„è®¡ç®—**
$$
\begin{gathered}
P(Y \mid X)=\sum_z P(Y \mid X, z) P(z \mid X)=\frac{P(Y, X)}{P(X)} \\
P(Y \mid d o(X))=\sum_z P(Y \mid X, z) P(z)=\sum_z \frac{P(Y, X, z) P(z)}{P(X, z)}
\end{gathered}
$$
å…¶ä¸­ $X, Y, z$åˆ†åˆ«ä»£è¡¨äº†å›¾ç‰‡ä¸­çš„object labelï¼ŒåŒæ—¶è¿™é‡Œæˆ‘ä»¬ç”¨ç‰©ä½“å‡ºç°çš„é¢‘ç‡æ¥ä»£æ›¿æ¦‚ç‡ï¼Œæ¯”å¦‚ $P(Sink|Hair drier)$å°±æ˜¯ç”¨â€œå«æœ‰$Sink$å’Œ$Hair drier$ä¸¤è€…çš„å›¾ç‰‡æ•°â€æ¯”ä¸Šâ€œåªå«æœ‰Hair drierçš„å›¾ç‰‡æ•°â€è®¡ç®—å¾—åˆ°çš„ã€‚ç”»å‡ºä¸¤è€…è®¡ç®—ç»“æœå·®å¼‚çš„å¯¹æ¯”å›¾ï¼ˆåªæ ‡æ˜äº†20ç±»ï¼‰ï¼š

![img](https://pic2.zhimg.com/80/v2-ddffe6ddaaf70839faa1d62a9ef25291_720w.jpg)

* ä¸¤ä¸ªCaseçš„åˆ†æ
  * $Sink å’Œ drier$ï¼Œæƒ³è¦æ¢å¯»åœ¨**å·²çŸ¥å¹é£æœº**çš„æƒ…å†µä¸‹ï¼Œå»é¢„æµ‹æ°´æ± çš„å¯èƒ½æ€§å¤§å° $P(Sink|drier)$
    * **åœºæ™¯å› ç´ **è€ƒè™‘åœ¨å†…ï¼Œå¯¹ä¸åŒçš„åœºæ™¯è¿›è¡Œåˆ†å±‚ï¼ˆå› ä¸ºåœºæ™¯å°±æ˜¯ç”±objectç»„æˆçš„ï¼‰ï¼Œå¾—åˆ°å®é™…çš„å› æœæ•ˆåº”ï¼Œæ¯”å•çº¯Associationç®—çš„æ•°å€¼è¦ä½
  * äººå’Œé©¬æ¡¶ï¼Œæ¢å¯»â€œé©¬æ¡¶â€å’Œâ€œäººâ€ä¹‹é—´å¯èƒ½å­˜åœ¨çš„å› æœæ•ˆåº”
    * æ•°æ®é›†ä¸­äººå’Œé©¬æ¡¶ä¸€èµ·å‡ºç°çš„æ ·æœ¬å…¶å®ä¸å¤šï¼ˆä¹Ÿä¸ä¼šæœ‰å¾ˆå¤šäººåœ¨é©¬æ¡¶æ—è¾¹æ‹ç…§ï¼‰
    * å¦‚æœæƒ³è¦åšå‡ºæ›´robustçš„é¢„æµ‹ï¼Œæˆ‘ä»¬å°±éœ€è¦è€ƒè™‘æ··æ‚å› å­**confounder**ï¼Œ æ¯”å¦‚ç“¶å­ã€æ°´æ± ã€æ¯å­ç­‰ç­‰ã€‚æŒ‰ç…§confounder è¡Œ**åˆ†å±‚è®¡ç®—**ï¼Œæœ€åå†åŠ æƒæ±‚å’Œã€‚

* æ–¹æ³•ï¼ˆå› æœå¹²é¢„**Intervention**ï¼‰

  * ä»£ç†ä»»åŠ¡ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰ï¼š**ç»™å®šRoI Xçš„featureå»é¢„æµ‹RoI Yçš„ç±»åˆ«**

  * åŒ…æ‹¬å¾ˆå¤šæ½œåœ¨çš„**æ··æ‚å› å­**ï¼Œå¦‚æœç›´æ¥é¢„æµ‹å‘¨å›´ç‰©ä½“Yå°±ä¸å¯é¿å…çš„ä¼šè¢«ä¸Šæ–‡æåˆ°çš„æ··æ‚å› å­**confounder**æ‰€å½±å“ã€‚æ ¹æ®æˆ‘ä»¬åˆšåˆšä»‹ç»çš„**â€œdoç®—å­â€**çš„ç†è®ºï¼Œè§£å†³çš„åŠæ³•ä¹Ÿä¸éš¾ï¼Œåªè¦èƒ½æ‰¾åˆ°confounderç„¶åå¯¹ä»–ä»¬ä½¿ç”¨**backdoorç†è®º**è¿›è¡Œæ§åˆ¶å³å¯ã€‚

  * æ··æ‚å› å­æ˜¯ä»€ä¹ˆï¼Ÿ æˆ‘ä»¬ç›´æ¥æŠŠæ•´ä¸ªæ•°æ®é›†ä¸Šçš„**object RoIç‰¹å¾ï¼ˆFaster RCNNä¸­æ¥ï¼‰åœ¨æ¯ä¸ªç±»åˆ«ä¸Šå–å¹³å‡**ï¼Œå½“ä½œè¿™ä¸ªç±»åˆ«çš„è¡¨ç¤ºï¼Œè¿›è€Œæ„å»ºå‡ºä¸€ä¸ª **ç±»åˆ«æ•°x1024** çš„confounderå­—å…¸ä½œä¸º$Z$ï¼ˆæ¯”å¦‚MSCOCOæœ‰80ç±»ï¼Œå°±æ˜¯ 80x1024ï¼‰ï¼Œå®ƒåŒ…å«ç€æ‰€æœ‰å¯èƒ½çš„æ··æ‚å› å­ã€‚

  * åé—¨è°ƒæ•´

    ![img](https://raw.githubusercontent.com/Gary-code/pic/main/img/v2-514061ff24e803c016324ead8bcf84b1_720w.jpg)

    * æˆ‘ä»¬æŠŠconfounder dictionary $Z$ä¸­çš„ç‰©ä½“$z_i$â€œborrowâ€åˆ°å½“å‰å›¾ç‰‡ä¸­ï¼Œæ³¨æ„è¿™é‡Œçš„ç‰©ä½“$z_i$ä¸éœ€è¦æ˜¯å½“å‰å›¾ç‰‡ä¸­å­˜åœ¨çš„ï¼Œæ‰€ä»¥æ˜¯ä¸€ç§globalå±‚é¢çš„å®šä¹‰ã€‚
    * ç„¶åæŠŠå€Ÿæ¥çš„$z_i$â€œputâ€åˆ°$X, Y$å‘¨å›´å’Œ$X, Y$å¯¹æ¯”ï¼Œä¾‹å¦‚ä¸Šå›¾ä¸­çš„æŠŠ sinkã€handbagã€chairç­‰ç­‰ç§»åˆ° toilet å’Œ person å‘¨å›´è¿›è¡Œbackdoorçš„è®¡ç®—ã€‚

  * æ¨¡å‹

    * æ•´ä¸ªinterventionæ•´åˆæˆä¸€è·¯context predictorã€‚
    * åŒæ—¶ä¸ºäº†ä¸è®©ç½‘ç»œå¿˜æ‰è¯†åˆ«RoIæœ¬èº«ç±»åˆ«çš„èƒ½åŠ›ï¼Œcontext predictorçš„åŸºç¡€ä¸Šåˆä¿ç•™äº†åŸå…ˆçš„è‡ªèº«ç±»åˆ«é¢„æµ‹**self predictor**ã€‚

  ![img](https://pic2.zhimg.com/80/v2-d3a05b26274f54a8bd785209f1b6a4c1_720w.jpg)

  æ³¨æ„ï¼šVC R-CNNçš„å®ç°å’ŒåŸå…ˆçš„Faster R-CNNç›¸æ¯”ï¼Œ**å»é™¤äº†RPNç½‘ç»œ**ï¼ˆRegion Proposal Networkï¼‰ï¼Œä¸å†è®­ç»ƒç½‘ç»œproposeè¾¹ç•Œæ¡†ï¼Œè€Œæ˜¯ç›´æ¥å°†æ•°æ®é›†**ground-truthçš„bounding boxåæ ‡è¾“å…¥åˆ°å…¶ä¸­**ï¼Œç›´æ¥æå–regionçš„ç‰¹å¾ã€‚è€Œåœ¨è®­ç»ƒå®Œæˆåçš„featureæå–é˜¶æ®µï¼Œç›¸å¯¹åº”çš„ï¼Œåªè¦ç»™å®šå›¾ç‰‡å’Œbounding boxåæ ‡ï¼Œéƒ½å¯ä»¥è·å¾—å¯¹åº”çš„VCç‰¹å¾ã€‚å°±è¿™æ ·ï¼Œæˆ‘ä»¬åˆ©ç”¨bottomupç‰¹å¾å·²æœ‰çš„è¾¹ç•Œæ¡†åæ ‡æå–VCç‰¹å¾åï¼Œå°†å…¶å¹¶åœ¨å…ˆå‰çš„bottomupç‰¹å¾ä¸Šä½œä¸ºæ–°çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¼ ç»Ÿçš„ Vision&Language ä¸‰å¤§ä»»åŠ¡ä¸ŠæŒ‘é€‰äº†ç»å…¸modelå’ŒSOTA modelè¿›è¡Œäº†æµ‹è¯•ï¼Œå‘ç°åœ¨å„ä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜æ˜¾çš„æå‡ï¼Œå°¤å…¶æ˜¯åœ¨image captioningä¸Šçš„æå‡å°¤å…¶å¤§ã€‚åŒæ—¶ä¸ºäº†éªŒè¯æ€§èƒ½çš„æå‡ä¸æ˜¯ç”±äºå‚æ•°å¢å¤šå¸¦æ¥çš„ï¼Œæˆ‘ä»¬è¿˜åœ¨åŸæœ‰ç‰¹å¾ä¸Šå¹¶ä¸Šäº†ablativeçš„ç‰¹å¾ï¼ˆå•ç‹¬objectç‰¹å¾ï¼Œç”¨correlationè®¡ç®—çš„ç‰¹å¾ï¼‰ï¼Œå…·ä½“å¯ä»¥å‚è€ƒè®ºæ–‡çš„å®éªŒéƒ¨åˆ†ã€‚

:hammer_and_wrench: **MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering**, in CVPR 2022.  [[pdf](https://arxiv.org/pdf/2203.09138.pdf)] [[torch](https://github.com/AndersonStra/MuKEA.)]

* åŠ¨æœº

  * è¿‡å»åŸºäºçŸ¥è¯†çš„ï¼Œéƒ½åªæ˜¯è€ƒè™‘äº†æ–‡æœ¬ä¸Šçš„çŸ¥è¯†ï¼Œç¼ºä¹å¯¹å¤šæ¨¡æ€çŸ¥è¯†çš„ç†è§£

  ![image-20220901165323764](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901165323764.png)

* ä¸»è¦è´¡çŒ®

  * ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€çŸ¥è¯†è¡¨ç¤º $(Entity, relation, answer)$
  * **pre-training and fine-tuning** strategy to accumulate both **out-domain and in-domain** knowledge

![image-20220901165520112](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901165520112.png)

* ç»†èŠ‚

  * ä¸‰ä¸ª**æŸå¤±å‡½æ•°**çš„è®¾è®¡

    * `Triplet TransE Loss`: ä¿æŒembeddingçš„ç»“æ„ï¼ˆé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼‰

    $$
    \mathcal{L}_{\text {TransE }}=\sum_{t^{+} \in \mathcal{A}^{+}} \sum_{t^{-} \in \mathcal{A}^{-}}\left[\gamma+\mathrm{d}\left(h+\boldsymbol{h}, \boldsymbol{t}^{+}\right)-\mathrm{d}\left(\boldsymbol{h}+\boldsymbol{r}, \boldsymbol{t}^{-}\right)\right]_{+}
    $$

    * `Triplet Consistency Loss`ï¼š ä¿è¯ä¸¥æ ¼çš„**æ‹“æ‰‘å…³ç³»**

    $$
    \mathcal{L}_{\mathrm{Tri}}=\operatorname{MSE}\left(h+r, t^{+}\right)
    $$

    * `Semantic Consistency Loss`: ä¿æŒåœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„è¡¨è¾¾ä¸€è‡´æ€§

    $$
    {P\left(t^{+}\right)=\operatorname{softmax}\left((T)^{T}(h+r)\right)} \\{\mathcal{L}_{\mathrm{Sem}}=-\log \left(P\left(t^{+}\right)\right)}
    $$

    

  * é¢„è®­ç»ƒå’Œå¾®è°ƒç­–ç•¥

    * å…ˆåœ¨`VQA 2.0`æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ¥æ”¶é›†è§†è§‰ä¸»å¯¼çš„çŸ¥è¯†
    * åœ¨`KB-VQA`æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ

  * å…³äºå°¾éƒ¨`Entity`

    * è®­ç»ƒçš„æ—¶å€™ç›´æ¥åš`teacher-forcing`
    * æ¨ç†çš„æ—¶å€™è®¡ç®—$\mathbf{h}_{inf}+\mathbf{r}_{inf}$ ä¸ `look up` table $\mathbf{T}$çš„æœ€å°è·ç¦»

    $$
    \boldsymbol{t}_{\inf f}=\underset{\boldsymbol{t}_i \in T}{\arg \min } \mathrm{d}\left(\boldsymbol{h}_{\text {inf } f}+\boldsymbol{r}_{\text {inf } f}, \boldsymbol{t}_{\mathrm{i}}\right)
    $$

    

![image-20220901170039718](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901170039718.png)

**[VCR] Explicit Cross-Modal Representation Learning for Visual Commonsense Reasoning**, in TMM 2022. [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9465732)

* åŠ¨æœºï¼šä¸ºäº†åŠ å¼º`VCR`ä»»åŠ¡çš„**reasoning**è¿‡ç¨‹ï¼Œä¸å†é‚£ä¹ˆéšå¼
* æ–¹æ³•

![image-20220923223408003](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220923223408003.png)

* ä¾‹å­

![image-20220923223502035](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220923223502035.png)



### :sunny: Textual

:fire:  :hammer_and_wrench: **[Question Answering] Commonsense for Generative Multi-Hop Question Answering Tasks**, in EMNLP 2018. [[pdf]](https://arxiv.org/abs/1809.06309) [[tensorflow]](https://github.com/yicheng-w/CommonSenseMultiHopQA)

:hammer_and_wrench: **[Dialogue System] Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering**, in AAAI 2020. [[pdf]](https://arxiv.org/abs/1912.07491) [[torch]](https://github.com/siat-nlp/TransDG)

**[Question Answering] Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs**, in EMNLP 2019. [[pdf\]](https://arxiv.org/abs/1910.08435)

:fire: :hammer_and_wrench: **[Question Answering] ** **Improving Multi-hop Question Answering over Knowledge Graphs usingKnowledge Base Embeddings**, in ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.412/)] [[torch](https://github.com/malllabiisc/EmbedKGQA)]

:hammer_and_wrench: **Found a Reason for me? Weakly-supervised Grounded Visual Question Answering using Capsules**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Urooj_Found_a_Reason_for_me_Weakly-supervised_Grounded_Visual_Question_Answering_CVPR_2021_paper.pdf)] [[torch](https://github.com/aurooj/ WeakGroundedVQA_Capsules.git)]

* ä¸ç”¨faster-rcnn
* è®­ç»ƒè¾“å…¥æ˜¯é—®é¢˜å’Œç­”æ¡ˆï¼Œè¾“å‡ºæ˜¯é¢„æµ‹ç­”æ¡ˆå¯¹åº”çš„**grouding area**ã€‚

**KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base**, in ACL 2022. [[pdf](https://aclanthology.org/2022.acl-long.422.pdf)] [[project](https://github.com/shijx12/ KQAPro_Baselines)]

* æ›´åŠ å¤æ‚çš„æ•°æ®é‡æ›´å¤§çš„å¼•å…¥çŸ¥è¯†çš„æ•°æ®é›†
  * å¹¶ä¸”ç»™å‡ºäº†ä¸¤ç§reasoningçš„è¿‡ç¨‹
  * å¯ä»¥åšQAå’Œ**è¯­ä¹‰è§£æ**æœåŠ¡
  * åˆ©ç”¨æ›´åŠ å¤æ‚çš„æ¨¡ç‰ˆå’ŒçŸ¥è¯†ç”Ÿæˆé—®é¢˜

![image-20220912160811327](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912160811327.png)

* [è¯¦ç»†ä»‹ç»çš„blog](https://blog.csdn.net/weixin_47903246/article/details/124649493)



:hammer_and_wrench: **[VCR] Heterogeneous Graph Learning for Visual Commonsense Reasoning**, in NIPS 2019. [[pdf](https://arxiv.org/abs/1910.11475)] [[torch](https://github.com/yuweijiang/HGL-pytorch)]

* ä¸ä¼ ç»Ÿçš„`VQA`ä¸å¤ªä¸€æ ·ï¼ŒR: è§£é‡Šï¼ˆReasonï¼‰
  * ä¸‰ä¸ªå­ä»»åŠ¡åˆ†åˆ«æ˜¯: $Q \rightarrow A$, $QA \rightarrow R$, $Q \rightarrow AR$
* æ–¹æ³•ï¼šæ„å»ºå¼‚æ„å›¾

![image-20220921151513456](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921151513456.png)

![image-20220921151544188](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921151544188.png)



:hammer_and_wrench: **[VCR] Connective Cognition Network for Directional Visual Commonsense Reasoning**ï¼Œin NIPS 2019.  [[pdf](https://proceedings.neurips.cc/paper/2019/file/8a56257ea05c74018291954fc56fc448-Paper.pdf)] [[torch](https://github.com/AmingWu/CCN)]

* ä¸ä¸Šä¸€ç¯‡è®ºæ–‡æ€æƒ³æ¯”è¾ƒç±»ä¼¼ï¼Œå‚è€ƒç¥ç»ç§‘å­¦å½“ä¸­å°†ç¥ç»å…ƒæ•´åˆèµ·æ¥çš„æ€æƒ³

* åšæ³•

![image-20220921171105320](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171105320.png)

* æ¨¡å‹ç›¸å…³ç»†èŠ‚

![image-20220921171210400](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171210400.png)

* ç¬¬ä¸€partä¸­**è¿æ¥**çš„æ„å»º

![image-20220921171342266](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220921171342266.png)

## :book: Paraphrase

---



:hammer_and_wrench: **[Sentence Discrimination] Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator**,in COLING 2018. [[pdf](https://aclanthology.org/C18-1230/)] [[torch](https://github.com/badripatro/PQG)]

:hammer_and_wrench: **[Hierarchical Sketch&Paraphrase Generation] Hierarchical Sketch Induction for Paraphrase Generation**, in ACL 2022.[[pdf](https://aclanthology.org/2022.acl-long.178.pdf)] [[torch](https://github.com/tomhosking/hrq-vae)]

### :whale2: Related Big Model

:fire: :hammer_and_wrench: **[Cross-Modal&Contrastive Learning] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning**, in ACL(long paper) 2021. [[pdf](https://aclanthology.org/2021.acl-long.202/)] [[project from Baidu](https://unimo-ptm.github.io/)]

:hammer_and_wrench: **[MultiModal] UniT: Multimodal Multitask Learning with a Unified Transformer**, ICCV 2021. [[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.pdf)] [[project from Fair](https://mmf.sh/)]

## :framed_picture: Image Caption

---



:white_check_mark: :hammer_and_wrench: **[Image Caption] Generating Diverse and Descriptive Image Captions Using Visual Paraphrases**, in ICCV 2019. [[pdf](https://ieeexplore.ieee.org/document/9010984)] [[torch](https://github.com/pkuliu/visual-paraphrases-captioning)]

* è¯¥è®ºæ–‡ç ”ç©¶äº†ç›®å‰å›¾åƒçš„æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å¤è¿°çš„ä¸¤é˜¶æ®µè§£ç çš„æ¨¡å‹ã€‚
  * ç»™å®šå›¾åƒè¾“å…¥ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç”Ÿæˆåˆæ­¥çš„å¥å­ï¼Œå†å°†å…¶æ”¹å†™ä¸ºå†…å®¹æ›´åŠ å¤šæ ·å’Œä¸°å¯Œçš„æè¿°ã€‚åœ¨MS COCOå›¾åƒæè¿°æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ã€‚

  * é‡ç‚¹æ¢ç´¢**visual paraphrases** è§’è‰² + **scoring function**
  
    * ```mermaid
      graph LR
      ä¸äººç±»ç›¸æ¯” --æ–‡ç« ä¸­æœ‰example--> ç¼ºå°‘å¤šæ ·æ€§å’Œå…·ä½“æ€§ --> ä¸¤é˜¶æ®µè§†è§‰å¤è¿°æ–¹æ³• --> MSCOCOæ•°æ®é›†
      ```
  
  
  * æ•…äº‹å±•å¼€:
  
    * ```mermaid
      graph LR
      æ ‡å‡† -->æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> å½¢å®¹è¯
      æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> ç»†èŠ‚,with
      å½¢å®¹è¯ --> Pa((Paraphrase))
      ç»†èŠ‚,with --> Pa
      Pa --> visual-paraphrase
      visual-paraphrase --> sentence_pairs --> ä¸¤é˜¶æ®µç¼–ç 
      ```
  
    * ```mermaid
      graph LR
      ç›¸å…³å·¥ä½œ --caption--> å¤šcaption.vs.å•caption --paraphrases--> æœªå¤„ç†ç‰¹å¾å’Œè§†è§‰ä¿¡æ¯ --ä¸¤é˜¶æ®µç¼–ç --> ä¸­é—´seq.vs.2captions 
      ```
  
  
  * æ¨¡å‹æ–¹æ³•ï¼š
  
    * ```mermaid
      graph LR
      é€‰æ‹©è§†è§‰å¤è¿°captionå¯¹ --> è¯„åˆ†å‡½æ•° --> è®¾è®¡ä¸‰ä¸ªAttentionæ“ä½œ,å­¦ä¹ åˆ°å¤šæ¨¡æ€çŸ¥è¯† --> æœ€åsoftmaxè¾“å‡º
      ```

* æ›´å¤šç»†èŠ‚å¯è§æˆ‘ä¸ªäººçš„[slide](https://kdocs.cn/l/conDzdschwAn)

:white_check_mark: ::fire: :hammer_and_wrench: **[Text Generation & Image Caption] Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions**, in CVPR 2019. [[pdf](https://openaccess.thecvf.com/content_CVPR_2019/html/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.html)] [[torch](https://github.com/aimagelab/show-control-and-tell)]

![](https://s2.loli.net/2022/04/09/COnvomETrl6GRf2.png)

![](https://s2.loli.net/2022/04/09/7ASmXcCazOh9GsU.png)

```mermaid
  graph LR
  å¤–éƒ¨ä¿¡å·æ§åˆ¶ --> å›¾åƒä¸­çš„ä¸€ç»„åŒºåŸŸå— --> core((æ ¸å¿ƒ))
  core --> æ”¹å˜chunkçš„é¡ºåº
  core --> æ”¹å˜å›¾åƒçš„åŒºåŸŸ
  model((æ¨¡å‹)) --åŸºäºåŒºåŸŸçš„ç‰¹å¾ä¸çŠ¶æ€--> LSTM((LanguageModel,ä¸¤å±‚LSTM)) --ç¬¬ä¸€å±‚--> è®¡ç®—attention --æ³¨æ„--> æ‰€æœ‰åŒºåŸŸçš„ç‰¹å¾å‘é‡è¿›è¡Œmean-poolingä½œä¸ºå›¾åƒçš„æ€»ä½“ç‰¹å¾I
  LSTM --ç¬¬äºŒå±‚--> é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯
  model --ä½•æ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå›¾åƒåŒºåŸŸ--> å—è½¬ç§»é—¨ --è®¡ç®—gt--> åŸºäºç¬¬ä¸€å±‚LSTMçš„çŠ¶æ€è®¾ç«‹ä¸€ä¸ªchunk-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sc_t-rtçš„attention
  model --è§†è§‰è¯oræ–‡æœ¬æ¬¡--> AdaptiveAttention --> è®¾ç½®ä¸€ä¸ªvisual-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sv_t-rtçš„attention --> attentionçš„ç»“æœ,å¯ä»¥è®¡ç®—å‡ºå½“å‰æ—¶åˆ»æ¨¡å‹æ­£åœ¨å…³æ³¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ct
  model --æ— åºé›†åˆæ’åº--> æ’åºç½‘ç»œ --> Rä¸­åŒ…å«Nä¸ªåŒºåŸŸé›† --å…¨è¿æ¥å±‚--> æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --Sinkhornç®—å­--> è½¯ç½®æ¢çŸ©é˜µ 
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --> æœ€å°åŒ–è½¯ç½®æ¢ä¸çœŸå®ç»“æœä¹‹é—´çš„å‡æ–¹è¯¯å·®
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --æµ‹è¯•åŒˆç‰™åˆ©ç®—æ³•è¿›è¡ŒåŒ¹é…--> è½¯ç½®æ¢çŸ©é˜µè½¬åŒ–ä¸ºæœ€ç»ˆçš„ç½®æ¢,ä»¥æ­¤æ¥å¯¹Rè¿›è¡Œæ’åº
```

 [è¯¦ç»†è®²è§£](https://zhuanlan.zhihu.com/p/150667499)

:hammer_and_wrench: **Length-Controllable Image Captioning**, in ECCV 2020 by [Qi Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+Q) and  Mingkui Tan.  [[pdf](https://arxiv.org/abs/2007.09580)] [[torch](https://github.com/bearcatt/LaBERT)]

* åŠ¨æœº
  * ä¸ºäº†è®©å¥å­æ›´åŠ ç²—ç•¥æˆ–è€…ç»†èŠ‚ï¼Œæå‡º**é•¿åº¦å¯æ§**çš„captionç”Ÿæˆ
  * è¿‡å»ç”±äºæ–¹æ³•æ˜¯è‡ªå›å½’çš„ï¼Œæ‰€ä»¥è®¡ç®—å¤æ‚åº¦ä¼šéšç€å¥å­é•¿åº¦ä¸Šå‡è€Œä¸Šå‡ã€‚ï¼ˆæ¨¡å‹ä¸Šçš„åˆ›æ–°ï¼‰

![image-20220831210727673](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831210727673.png)

ä¹‹å‰çš„SOTAæ–¹æ³•å¯èƒ½ä¼šé—æ¼ä¸€äº›å…³é”®çš„ä¿¡æ¯ï¼Œå¦‚æœæˆ‘æƒ³è¦æ›´åŠ ç»†èŠ‚ç‚¹çš„æè¿°ï¼Œä»–ä»¬æ— æ³•ç”Ÿæˆã€‚

* æ–¹æ³•

  > è¿‡å»ç”±äºæ–¹æ³•æ˜¯è‡ªå›å½’çš„ï¼Œæ‰€ä»¥è®¡ç®—å¤æ‚åº¦ä¼šéšç€å¥å­é•¿åº¦ä¸Šå‡è€Œä¸Šå‡ã€‚åœ¨è¿™é‡Œæå‡ºäº† non-autoregressiveçš„æ–¹æ³•ã€‚

  * è·å–å¥å­é•¿åº¦ä¿¡æ¯ï¼ˆlevel -> $[L_{low}, L_{high}]$ï¼‰åšembedding

  * æå‡ºDecode é˜¶æ®µ (non-autoregressive) **LaBERT**

    * ä½¿ç”¨ä½ç½®ä¿¡æ¯æ¥é¢„æµ‹mask

    * ä½¿ç”¨é•¿åº¦ä¿¡æ¯æ¥é¢„æµ‹unmask

    * æ¨ç†çš„æ—¶å€™é¼“åŠ±ç”Ÿæˆ**æ›´é•¿çš„å¥å­**

      * exponentially decay: $p_i\left(s_i=[\mathrm{EOS}]\right) \leftarrow \gamma^{L_{\text {high }}-i} p_i\left(s_i=[\mathrm{EOS}]\right), \forall i \in\left[L_{\text {low }}, L_{\text {high }}\right]$

        ![image-20220831212334860](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831212334860.png)

      * æ¯ä¸€æ­¥éƒ½ä¼šå¯¹æœ€ä½ç½®ä¿¡åº¦çš„å•è¯è¿›è¡Œmask

:hammer_and_wrench: **Human-like Controllable Image Captioning with Verb-specific Semantic Roles**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Human-Like_Controllable_Image_Captioning_With_Verb-Specific_Semantic_Roles_CVPR_2021_paper.pdf)] [[torch](https://github.com/mad-red/VSR-guided-CIC)]

* ä¸ä¸Šé¢ä¸¤ç¯‡å·¥ä½œå¯æ§æ€§çš„å¯¹æ¯”

![image-20220831213129731](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831213129731.png)

* åŠ¨æœº

  * äº‹ä»¶å…¼å®¹æ€§ï¼Œä¸¤ä¸ªä¸å…¼å®¹çš„äº‹ä»¶ä¸åº”è¯¥åˆåœ¨ä¸€èµ·

  * é‡‡æ ·çš„å…¼å®¹æ€§ï¼Œä¸åˆç†çš„é‡‡æ ·ä¸åº”è¯¥å‡ºç°åœ¨å¥å­å½“ä¸­

  * å¯¹äºä¸Šé¢çš„caseï¼š 

    ```python
    verb=sit, Arg1="thing sitting", Arg2="sitting position" 
    verb=read, Arg0="reader", Arg1="thing read"
    ```

* æ–¹æ³•ä¸Šæ˜¯å…ˆæŠ½å–å‡ºæ¥çº¦æŸçš„æ ‡ç­¾ï¼Œå†decoder

![image-20220831213747447](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831213747447.png)

:star: **MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning**, in AAAI 2022.  [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/20243)]

* åŠ¨æœº
  * ä¸ºäº†captionçš„ç”Ÿæˆæ›´åŠ ä¸°å¯Œå¤šæ ·ï¼Œå¹¶ä¸”**æ— éœ€è¿‡å¤šçš„æ ‡æ³¨æ•°æ®**ï¼
  * captionç›´æ¥åšåˆ°**åœºæ™¯æ–‡æœ¬**çº§åˆ«
* æ–¹æ³•
  * Unpired Captioningçš„æ–¹æ³•ï¼ˆå…¶å®å°±æ˜¯`GAN`çš„æ€æƒ³ï¼‰
  * å­¦åˆ°äº†æ¨¡æ€å†…éƒ¨ï¼Œè·¨æ¨¡æ€ä¹‹é—´çš„å…³ç³»
  * Unpaired å­¦ä¹ çš„èŒƒå¼ï¼Œæ— éœ€è¿‡å¤šç›‘ç£ä¿¡å·

![image-20220904163643769](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220904163643769.png)





:hammer_and_wrench: :fire: **Show, Edit and Tell: A Framework for Editing Image Captions**, in CVPR 2020.  [[pdf](https://arxiv.org/abs/2003.03107)] [[torch](https://github.com/fawazsammani/show-edit-tell)]

* ç›´æ¥å¯¹ç”Ÿæˆçš„captionè¿›è¡Œç¼–è¾‘ä¿®æ”¹

:hammer_and_wrench: **Towards Accurate Text-based Image Captioning with Content Diversity Exploration**, in CVPR 2021. [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Towards_Accurate_Text-Based_Image_Captioning_With_Content_Diversity_Exploration_CVPR_2021_paper.pdf)]  [[torch](https://github.com/guanghuixu/AnchorCaptioner)]

* åŠ¨æœº
  * Captionç”Ÿæˆçš„å¤šæ ·æ€§
  * æŒ‘æˆ˜
    * ä¸çŸ¥é“åº”è¯¥å¦‚ä½•é€‰æ‹©æ–‡æœ¬ä¿¡æ¯
    * æ–‡æœ¬å’Œå›¾ç‰‡ä¹‹é—´çš„å…³ç³»
    * å¤šæ ·æ€§captionçš„ç”Ÿæˆ

![image-20220831221056898](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831221056898.png)



* æ¨¡å‹æ–¹æ³•

![image-20220831221253743](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831221253743.png)



**Improving OCR-based Image Captioning by Incorporating Geometrical Relationship**, in CVPR 2021.  [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf)]

* åŠ¨æœº
  * æ— æ³•å»ºç«‹OCRæŠ½å‡ºæ¥ä¸œè¥¿ä¹‹é—´çš„å…³ç³»
* æ–¹æ³•
  * é€šè¿‡é«˜åº¦ï¼Œå®½åº¦ã€è·ç¦»ã€IoUå’Œæ–¹å‘æ„å»ºç›¸åº”çš„OCR

![image-20220831223252058](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831223252058.png)



 :hammer_and_wrench: **Towards Unique and Informative Captioning of Images**, in ECCV 2020.  [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-58571-6_37.pdf)] [[torch](https://github.com/princetonvisualai/SPICE-U)]

* ç›®å‰é—®é¢˜ï¼š

![image-20220901103845451](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901103845451.png)

* å…³é”®è´¡çŒ®ï¼Œåšäº†ä¸€ä¸ª**æ–°çš„è¯„ä»·æŒ‡æ ‡**

:hammer_and_wrench: **Comprehensive Image Captioning via Scene Graph Decomposition**, in ECCV 2020.  [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-58568-6_13.pdf)] [[torch](https://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html)]

* åœºæ™¯å›¾åˆ†è§£æ¥å®ç°å¤šæ ·æ€§

![image-20220901104357419](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104357419.png)

![image-20220901104406842](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104406842.png)

:hammer_and_wrench: **In Defense of Scene Graphs for Image Captioning**, in ICCV 2021.  [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9710596)]  [[torch](https://github.com/ Kien085/SG2Caps)]

* åŠ¨æœº
  * å¼¥è¡¥æ–‡æœ¬åœºæ™¯å›¾è¿˜æœ‰è§†è§‰åœºæ™¯å›¾ç›´æ¥çš„Gap
  * ä»¥å¾€çš„å·¥ä½œåœ¨è®­ç»ƒcaptioneræ—¶ï¼Œå¾€å¾€ç”¨**TSGä½œä¸ºè¾“å…¥**ï¼Œæµ‹è¯•æ—¶å†æ¢æˆVSG
  * VGæ•°æ®é›†ä¸Šå­¦å¾—çš„åœºæ™¯å›¾ä¸­relationshipå¤šæ˜¯has, onè¿™ç±»**æ— æ„ä¹‰çš„å…³ç³»**
  * VSGä¸TSGå¹¶ä¸å…¼å®¹  ï¼ˆä¸¤ä¸ªåœºæ™¯å›¾ä¹‹é—´ï¼‰

![image-20220831224945278](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831224945278.png)

* åŸºæœ¬æ€æƒ³
  *  close the **semantic gap** between the two scene graphs
  * ä½¿ç”¨**HOIä¿¡æ¯å¢å¼ºVSG**ï¼Œå¹¶å¼•å…¥object locationä¿¡æ¯æå‡VSGçš„è¡¨è¾¾èƒ½åŠ›

![image-20220831224458911](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831224458911.png)

* å…·ä½“æ–¹æ³•

  * VSGæ„å»º
    * VGæ•°æ®é›†è®­ç»ƒä¸€ä¸ªVSG generatorï¼ŒåŒä»¥å¾€å·¥ä½œä¸€æ ·å¯¹MSCOCOä¸­çš„å›¾ç‰‡ç”ŸæˆVSGã€‚ä¸æ­¤åŒæ—¶ï¼Œä½œè€…åˆåœ¨MSCOCOä¸Šè®­ç»ƒäº†ä¸€ä¸ªobject detectorï¼Œå¯¹å›¾ç‰‡æ£€æµ‹å‡ºä¸€ç³»åˆ—çš„ç‰©ä½“ã€‚
  * VSGç¼–ç 
    * éšåä½¿ç”¨HOI inferenceå¯¹ä¸äººç›¸å…³çš„ç‰©ä½“è¿›è¡Œå…³ç³»åŠå±æ€§çš„é¢„æµ‹ã€‚æœ€åå–åŸå§‹VSGä¸HOI (æ£€æµ‹åˆ°çš„ç‰©ä½“) graphçš„å¹¶é›†ä½œä¸ºæœ€ç»ˆVSGã€‚
    * ä½¿ç”¨å¤šä¸ªGCNå¯¹å…¶è¿›è¡Œç¼–ç ï¼Œä¸åŒç±»å‹çš„èŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„GCNå‚æ•°ã€‚
  * decodeé˜¶æ®µ (Up-down)
    * ä»…ä»…ä½¿ç”¨scene graphï¼Œä¸ä½¿ç”¨ä»»ä½•è§†è§‰ç‰¹å¾ï¼ŒSG2Capsæ¨¡å‹ä¾¿å¯ä»¥å–å¾—æœ‰ç«äº‰åŠ›çš„æè¿°ç”Ÿæˆç»“æœã€‚
  * caseå±•ç¤º

  ![image-20220831230157943](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220831230157943.png)



:hammer_and_wrench: **Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning**, in CVPR 2022. [[pdf]()] [[torch](https://github.com/GT-RIPL/Xmodal-Ctx)]

* å…³æ³¨åˆ°æ›´å¤šçº§åˆ«çš„ä¿¡æ¯

![image-20220901105828069](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901105828069.png)

* æ–¹æ³•ä¸Šä¸»è¦åŠ å…¥äº†Crop

![image-20220901105902348](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901105902348.png)

:hammer_and_wrench: **Comprehending and Ordering Semantics for Image Captioning**, in CVPR. [[pdf](https://arxiv.org/pdf/2206.06930.pdf)] [[torch](https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet)]

* **è¯­ä¹‰çš„è¯­è¨€æ’åº**ï¼ˆä¸å•å•æ˜¯å¯¹è±¡ï¼‰åŒæ ·å¾ˆé‡è¦

![image-20220901111757395](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111757395.png)

* æ–¹æ³•

![image-20220901111817536](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111817536.png)

:hammer_and_wrench: **DIFNet: Boosting Visual Information Flow for Image Captioning**, in CVPR 2022.  [[pdf](DIFNet: Boosting Visual Information Flow for Image Captioning)] [[torch](https://github.com/mrwu-mac/DIFNet)]

* è€ƒè™‘äº†ä¿¡æ¯æµçš„ä¿¡æ¯

![image-20220901111002940](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111002940.png)

![image-20220901111014355](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901111014355.png)

:hammer_and_wrench: **Injecting Semantic Concepts into End-to-End Image Captioning**, in CVPR 2022.  [[pdf](https://arxiv.org/abs/2112.05230)]  [[torch](https://github.com/jacobswan1/ViTCAP)]

* ç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œdetector-free å’ŒåŠ å…¥è¯­ä¹‰concept
* è¿‡å»çš„å·¥ä½œ

![image-20220901104826507](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104826507.png)

* åŠ å…¥Concept
  * é€šè¿‡æŠ½å–captionä¸­çš„åŠ¨åè¯æˆ–è€…é€šè¿‡çŸ¥è¯†è’¸é¦å¾—åˆ°ä¸€äº›conceptä½œä¸º**ä¼ªæ ‡ç­¾**åšåˆ†ç±»

![image-20220901104847149](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901104847149.png)



:hammer_and_wrench: **Show, Deconfound and Tell: Image Captioning with Causal Inference**, in CVPR 2022.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Show_Deconfound_and_Tell_Image_Captioning_With_Causal_Inference_CVPR_2022_paper.pdf)] [[torch](https: //github.com/CUMTGG/CIIC)]

* è§£å†³æ•°æ®é›†ä¸­å¤§é‡å‡ºç°äº†ï¼Œæ¨¡å‹**short-cut path** çš„é—®é¢˜

![image-20220901110327977](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901110327977.png)

* ä¸»è¦ä¸ºäº†è§£å†³ä¸¤ä¸ªCaptionå­˜åœ¨çš„é—®é¢˜
  * è¯†åˆ«**å¯¹è±¡é”™è¯¯**ï¼ˆé•¿å¤´å‘çš„ç”·äººè¯†åˆ«æˆäº†å¥³äººï¼‰
  * æè¿°å¾—**ä¸å¤Ÿå…³é”®å’Œè¯¦ç»†**

* Encoderé˜¶æ®µï¼ˆè§£å†³åˆ†ç±»å‡†ç¡®æ€§çš„é—®é¢˜ï¼‰
  * åŸºäºFaster-RCNNå¾—åˆ°æ— åçš„ç‰©ä½“åˆ†ç±»

![image-20220901110456322](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220901110456322.png)

![image-20220915113730411](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915113730411.png)

![image-20220915113654808](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915113654808.png)

* decoderé˜¶æ®µè€ƒè™‘ç”Ÿæˆå•è¯çš„bias

![image-20220915114452498](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915114452498.png)



![image-20220915114520789](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220915114520789.png)



**[å› æœå…³ç³» + å¼ºåŒ–å­¦ä¹ ] Dependent Multi-Task Learning with Causal Intervention for Image Captioning**, in IJCAI 2021.  [[pdf](https://www.ijcai.org/proceedings/2021/0312.pdf)] 

> è¯´å®è¯è¿™ç¯‡è®ºæ–‡å†™ä½œ**æœ‰ç‚¹å¤ªå¤æ‚äº†ï¼Œå¾ˆéš¾çœ‹æ‡‚**

* è§£å†³captionç”Ÿæˆ**åäº‹å®**ä¸**ä¸å¤Ÿè¯¦ç»†**çš„é—®é¢˜

![image-20220914112625963](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220914112625963.png)

* **å› æœå¹²é¢„åˆ†æè¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œè¯¦è§è®ºæ–‡**

![image-20220914112637246](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220914112637246.png)



## :sunglasses: Video Understanding

### :video_camera: Features Fusion

:white_check_mark: :fire: :hammer_and_wrench: **[TSN] Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**, in ECCV 2016.  [[pdf](https://arxiv.org/abs/1608.00859)]  [[torch](https://github.com/yjxiong/temporal-segment-networks)]

* æŠ½å–æ‰€æœ‰å¸§æ˜¯ä¸ç°å®çš„ï¼ŒTSNå°†å…¶**ç­‰é—´éš”åˆ†**ä¸º$K$ä¸ªç‰‡æ®µï¼ˆi.e., $K=16$ï¼‰,åœ¨æ¯ä¸ªç‰‡æ®µä¸­è°å¯„æŠ½å–ä¸€å¸§ä½œä¸ºè¾“å…¥

* æä¾›äº†éå¸¸å¸¸ç”¨çš„æ•°æ®äº‰å¼ºæ–¹å¼å’Œä¸€äº›è®­ç»ƒæ—¶å€™çš„trickï¼ˆä¸»è¦åŒ…æ‹¬location jittering, horizontal flipping, corner cropping, and scale jitteringï¼‰

* ä»ç„¶åˆ©ç”¨åŒæµçš„æ€è·¯ï¼Œè®©æ¯ä¸ªç‰‡æ®µä¿¡æ¯æœ€åé€šè¿‡ä¸€ä¸ªå…±è¯†ç½‘ç»œå†Fusion

  ![](https://pic4.zhimg.com/80/v2-67b66b3618606af8b81d1f77b1f92a3b_1440w.jpg)

:white_check_mark: :fire: :hammer_and_wrench: **[TRN] Temporal Relation Reasoning in Videos**, in ECCV 2018.  [[pdf]()] [[torch](https://github.com/zhoubolei/TRN-pytorch)]

![img](https://pic4.zhimg.com/80/v2-86fa6c271c9d2dfad07d4603ed457a83_1440w.jpg)

* èåˆå°ºåº¦ç¡®å®š (éœ€è¦å¤šå°‘ä¸ªè§†é¢‘å¸§æ¥èåˆ)ã€å¦‚å›¾æ‰€ç¤ºã€‘æœ‰2ï¼Œ3ï¼Œ4è¿™ä¸‰ç§å°ºåº¦
* æ¯ä¸ªå°ºåº¦ä¸‹éœ€è¦å¤šå°‘ç»„è§†é¢‘å¸§
* åœ¨åº”ç”¨å¤šå°ºåº¦TRNçš„æ—¶å€™ï¼Œä¸€èˆ¬ä¼šé¢å¤–å¢åŠ ä¸€ä¸ªå…¨å¸§çš„å°ºåº¦ï¼Œå³12å¸§ç‰¹å¾å…¨éƒ¨concatåˆ°ä¸€èµ·ï¼Œä»¥å……åˆ†åˆ©ç”¨æœ‰æ•ˆä¿¡æ¯ã€‚
* **å¹³è¡¡**æ•ˆæœå’Œè®¡ç®—é€Ÿåº¦ï¼Œ**ç®€å•å¥½ç”¨**

:white_check_mark: :fire: :hammer_and_wrench: **[TSM] TSM: Temporal Shift Module for Efficient Video Understanding**, in ICCV 2019.  [[pdf](https://arxiv.org/abs/1811.08383)] [[torch](https://github.com/mit-han-lab/temporal-shift-module)]

* å¯¹æŸäº›é€šé“shiftï¼Œå¾—åˆ°å‰ä¸€å¸§æˆ–è€…åä¸€å¸§çš„ç‰¹å¾

  ![image-20220709174802714](https://s2.loli.net/2022/07/09/cb1f8LWpV9JotO3.png)

* ç”±äºshiftæ˜¯æœ‰æŸå¤±çš„ï¼Œä¸ºæ­¤è®¾è®¡æ®‹å·®æ¥è¿›è¡Œå¼¥è¡¥ï¼ˆåŸæ¥çš„ä¸æ®‹å·®çš„å¯¹æ¯”ï¼‰

![image-20220709174842935](https://s2.loli.net/2022/07/09/2dTjLBJwQ5FUber.png)

:white_check_mark: :fire: :hammer_and_wrench: **[LRCN] Long-term Recurrent Convolutional Networks for Visual Recognition and Description**, in CVPR 2015.  [[pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf)] [[torch](https://github.com/garythung/torch-lrcn)]

* CNNæŠ½å‡ºæ¥çš„å¸§ç‰¹å¾å†æ”¾è¿›å»`LSTM`å¾—åˆ°æ¯å¸§çš„æ—¶åºç‰¹å¾

> å…³äºè§†é¢‘ç‰¹å¾æŠ½å–ï¼Œä¸‹é¢è®²ä¸€ä¸‹`netvlad`ç³»åˆ—çš„ç»“æ„,NextVladå°±æ˜¯ä¸“é—¨é’ˆå¯¹è§†é¢‘å¸§èåˆæ¥åšçš„ä¼˜åŒ–ã€‚
>
> [ç›¸å…³åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/385512915)

:fire: :hammer_and_wrench: **[NetVLAD] NetVLAD: CNN architecture for weakly supervised place recognition**, in CVPR 2016.  [[pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)] [[torch (simple)](https://github.com/lyakaap/NetVLAD-pytorch)]

* VLADç®—æ³•ï¼ˆå®é™…ä¸Šå°±æ˜¯Kmeansï¼‰ï¼š
  $$
  V(j, k)=\sum_{i=1}^{N} a_{k}\left(x_{i}\right)\left(x_{i}(j)-c_{k}(j)\right), \quad k \in K, j \in D
  $$

* æœ¬æ–‡ä½¿ç”¨`CNN`æ¨¡æ‹Ÿè¯¥VLADç®—æ³•çš„è¿‡ç¨‹

  * å¹³æ»‘åŒ–$\alpha$ ä½¿å…¶å˜æˆä¸€ä¸ª0-1åˆ†å¸ƒçš„æƒé‡å‚æ•°ï¼Œä½¿ç”¨$1 \times 1$å·ç§¯+softmax è¿›è¡Œè¯¥è¿‡ç¨‹ï¼Œå¹³æ»‘æ¨å¯¼å…¬å¼$\bar{a}_{k}\left(\mathbf{x}_{i}\right)=\frac{e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k}\right\|^{2}}}{\sum_{k^{\prime}} e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k^{\prime}}\right\|^{2}}}$

  

  ![image-20220712104829439](https://s2.loli.net/2022/07/12/aFlJSCw8dvBnzEH.png)

   

:fire: :hammer_and_wrench: **[NextVLAD] NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**, in ECCV workshop 2018.  [[pdf](https://arxiv.org/pdf/1811.05014.pdf)] [[tensorflow](https://github.com/linrongc/youtube-8m)]

> åŒæ—¶ï¼Œè¿˜æœ‰[å…³äºå¤šæ¨¡æ€ï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰Transformeræ¨¡å‹çš„åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/388361095)





### :timer_clock: Temporal Grounding

> æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªååˆ†ç»å…¸çš„ä»»åŠ¡ï¼ˆTemporal Groundingï¼‰æ¥çœ‹çœ‹è§†é¢‘çš„ç‰¹å¾æ˜¯å¦‚ä½•åˆ©ç”¨çš„

:fire::hammer_and_wrench: **[Video-NLP] Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language**, in AAAI 2020.  [[pdf](https://arxiv.org/pdf/1912.03590.pdf)] [[torch](https://github.com/microsoft/VideoX)]

> extend version: **MS-2D-TAN**, in TPAMI 2021.  [[pdf](https://arxiv.org/pdf/2012.02646.pdf)]  [[torch](https://github.com/microsoft/VideoX)]

* 2D: start time & end time æ„æˆçš„é‚»æ¥çŸ©é˜µ

![image-20220723153302775](https://s2.loli.net/2022/07/23/lUBL6uPHRFCzvxt.png)

* **æ ¸å¿ƒæ€æƒ³**ï¼š

  * ä½¿ç”¨max pooling ï¼ˆæœ¬æ–‡ä½¿ç”¨ï¼‰ æˆ–è€… stack convolutionçš„è·å–moment featureï¼Œå¦‚ä¸Šå›¾`2D Temporal Feature Map Extraction`æ‰€ç¤º
  * ç”±äºè¿™æ ·å­çš„è®¡ç®—å¼€é”€å¤ªå¤§äº†ï¼Œä½¿ç”¨**ç‰¹å®šçš„é‡‡æ ·æ–¹å¼**è¿›è¡Œè°ƒæ•´ï¼Œè¯¦è§è®ºæ–‡ï¼ï¼ˆè·ç¦»è¿‘çš„é‡‡æ ·å¤šä¸€ç‚¹ï¼Œè¿œçš„é‡‡æ ·å°‘ä¸€ç‚¹ï¼‰
  * å¤šæ¨¡æ€èåˆï¼ˆ`Hadamard product`ï¼‰

  $$
  \mathbf{F}=\left\|\left(\mathbf{w}^{S} \cdot \mathbf{f}^{S} \cdot \mathbb{1}^{T}\right) \odot\left(\mathbf{W}^{M} \cdot \mathbf{F}^{M}\right)\right\|_{F}
  $$

  * æŸå¤±è®¡ç®—æ—¶å€™ï¼Œå¯¹`IoU`è¿›è¡Œä¸€ä¸ªscaleå˜æˆç›‘ç£ä¿¡å·

  $$
  y_{i}= \begin{cases}0 & o_{i} \leq t_{\min } \\ \frac{o_{i}-t_{\min }}{t_{\max }-t_{\min }} & t_{\min }<o_{i}<t_{\max } \\ 1 & o_{i} \geq t_{\max }\end{cases}
  $$

  * BCE loss:

  $$
  L o s s=\frac{1}{C} \sum_{i=1}^{C} y_{i} \log p_{i}+\left(1-y_{i}\right) \log \left(1-p_{i}\right)
  $$

  

:fire: :hammer_and_wrench: **Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding**, in AAAI 2022. [[pdf](https://arxiv.org/abs/2109.04872)] [[torch](https://github.com/MCG-NJU/MMN)] [[blog](https://zhuanlan.zhihu.com/p/446203594)]

* ä¸»å¹²ç½‘ç»œæ˜¯æ²¿ç”¨[TDN](https://arxiv.org/abs/2012.10071)

* ä½¿ç”¨äº†**metric learning**çš„æ–¹æ³•å¹¶ä¸”å¼•å…¥**è´Ÿæ ·æœ¬**æ¥åšTemporal Groundingçš„ä»»åŠ¡

  * è§†é¢‘é—´çš„è´Ÿæ ·æœ¬ï¼ˆ`IoU`æ¥æ ‡å®šç›‘ç£ä¿¡å·`yi`ï¼Œä¸`2D-TAN`ä¸€æ ·å¤„ç†å¾—æ¥çš„ï¼Œè®°å¾—`scale`ä¸€ä¸‹ï¼‰
  * æ–‡æœ¬ä¸­çš„è´Ÿæ ·æœ¬ï¼Œä»å…¶ä»–è§†é¢‘çš„æ–‡æœ¬è¯­å¥å½“ä¸­é€‰å–å‡ºæ¥

* è´¡çŒ®

  * æ„é€ äº†æ–°çš„ç›‘ç£ä¿¡å·ï¼šè§†é¢‘é—´çš„æ­£è´Ÿæ ·æœ¬(`IoU`æ¥é‡‡æ ·)ï¼Œ å¥å­å’Œè§†é¢‘å¯¹åº”çš„æ­£è´Ÿæ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬å¥å­ä»åˆ«çš„è§†é¢‘æŠ½å–è¿‡æ¥ï¼‰
  * ä¸€ä¸ªè§†é¢‘åªéœ€è¦å»ºæ¨¡ä¸€æ¬¡ï¼Œå¤§å¤§èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œä»¥å¾€çš„fusionæ–¹æ³•éƒ½æ˜¯è¦æ–‡æœ¬-è§†é¢‘å¸§å¯¹æ¥å»ºæ¨¡

* Trick

  * ä¸ºäº†ç¼–ç å…¬å¹³ï¼Œä½¿ç”¨é¢„è®­ç»ƒå¥½çš„`DistilBERT`æ¥è¿›è¡Œç¼–ç å¥å­

* æŸå¤±å‡½æ•°è®¡ç®—

  * å’Œ`2D-TDN`ä¸€æ ·çš„`BCE_loss`
  * ç±»ä¼¼äº`InfoNCE loss`çš„è®¾è®¡å¯¹æ¯”æŸå¤±

  $$
  \begin{aligned}
  &p\left(i_{s} \mid v\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)}{\exp \left(\left(\mathbf{f}_{i}^{S T} \mathbf{f}^{V}-m\right) / \tau_{v}\right)+\sum_{j \neq i}^{N_{s}} \exp \left(\mathbf{f}_{j}^{S T} \mathbf{f}^{V} / \tau_{v}\right)} \\
  &p\left(i_{v} \mid s\right)=\frac{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)}{\exp \left(\left(\mathbf{f}_{i}^{V T} \mathbf{f}^{S}-m\right) / \tau_{s}\right)+\sum_{j \neq i}^{N_{v}} \exp \left(\mathbf{f}_{j}^{V T} \mathbf{f}^{S} / \tau_{s}\right)} \\
  &L_{m m}=-\left(\sum_{i=1}^{N} \log p\left(i_{v} \mid s_{i}\right)+\sum_{i=1}^{N} \log p\left(i_{s} \mid v_{i}\right)\right)
  \end{aligned}
  $$

  

### :man_student: Video Question Answer

**Invariant Grounding for Video Question Answering**, in CVPR 2022 oral.  [[pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Invariant_Grounding_for_Video_Question_Answering_CVPR_2022_paper.pdf)] [[torch](https://github.com/yl3800/IGV)]

> è¿™ç¯‡æ–‡ç« æ„Ÿè§‰æ˜¯ä¸€ç¯‡å¾ˆæ ‡å‡†çš„`CVPR`çš„ä¸­è§„ä¸­çŸ©æ–‡ç« ï¼Œå†™ä½œç”¨è¯ä¸Šéå¸¸å‡ºè‰²çš„

* å…ˆåšäº†Groundingçš„æ£€æµ‹ï¼Œæ£€æµ‹å‡ºé—®é¢˜ç›¸å…³å¸§ï¼ˆæœ‰å› æœå…³ç³»`Casual`ï¼‰è¿˜æœ‰æ— å…³å¸§ï¼ˆè¡¥å¿å¸§`Complement`ï¼‰
* æ„å»ºè´Ÿæ ·æœ¬åˆ°æ— å…³å¸§å½“ä¸­ï¼Œä½¿ç”¨`memory bank`æ¥å­˜å‚¨æ‰€æœ‰æ ·æœ¬ (å› æ­¤è¦æ³¨æ„å­˜å‚¨çš„ç‰¹å¾ç»´åº¦ä¸èƒ½å¤ªå¤§)

**Video as Conditional Graph Hierarchy for Multi-Granular Question Answering**ï¼Œin AAAI 2022. [[pdf](https://arxiv.org/abs/2112.06197)] [[torch](https://arxiv.org/abs/2112.06197)]

* ç°æœ‰çš„æ–¹æ³•å¯¹è§†é¢‘é—®é¢˜çš„å›ç­”ç¼ºä¹**å¯è§£é‡Šæ€§**
* æ„å»ºäº†ä¸¤ç§è§†è§’æ¥çœ‹é—®é¢˜
  * bottom-upï¼Œ ä¸åŒçš„è§†é¢‘ç‰¹å¾å†³å®šäº†ä¸åŒçš„å±æ€§levelï¼ˆå®ä½“ï¼ŒåŸå­ï¼ŒåŠ¨ä½œï¼Œäº‹ä»¶ï¼‰
  * up-bottomï¼Œé—®é¢˜ä¸­çš„ä¸åŒå•è¯ï¼Œå…³è”äº†ä¸åŒçš„level
* æ„å»º**å›¾ç¥ç»ç½‘ç»œ**æ¥æ¨¡æ‹Ÿè¿™äº›levelæ€è€ƒçš„è¿‡ç¨‹

![image-20220910231620242](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220910231620242.png)

:fire: :hammer_and_wrench: **[äº¤é€šäº‹æ•…QAæ•°æ®é›†] SUTD-TraffificQA: A Question Answering Benchmark and an Effificient Network for Video Reasoning over Traffific Events**, in CVPR 2021. [[pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_SUTD-TrafficQA_A_Question_Answering_Benchmark_and_an_Efficient_Network_for_CVPR_2021_paper.pdf)] [[project](https://github.com/SUTDCV/SUTD-TrafficQA[)]

* å¯¹æ¯”èµ·å…¶ä»–QAæ•°æ®é›†
  * éœ€è¦æ¨¡å‹æœ‰å› æœæ¨ç†å’Œè®¤çŸ¥å‘å±•ï¼ˆcognitive developmentï¼‰
* å¤„ç†æ–¹æ³•
  * å¯¹ç²—ç»†ä¸¤ç§ç²’åº¦è¿›è¡Œè¯†åˆ«å’Œè®¡ç®—ï¼ˆä¸åŒçš„CNNç½‘ç»œï¼‰ï¼Œå¤§å¤§åŠ å¿«äº†æ¨¡å‹çš„è¿ç®—æ—¶é—´

![image-20220912152326067](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912152326067.png)

**Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering**, in TPAMI 2022. [[pdf](https://arxiv.org/pdf/2207.12647.pdf)]

> è¿™ç¯‡è®ºæ–‡æ¨¡å‹è¾ƒä¸ºå¤æ‚ï¼Œæ‰€ä»¥è¿™é‡Œåªè®²è¯‰å…¶æ ¸å¿ƒæ€æƒ³

* åŠ¨æœº

  * ç°æœ‰æ–¹æ³•åªå…³æ³¨äº†å¾ˆç®€å•çš„äº‹ä»¶ï¼Œæ¯”å¦‚è¯´çœ‹ç”µå½±ï¼Œæ— æ³•å…³æ³¨çœŸæ­£äº‹ä»¶çº§çš„å› æœå…³ç³»

    ![image-20220912224156198](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224156198.png)

  * è¯­è¨€å’Œå›¾åƒå½“ä¸­çš„å¹²æ‰°å› ç´ ï¼ˆConfounderï¼‰

    * è¿‡äºå…³æ³¨ä¸€äº›æ˜¾å¼çš„ä¸œè¥¿ï¼Œå¿½ç•¥äº†ä¸€äº›å¾ˆé‡è¦çš„ä¸œè¥¿

  ![image-20220912224356724](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224356724.png)

* æ–¹æ³•

![image-20220912224635966](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220912224635966.png)

* è¯¦ç»†ç»†èŠ‚è§è®ºæ–‡ï¼



### :writing_hand: Video Caption

**[Video Caption] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2101.12059)]

:hammer_and_wrench: :fire: **[Video Caption] Robust Change Captioning**, in ICCV 2019. [[pdf](https://arxiv.org/pdf/1901.02527.pdf)] [[torch](https://github.com/Seth-Park/RobustChangeCaptioning)]

* è¾“å…¥ä¸ºå‰åå›¾åƒå¯¹ï¼Œäº”ç§å˜åŒ–ç±»å‹ï¼ˆcolor/material change,adding/dropping/moving an objectï¼‰
* æå‡ºä¸€ä¸ªæœ‰è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[CLEVR-Change](https://cs.stanford.edu/people/jcjohns/clevr/)ï¼ˆ80Kå›¾ç‰‡å¯¹ï¼‰ï¼Œå¹¶åœ¨æ— è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[Spot-the-Diff](https://github.com/harsh19/spot-the-diff)å–å¾—SOTAæ•ˆæœã€‚
* æ¨¡å‹ï¼šDual æ³¨æ„åŠ›ï¼Œ åˆ†è¾¨**è§†ç‚¹å˜åŒ–**ï¼Œ å…¶å®æ˜¯é€šè¿‡è¾“å…¥ä¸¤å¼ å·®ä¸å¤šçš„å›¾ç‰‡ï¼Œæå‰æ ‡å®šå¥½æ•°æ®é›†è·å¾—çš„ï¼Œæœ‰ç‚¹è¢«å‘çš„æ„æ€![image-20220522213419579](https://s2.loli.net/2022/05/22/fiUArgZIjlzw4p1.png)

:hammer_and_wrench: :fire: **[Video Caption] Semantic Grouping Network for Video Captioning**, in AAAI 2021. [[pdf](https://arxiv.org/pdf/2102.00831.pdf)] [[torch](https://github.com/hobincar/SGN)]

* ![image-20220621204108736](https://s2.loli.net/2022/06/21/DMmzxs7dKwyU6BE.png)

:hammer_and_wrench: :fire: **Hierarchical Context-aware Network for Dense Video Event Captioning**, in ACL 2021. [[pdf](https://aclanthology.org/2021.acl-long.156/)] [[torch](https://github.com/KirkGuo/HCN)]

* **å±€éƒ¨ä¿¡æ¯**+**å…¨å±€ä¿¡æ¯**ç»“åˆç”Ÿæˆdense caption ï¼ˆè¾“å…¥åŒ…æ‹¬**video** å’Œ **transcript**ï¼‰
* ä¸ºæ­¤è®¾è®¡äº†ä¸¤å¥—`Attention`æœºåˆ¶
  * falt attention + cross attention

![image-20220820000708776](https://s2.loli.net/2022/08/20/wdO5eWZcxgqsItT.png)

* è®¾è®¡äº†é—¨æœºåˆ¶æ¥decodeï¼ˆä¹‹å‰çš„æ–‡æœ¬ä¿¡æ¯ä¸æœªæ¥çš„æ–‡æœ¬ä¿¡æ¯ï¼‰



```mermaid
  graph LR
  SG(Semantic-Grouping) --å»æ‰å†—ä½™phrase--> ç›¸ä¼¼åº¦è®¡ç®—
  SG --attentionæœºåˆ¶ --> å¯¹å…¶phraseå’Œframe --> åŠ å…¥å¯¹æ¯”æŸå¤±,è®¡ç®—æ²¡æœ‰åŒ…å«negativeçš„æ¦‚ç‡
```

**å¯¹æ¯”æŸå¤±**$\mathcal{L}_{c a}=\sum_{(V, Y) \in \mathcal{D}} \sum_{t} \sum_{i}^{M_{t}}\left(-\log p_{c a}\left(s_{i, t}\right)\right)$, $p_{c a}\left(s_{i, t}\right)=\sum_{j=1}^{N} \alpha_{i, j, t}^{p o s}$    ($\alpha^{pos}$ ä¸ºæ­£æ ·æœ¬æ—¶å€™å¯¹é½æ³¨æ„åŠ›çš„æƒé‡) 



##  :apple: Causality Learning

:fire: :hammer_and_wrench: **Distilling Causal Effect of Data in Class-Incremental Learning**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2103.01737)] [[torch](https://github.com/JoyHuYY1412/DDE_CIL)]

* [[æ¨¡å‹å…¬å¼è§£é‡Š](https://zhuanlan.zhihu.com/p/358340627)]  [[è®ºæ–‡ä»‹ç»](https://www.163.com/dy/article/G4OHT10U0511DPVD.html)]

* åŠ¨æœº

  * å¯¹æ’èŠ‚ç‚¹çš„å­˜åœ¨ä½¿å¾—æ¨¡å‹å¯¹æ–°æ•°æ®ä¼šäº§ç”Ÿç¾éš¾æ€§é—å¿˜

  * è¿‡å»çš„æ–¹æ³•å½“ä¸­

    ![image-20220918101920897](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220918101920897.png)

    ![img](https://pic1.zhimg.com/80/v2-0a7452df24b00f8e7d186f0dcf0c0680_720w.jpg)

    

    		* **data reply**: éœ€è¦è¾ƒå¤§çš„**å­˜å‚¨**ç©ºé—´ï¼›**distillation**: ä¸æ˜¯**ç«¯åˆ°ç«¯**çš„è¡¨ç¤ºå­¦ä¹ ã€‚å› æ­¤ä½œè€…è€ƒè™‘å§æ˜¯å¦å­˜åœ¨ä¸€ç§ç«¯åˆ°ç«¯å½±å“çš„è’¸é¦æ–¹æ³•ã€‚

* æ€è·¯

  1. æ„å»º**å› æœè§’åº¦ä¸‹çš„ç±»åˆ«å¢é‡å­¦ä¹ **è¿‡ç¨‹
  2. åˆ†æ**ç¾éš¾æ€§é—å¿˜å‘ç”Ÿçš„åŸå› **ï¼ˆ**causal effect** lostï¼‰
  3. åˆ†æç°æœ‰å·¥ä½œå¦‚ä½•å®ç°æœ‰æ•ˆçš„æŠ—é—å¿˜ã€‚ åœ¨è¿™äº›åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‘ç°**æ§åˆ¶å¯¹æ’èŠ‚ç‚¹**æ˜¯ä¸€ç§å°šæœªåˆ©ç”¨ã€ä½†éå¸¸æœ‰æ•ˆçš„æŠ—é—å¿˜æ–¹æ³•ï¼Œåœ¨å„ç§ç±»åˆ«å¢é‡å­¦ä¹ çš„è®¾å®šä¸Šå–å¾—äº†ç¨³å®šçš„æå‡ã€‚
  4. åŒæ—¶è§£å†³äº†æ•°æ®ï¼ˆæ–°æ—§ç±»åˆ«ï¼‰é‡‡æ ·åˆ†å¸ƒ**ä¸å‡åŒ€**å¯¼è‡´çš„**bias**é—®é¢˜ã€‚

* **æ–‡ç« ç»†èŠ‚è¯¦è§å¼€å¤´çš„åšå®¢é“¾æ¥**

  

**Learning Causal Effects on Hypergraphs**, Best Paper of KDD 2022. [[pdf](https://arxiv.org/pdf/2207.04049.pdf)]

* **è¶…å›¾**ä¸Šè¿›è¡Œå› æœå½±å“åˆ†æï¼Œå¯¹æ¯”ä¼ ç»Ÿçš„ä»¥**ä¸¤ä¸¤èŠ‚ç‚¹**ä¸ºå›¾çš„æ›´åŠ æœ‰æ„ä¹‰ã€‚
* [[åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/564481108)] [[æ–¹æ³•ç»†èŠ‚](https://zhuanlan.zhihu.com/p/567996036)]



## :abc: Scene Text Recognization

:hammer_and_wrench: **From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network**, in ICCV 2021. [[pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2108.09661)] [[torch](https://link.zhihu.com/?target=https%3A//github.com/wangyuxin87/VisionLAN)]

* è¿‡å»çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«éœ€è¦ï¼šè§†è§‰ç‰¹å¾æŠ½å–å™¨ + è¯­è¨€æ¨¡å‹

* æœ¬æ–‡ç›´æ¥åœ¨è§†è§‰ç©ºé—´è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼ˆç±»ä¼¼äººç±»ï¼Œè¯­è¨€ä¿¡æ¯æ˜¯å¯ä»¥å­¦ä¹ çš„ï¼‰
  * å¯¹å­—ç¬¦çº§åˆ«çš„Maskæ“ä½œ![image-20220701212346925](https://s2.loli.net/2022/07/01/ZLFUIkb41S782GD.png)
    * è®­ç»ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨å¼±ç›‘ç£äº’è¡¥å­¦ä¹ ![image-20220701212430601](https://s2.loli.net/2022/07/01/kc3K7XxAN6SfRut.png)

:hammer_and_wrench: **Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition**, in AAAI 2022.  [[pdf]([https://arxiv.org/abs/2112.12916](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2112.12916))] [[torch]([https://github.com/adeline-cs/GTR](https://link.zhihu.com/?target=https%3A//github.com/adeline-cs/GTR))]
* åŠ å…¥ä¸€ä¸ªGCNå¼ºåŒ–äº†è§†è§‰å­¦ä¹ çš„è¿‡ç¨‹ï¼Œå¹¶ä¸”åšäº†ä¸€ä¸ªfusion



## :label: NER

>  Named Entity Recognition

**å½“å‰ç«èµ›NERä»»åŠ¡çš„baselineï¼š**

- BERT + BILSTM + CRF
- [åšå®¢è¿æ¥ NERé“æ‰“çš„baseline](https://zhuanlan.zhihu.com/p/166496466)

:fire: **Bidirectional LSTM-CRF Models for Sequence Tagging**, in 2015. [[pdf](https://arxiv.org/pdf/1508.01991v1.pdf)] [[code](https://paperswithcode.com/paper/bidirectional-lstm-crf-models-for-sequence)

* ä½¿ç”¨BiLSTM+CRFåšNERçš„å¼€å±±ä¹‹ä½œ
* [ç›¸å…³åšå®¢è¿æ¥](https://zhuanlan.zhihu.com/p/166496466)

:hammer_and_wrench: :fire: **Fast and Accurate Entity Recognition with Iterated Dilated Convolutions**, in EMNLP 2017.  [[pdf](https://arxiv.org/pdf/1702.02098.pdf)] [[tensorflow](https://github.com/iesl/dilated-cnn-ner)]

* Iterated Dilated Convolutions ç©ºæ´å·ç§¯ 


![image-20220825002628063](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825002628063.png)

* æ ¸å¿ƒæ€æƒ³

  * ä¼ ç»Ÿå·ç§¯çš„é—®é¢˜ï¼špoolingå±‚ä¼š**æŸå¤±ä¿¡æ¯**ï¼Œé™ä½ç²¾åº¦ã€‚é‚£ä¹ˆä¸åŠ poolingå±‚ä¼šä½¿**æ„Ÿå—é‡å˜å°**ï¼Œå­¦ä¸åˆ°å…¨å±€çš„ç‰¹å¾ã€‚å¦‚æœå•çº¯çš„å»æ‰poolingå±‚ã€æ‰©å¤§å·ç§¯æ ¸çš„è¯ï¼Œè¿™æ ·çº¯ç²¹çš„æ‰©å¤§å·ç§¯æ ¸åŠ¿å¿…å¯¼è‡´**è®¡ç®—é‡çš„å¢å¤§**ã€‚
  * CNNä¹Ÿå¯ä»¥è§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜

![image-20220825004251541](https://raw.githubusercontent.com/Gary-code/pic/main/img/image-20220825004251541.png)

* é€Ÿåº¦æ¯”ä»¥å‰çš„`Bi-LSTM-CRF`å¿«äº†éå¸¸å¤šï¼Œè€Œä¸”ç²¾åº¦æ²¡æœ‰ä¸‹æ»‘

:hammer_and_wrench: :fire: **BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision** , in KDD 2020.  [[pdf](https://arxiv.org/pdf/2006.15509.pdf)] [[torch](https://github.com/cliang1453/BOND)]

* è¿œè·ç¦»ç›‘ç£çš„é—®é¢˜

  * å™ªå£°ï¼Œè´Ÿæ ·æœ¬ä¸å¥½ç”Ÿæˆ
  * å®Œæ•´æ€§ä¸è¶³
  * `trade-off` åœ¨æ ‡æ³¨å‡†ç¡®åº¦å’Œè¦†ç›–èŒƒå›´ä¹‹é—´

* æƒ³æ³•

  * ç¬¬ä¸€é˜¶æ®µä½¿ç”¨`RoBERTa`å¾®è°ƒï¼Œé€‚åº”`NER`ä»»åŠ¡

    * ä½¿ç”¨`Early stopping` æ–¹æ³•é˜²æ­¢æ•°æ®è¿‡æ‹Ÿåˆè¿˜æœ‰å¯¹**æœªçŸ¥æ•°æ®å¢å¼ºæ³›åŒ–èƒ½åŠ›**
    * é¦–å…ˆé€šè¿‡`POS`è¯†åˆ«æ½œåœ¨å®ä½“ï¼Œç„¶åé€šè¿‡è¯­æ–™åº“è®¡ç®—æœ€å°æŸå¤±ç¡®å®šå®ä½“

    ![img](https://pic2.zhimg.com/80/v2-0951f0afefa53b2efc269f92136a3ae9_720w.jpg)

  * ç¬¬äºŒé˜¶æ®µè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ ï¼ˆteacher-studentæ¨¡å‹2ï¼‰

    * åº”å¯¹**å˜ˆæ‚å’Œä¸å®Œæ•´æ ‡æ³¨**çš„æŒ‘æˆ˜
    * teacherç”Ÿæˆä¼ªæ ‡ç­¾äº¤ç»™studentå»é¢„æµ‹
    * **é‡æ–°åŠ æƒçš„é«˜ç½®ä¿¡åº¦è½¯æ ‡ç­¾**
    * ==ç¬¬äºŒé˜¶æ®µçš„æ ¸å¿ƒå°±æ˜¯å¢å¼ºæ•°æ®çš„ç½®ä¿¡ç¨‹åº¦ï¼==

![img](https://pic4.zhimg.com/80/v2-8788aa61ea19b041e763b597d844ebcb_720w.jpg)

ä¸¤é˜¶æ®µçš„BONDæ¡†æ¶

* åœ¨é˜¶æ®µIä¸­ï¼Œç»è¿‡é¢„è®­ç»ƒçš„BERTç”¨äºæ—©åœçš„è¿œè·ç¦»NERä»»åŠ¡
* åœ¨é˜¶æ®µIIä¸­ï¼Œé¦–å…ˆä»é˜¶æ®µIä¸­å­¦ä¹ çš„æ¨¡å‹åˆå§‹åŒ–studentæ¨¡å‹å’Œteacheræ¨¡å‹ã€‚ç„¶åä½¿ç”¨teacheræ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹studentæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚åŒæ—¶ï¼Œç”±æ—©åœçš„studentè¿­ä»£æ›´æ–°teacheræ¨¡å‹ã€‚



:hammer_and_wrench: :fire: **[A Boundary-aware Neural Model for Nested Named Entity Recognition](https://aclanthology.org/D19-1034.pdf)** , in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1034.pdf)] [[torch](https://github.com/thecharm/boundary-aware-nested-ner)]

* è§£å†³`Nested NER`çš„é—®é¢˜
* ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•
  * é¢„æµ‹è¾¹ç•Œ
  * é¢„æµ‹å®ä½“åˆ†ç±»

:hammer_and_wrench: :fire: **Cross-Domain NER using Cross-Domain Language** , in ACL 2020.  [[pdf](https://aclanthology.org/P19-1236/)] [[torch](https://github.com/jiachenwestlake/Cross-Domain_NER)]

* è§£å†³**è·¨é¢†åŸŸæ— ç›‘ç£**çš„æ ‡æ³¨é—®é¢˜
* æ ¸å¿ƒæ€æƒ³

![img](https://pic4.zhimg.com/80/v2-52cb3242e0a708e48b29ab2f8d81e027_720w.jpg)

æœ€åº•ä¸‹çš„ä¸€å±‚æ˜¯æ•°æ®å±‚ï¼Œæ ‡å‡†æƒ…å†µä¸€ä¸‹æ€»å…±æœ‰å››ä»½è¯­æ–™ï¼Œåˆ†åˆ«å¯¹åº”**ä¸¤ä¸ªdomainä¸‹çš„ä¸¤ä¸ªtaskï¼ˆNERå’Œè¯­è¨€å»ºæ¨¡ï¼‰**ã€‚å…¶ä¸­Source Domainï¼ˆå³ä¿è¯æœ‰æ ‡è®°æ•°æ®ç”¨äºNERçš„domainï¼‰å¯¹åº”ä¹‹å‰æåˆ°çš„News Domainï¼Œå› ä¸ºè®ºæ–‡ä¸­Source Domainä½¿ç”¨çš„æ˜¯æ–°é—»æ•°æ®ã€‚å¦å¤–å¦‚æœæ˜¯æ— ç›‘ç£æŠ½å–Target Domainæ•°æ®åˆ™åªæœ‰ä¸‰ä»½è¯­æ–™ã€‚

ç”±åº•å‘ä¸Šç¬¬äºŒå±‚æ˜¯Word Embeddingå±‚ï¼Œè®ºæ–‡ä¸­çš„Word Embeddingç»“åˆäº†è¯çº§åˆ«å’Œå­—ç¬¦çº§åˆ«çš„å‘é‡è¡¨ç¤ºã€‚å³æŠŠè¯å‘é‡å’Œä¸€ä¸ªè¯çš„å­—ç¬¦åºåˆ—å½¢æˆçš„çŸ©é˜µç»è¿‡CNNå¤„ç†åçš„**å‘é‡concatenateèµ·æ¥**ã€‚

ç¬¬ä¸‰å±‚æ˜¯åŒå‘LSTMï¼Œç”¨äºåºåˆ—å¤„ç†ç¬¬äºŒå±‚çš„æ•°æ®ï¼Œç”Ÿæˆå‰åå‘hidden stateã€‚

ç¬¬å››å±‚LMå’ŒCRFï¼Œå³task modelå±‚ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬å››å±‚æœ‰ä¸‰ä¸ªæ¨¡å—ï¼Œä¸¤ä¸ªæ˜¯ç”¨äºNERçš„CRFæ¨¡å‹ï¼Œåˆ†åˆ«å¯¹åº”Source Domainå’ŒTarget Domainã€‚å¦ä¸€ä¸ªæ˜¯åŸºäºç¬¬ä¸‰å±‚BiLSTMçš„è¯­è¨€æ¨¡å‹ï¼ŒNSSoftmaxæ˜¯æŒ‡è¿™ä¸ªè¯­è¨€æ¨¡å‹åˆ©ç”¨**Negative Sampling Softmaxçš„æ–¹å¼è¿›è¡Œè®­ç»ƒ**ã€‚



* æœ€å…³é”®çš„åœ°æ–¹å°±æ˜¯`Bi-LSTM`çš„å‚æ•°æ˜¯ç”Ÿæˆçš„ï¼Œä¸åŒdomainçš„ä¸åŒtaskéœ€è¦çš„LSTMçš„å‚æ•°å’Œ$I$æœ‰å…³

$$
\begin{equation}
\theta_{\mathrm{LSTM}}^{d, t}=\mathbf{W} \otimes \mathbf{I}_{d}^{D} \otimes \mathbf{I}_{t}^{T},
\end{equation}
$$





**NERçš„æœªæ¥**

æ—¢ç„¶æ¨¡å‹æ‰“ä¸åŠ¨äº†ï¼Œç„¶åæˆ‘æ‰¾äº†æ‰¾ ACL2020åšNERçš„è®ºæ–‡ï¼Œçœ‹çœ‹ç°åœ¨çš„NERè¿˜åœ¨åšå“ªäº›äº‹æƒ…ï¼Œä¸»è¦åˆ†å‡ ä¸ªæ–¹é¢

1. **å¤šç‰¹å¾**ï¼šå®ä½“è¯†åˆ«ä¸æ˜¯ä¸€ä¸ªç‰¹åˆ«å¤æ‚çš„ä»»åŠ¡ï¼Œä¸éœ€è¦å¤ªæ·±å…¥çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå°±æ˜¯åŠ ç‰¹å¾ï¼Œç‰¹å¾è¶Šå¤šæ•ˆæœè¶Šå¥½ï¼Œæ‰€ä»¥å­—ç‰¹å¾ã€è¯ç‰¹å¾ã€è¯æ€§ç‰¹å¾ã€å¥æ³•ç‰¹å¾ã€KGè¡¨å¾ç­‰ç­‰çš„å°±ä¸€ä¸ªä¸ªåŠ å§ï¼Œç”šè‡³æœ‰äº›ä¸­æ–‡ NER ä»»åŠ¡é‡Œè¿˜åŠ å…¥äº†æ‹¼éŸ³ç‰¹å¾ã€ç¬”ç”»ç‰¹å¾...... å¿ƒæœ‰å¤šå¤§ï¼Œç‰¹å¾å°±æœ‰å¤šå¤š
2. **å¤šä»»åŠ¡**ï¼šå¾ˆå¤šæ—¶å€™åš NER çš„ç›®çš„å¹¶ä¸ä»…æ˜¯ä¸ºäº† NERï¼Œè€Œæ˜¯æœåŠ¡äºä¸€ä¸ªæ›´å¤§çš„ç›®æ ‡æˆ–ç³»ç»Ÿï¼Œæ¯”å¦‚ä¿¡æ¯æŠ½å–ã€é—®ç­”ç³»ç»Ÿç­‰ç­‰ã€‚å¦‚æœæŠŠæ•´ä¸ªå¤§ä»»åŠ¡åšä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œå°±éœ€è¦åšæˆä¸€ä¸ªå¤šä»»åŠ¡æ¨¡å‹ï¼ŒæŠŠ NER ä½œä¸ºå…¶ä¸­ä¸€ä¸ªå­ä»»åŠ¡ï¼›å¦å¤–ï¼Œå•çº¯çš„ NER ä¹Ÿå¯ä»¥åšæˆå¤šä»»åŠ¡ï¼Œæ¯”å¦‚å®ä½“ç±»å‹è¿‡å¤šæ—¶ï¼Œä»…ç”¨ä¸€ä¸ªåºåˆ—æ ‡æ³¨ä»»åŠ¡æ¥åŒæ—¶æŠ½å–å®ä½“ä¸åˆ¤æ–­å®ä½“ç±»å‹ï¼Œä¼šæœ‰äº›åŠ›ä¸ä»å¿ƒï¼Œå°±å¯ä»¥æ‹†æˆä¸¤ä¸ªå­ä»»åŠ¡æ¥åš
3. **æ—¶ä»¤å¤§æ‚çƒ©**ï¼šæŠŠå½“ä¸‹æ¯”è¾ƒæµè¡Œçš„æ·±åº¦å­¦ä¹ è¯é¢˜æˆ–æ–¹æ³•è·Ÿ NER ç»“åˆä¸€ä¸‹ï¼Œæ¯”å¦‚ç»“åˆå¼ºåŒ–å­¦ä¹ çš„ NERã€ç»“åˆ **few-shot learning** çš„ NERã€ç»“åˆå¤šæ¨¡æ€ä¿¡æ¯çš„ NERã€ç»“åˆè·¨è¯­ç§å­¦ä¹ çš„ NER ç­‰ç­‰çš„ï¼Œå…·ä½“å°±ä¸æäº† (Few-shot + Cross-domainæ˜¯ä¸ªä¸é”™çš„é€‰é¡¹ï¼)

ä½œè€…ï¼šç‹å²³ç‹é™¢é•¿
é“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/166496466
æ¥æºï¼šçŸ¥ä¹
è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚
