# Knowledge-enriched Text Generation paper reading

ğŸ˜ Awesome list of papers about knowledge-enhanced Question generation with notes.

:white_check_mark: : **already reading carefully**

:fire:: **high citation in recent years**

:hammer_and_wrench:: **available code**

> Content

[TOC]



---

## :grey_question: Question Generation

---

### :mountain_snow: **Textual Question Generating Crosstalk**

**ä¸€ã€åˆ©ç”¨ç­”æ¡ˆå’Œè¯­è¨€ç‰¹å¾**

1. **ä¸¤ç¯‡Ground Breaking Work**

:white_check_mark: :fire: **Neural question generation from text: A preliminary study**, in EMNLP 2017. [[pdf](https://arxiv.org/abs/1704.01792)] 

* åœ¨ç¼–ç æ—¶é¢å¤–è€ƒè™‘äº†ç­”æ¡ˆä½ç½®ä¸è¯­æ³•ä¿¡æ¯ï¼Œå–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚(ç°åœ¨æ¥çœ‹éå¸¸**basic**é‡è¦çš„ä¿¡æ¯ï¼)
  * word case åšè®­ç»ƒæ—¶å€™çš„teacher forcing
  * answer position feature
  * lexical features
    * **POS**
    * **NER**

```mermaid
graph LR
en((encoder)) --bi-GRU--> fe((feature-Rich)) --> word-vecotr
fe --> lexcial-feature-embedding-vectors --> POS+NER
fe --> answer-position-embedding --> BIO-tagging

word-vecotr --> åŒå‘çš„éšè—å±‚
POS+NER --> åŒå‘çš„éšè—å±‚
BIO-tagging --> åŒå‘çš„éšè—å±‚

de((decoder)) --å¸¦æ³¨æ„åŠ›æœºåˆ¶,ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›--> maxout-hidden+å…·ä½“éœ€è¦çœ‹referenceè®ºæ–‡
de --> GRU

de --> Copy-Mechanism,ä¸€æ ·ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ› --> è®¡ç®—å‡ºæ¦‚ç‡ä»sourceå¥å­ä¸­ç›´æ¥copyå•è¯
```



:white_check_mark: :fire: :hammer_and_wrench: **Learning to Ask: Neural Question Generation for Reading Comprehension**, in ACL 2017. [[pdf]](https://arxiv.org/abs/1705.00106) [[official code (torch)](https://github.com/xinyadu/nqg)]
* å°†ç«¯åˆ°ç«¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œåº”ç”¨äºé—®é¢˜ç”Ÿæˆ
* é‡‡ç”¨seq2seq+attentionæ¨¡å‹æ¶æ„
* æ‘†è„±äº†è½¬æ¢è§„åˆ™ä¸æ¨¡ç‰ˆçš„å±€é™ï¼Œå–å¾—äº†ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•æ›´å¥½çš„æ€§èƒ½
* åŠ å…¥äº†paragraph-level


```mermaid
graph LR
ä»»åŠ¡éš¾ç‚¹ --æ›´åŠ æ¥è¿‘äºäººç±»--> åŒä¹‰è¯æ›¿æ¢+çŸ¥è¯†å¼•å…¥ --> ç›¸å…³å·¥ä½œ --> è¿‡å»:rule-based 
ç›¸å…³å·¥ä½œ --> å…¶ä»–æ•°æ®æ˜ å°„è‡ªç„¶è¯­è¨€

Seq2Seq --> en((encoder)) --bidirectional--> softè®¡ç®—æ³¨æ„åŠ›åˆ†æ•° --> lstm((LSTM))  --> only-sentence
lstm --> sentence+paragraph --> truncateæˆªæ–­,å½“ç„¶æ›´å¥½çš„æ–¹æ³•æ˜¯åˆ‡ç‰‡

Seq2Seq --> de((decoder)) --word-level-prediction--> LSTM((LSTM)) --> éšè—å±‚åˆå§‹åŒ– --basic-model --> å¥å­encoderçš„æœ€åéšè—å±‚
LSTM --oours--> å¥å­+æ®µè½çš„encoderè¾“å‡º


```

2. ç­”æ¡ˆç¼–ç 

:white_check_mark: :fire: **Improving Neural Question Generation using Answer Separation**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1809.02393)] 
* å¾ˆå¤šåŸºç¡€æ“ä½œ
* åœ¨ç­”æ¡ˆä¸Šåšäº†ç®€å•é«˜æ•ˆçš„é¢„å¤„ç†
  * Mask åŸæ–‡ä¸­çš„ç­”æ¡ˆ
  * å¯¹ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯åšæŠ½å–ï¼Œè®¡ç®—attention

3. è¯­è¨€ç‰¹å¾å¼ºåŒ–

> ä¼ ç»Ÿçš„æœ‰**POS**ï¼ˆè¯æ€§æ ‡æ³¨ï¼‰å’Œ**NER**ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰ã€‚åç»­è¿˜æœ‰ä¸€äº›æ›´åŠ ç»†å¾®çš„å¤„ç†

:fire: **Learning to Generate Questions by Learning What not to Generate**, in WWW 2019.  [[pdf](https://arxiv.org/pdf/1902.10418.pdf)] 

* clue å’Œ copyçš„æœºåˆ¶
* ![image-20220703112845472](https://s2.loli.net/2022/07/03/hoa1kTVzIpXGeQK.png)
* ![image-20220703113749291](https://s2.loli.net/2022/07/03/bdToKkIADvemV6B.png)
* æ–‡ç« è´¡çŒ®
  * å¸®åŠ©æ¨¡å‹å†³ç­–ä»€ä¹ˆæ—¶å€™ç”Ÿæˆï¼Œä»€ä¹ˆæ—¶å€™copy
  * ç”Ÿæˆå¤šä¸ªé—®é¢˜

4. ç–‘é—®è¯ç±»å‹ï¼ˆquestion typeï¼‰

:fire: **Question Generation for Question Answering**, in EMNLP 2017.  [[pdf](https://aclanthology.org/D17-1090.pdf)]

**Question-type Driven Question Generation**, in EMNLP 2019.  [[pdf](https://arxiv.org/pdf/1909.00140.pdf)]

* å¼•å…¥å¯¹ç–‘é—®è¯çš„é¢„æµ‹æ¨¡å—ï¼Œå¹¶ä¸”åŠ å…¥å¯¹åº”çš„æŸå¤±å‡½æ•°
* ![image-20220703121312077](https://s2.loli.net/2022/07/03/uhcs1erWpQ9UFKL.png)
* ![image-20220703121503229](https://s2.loli.net/2022/07/03/GTO7j5c1AkXNnqY.png)
* ![image-20220703121520488](https://s2.loli.net/2022/07/03/seIfghRSbvpO68a.png)
* [æŸå¤±å‡½æ•°å¼•æ–‡](https://aclanthology.org/P17-1099.pdf)ï¼š <img src="https://s2.loli.net/2022/07/03/UZv17XkMyLRPOgd.png" alt="image-20220703122045296" style="zoom: 50%;" />

* :hammer_and_wrench:  **Inquisitive Question Generation for High Level Text Comprehension**, in EMNLP 2020. [[pdf](https://aclanthology.org/2020.emnlp-main.530.pdf)]

**äºŒã€æ®µè½çº§åˆ«ç‰¹å¾**

1. å¼ºåŒ–æ®µè½çº§åˆ«æ–‡æœ¬çš„ç‰¹å¾

:fire: :hammer_and_wrench: **Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks**, in EMNLP 2018. [[pdf](https://github.com/seanie12/neural-question-generation)] [[torch](https://github.com/seanie12/neural-question-generation)]

* ä¸»è¦è´¡çŒ®éƒ½åœ¨æ¨¡å‹ä¸Šé¢ï¼ŒåŸºäºseq2seqè®¾è®¡ï¼š![image-20220704150006013](https://s2.loli.net/2022/07/04/trhe5uTRkUqyfsx.png)

  * gate self-attention: ä¸ªäººè§‰å¾—æ˜¯ä¸€å¥—å¾ˆå¸¸ç”¨çš„æ¡†æ¶ï¼Œå¯ä»¥å­¦ä¹ ä¸€ä¸‹ï¼Œä¹Ÿéå¸¸ç®€å•

  * Maxout ==**Pointer**== & Decoding **å…¨æ–°çš„å¤„ç† copy æœºåˆ¶**  (æœ‰ç©ºå¯ä»¥è‡ªè¡Œå»çœ‹çœ‹ä»£ç ï¼)

    * ä¹‹å‰copyå¾—åˆ†ï¼š$\operatorname{sc}^{\text {copy }}\left(y_{t}\right)=\left\{\begin{array}{l}\sum_{k, \text { where } x_{k}=y_{t}} r_{t, k}, \quad y_{t} \in \chi \\ -i n f, \text{otherwise}\end{array} \quad\right.$ , é—®é¢˜åœ¨äºè‹¥æ–‡ç« ä¸­æŸä¸ªå•è¯é‡å¤å‡ºç°å¤šæ¬¡ï¼Œåˆ™å¯¹è¯¥å•è¯copyä¹Ÿä¼šå¤šï¼Œå½±å“è¯­å¥é€šé¡ºã€‚

    * ä¸ºæ­¤æ”¹è¿›ä¸ºMaxout Pointerï¼š
      $$
      \operatorname{sc}^{\text {copy }}\left(y_{t}\right)= \begin{cases}\max _{k, \text { where } x_{k}=y_{t}} r_{t, k}, & y_{t} \in \chi \\ -i n f, & \text { otherwise }\end{cases}
      $$

:fire: **Capturing Greater Context for Question Generation**, in AAAI 2020.  [[pdf](https://arxiv.org/pdf/1910.10274.pdf)]

* é˜¶æ®µçš„æ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹ï¼ˆmulti-stage attention mechanismï¼‰,ä½¿ç”Ÿæˆçš„é—®é¢˜æ›´åŠ å¤æ‚

:fire: **Neural Models for Key Phrase Extraction and Question Generation**,in ACL 2018 Workshop. [[pdf](https://aclanthology.org/W18-2609/)]

* å°†passageå’Œanswerçš„è¡¨ç¤ºï¼ˆåŒ…å«bertå‘é‡ï¼Œgloveå‘é‡ï¼Œè¯æ±‡ç‰¹å¾ç­‰ï¼‰è¿›è¡Œå¤šæ¬¡åå¤çš„äº¤äº’è¿›è¡Œç¼–ç ï¼ˆ**éå¸¸ç»†èŠ‚**çš„deep alignment networkï¼‰
* åˆ©ç”¨GNNæ¥ç¼–ç ï¼ˆä½¿ç”¨äº†ä¸¤ç§æ–¹å¼ï¼‰ï¼š
  * å¯¹sentenceåšdependency parsingï¼Œç„¶åç›¸é‚»çš„å¥å­é“¾æ¥å¾—åˆ°passageçš„å›¾
  * é€šè¿‡self attentionçš„æ–¹å¼å¾—åˆ°passage çš„å›¾ï¼ˆæƒå€¼çŸ©é˜µï¼‰

:fire: **Improving Question Generation With to the Point Context**, in EMNLP 2019.  [[pdf](https://aclanthology.org/D19-1317.pdf)]

* è”åˆå»ºæ¨¡éç»“æ„åŒ–å¥å­ï¼ˆåŸæ–‡ï¼‰å’Œç»“æ„åŒ–ç­”æ¡ˆç›¸å…³å…³ç³»ï¼ˆ answer-relevant relation é¢„å…ˆä»å¥å­ä¸­æå–ï¼‰ä»¥ç”Ÿæˆé—®é¢˜(**æŠ“å–é‡ç‚¹ä¸Šä¸‹æ–‡**)



**ä¸‰ã€å¤šä»»åŠ¡è®­ç»ƒ**

**Multi-Task Learning with Language Modeling for Question Generation**, in EMNLP 2019. [[pdf](http://aclanthology.lst.uni-saarland.de/D19-1337.pdf)]

* æŠŠè¯­è¨€æ¨¡å‹ï¼ˆé¢„æµ‹å‰åè¯ï¼‰å’ŒQGä½œä¸ºmulti-taskä¸€èµ·è¿›è¡Œè®­ç»ƒã€‚
* ä¸¤ä¸ªä»»åŠ¡æ˜¯å±‚çº§çš„å…³ç³»ï¼Œå…ˆè¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„æµ‹ï¼Œç„¶åå°†è¯­è¨€æ¨¡å‹çš„hiddenä½œä¸ºç‰¹å¾æä¾›ç»™åé¢seq2seq

:fire: **Improving Question Generation with Sentence-level Semantic Matching and Answer Position Inferring**, in AAAI 2019.  [[pdf](https://arxiv.org/abs/1912.00879)]

* ä½œè€…è®¤ä¸ºç”Ÿæˆé”™è¯¯è¯çš„åŸå› æ˜¯æ²¡æœ‰æ­£ç¡®çš„åˆ©ç”¨answer positonä¿¡æ¯ï¼Œcopyæ— å…³è¯çš„åŸå› æ˜¯ç¼ºä¹å±€éƒ¨è¯­ä¹‰ä¿¡æ¯ã€‚

* ä¸ºäº†åˆ†åˆ«ç¼“è§£è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œä½œè€…ä¹Ÿæ˜¯è®¾è®¡äº†ä¸¤ä¸ªè¾…åŠ©ä»»åŠ¡ï¼š

  ![img](https://pic3.zhimg.com/80/v2-6e137d01fd833693fcc6cf526a39fb8e_720w.jpg)

****

### :sunrise: Visual QG

:hammer_and_wrench: **[No Visual] Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing**, in AAAI 2021. [[pdf](https://ojs.aaai.org/index.php/AAAI/article/view/17544)] [[torch](https://github.com/VISLANG-Lab/EGSS)]

* Multi-feature Encoder: ä½¿ç”¨äº†POSï¼ˆè¯æ€§æ ‡æ³¨ï¼‰+ NERï¼ˆå…³ç³»æŠ½å–ï¼‰

:hammer_and_wrench: **Multiple Objects-Aware Visual Question Generation**, in ACM MM 2021. [[pdf](https://dl.acm.org/doi/abs/10.1145/3474085.3476969)]
* **å†™ä½œä¸Šå†™å¾—å¾ˆå®åœ¨ï¼Œå¾ˆå®¹æ˜“æ‡‚**ï¼Œæœ‰å¾ˆå¤šæ‰¿ä¸Šå¯ä¸‹çš„å¥å­ã€‚
* é¦–æ¬¡å°†**å¯¹è±¡**èå…¥åˆ°é—®é¢˜ç”Ÿæˆä»»åŠ¡å½“ä¸­

![image-20220629230122528](https://s2.loli.net/2022/06/29/6Mn4HPG9ZiCjOqv.png)

:hammer_and_wrench:  **Difficulty-Controllable Visual Question Generation**, in APWeb-WAIM 2021. [[pdf](https://link.springer.com/content/pdf/10.1007/978-3-030-85896-4_26.pdf)]

* **éš¾åº¦å¯æ§**çš„é—®é¢˜ç”Ÿæˆï¼šé‡‡ç”¨äº†æ•™è‚²å­¦é¢†åŸŸæ”¶é›†å¥½çš„é—®é¢˜éš¾åº¦æ ‡ç­¾(DIF), è¯¦è§[é“¾æ¥](https://www.apims.net/index.php/apims/article/view/9)
* åœ¨VQA2.0æ•°æ®é›†çš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªåŒ…å«åŒºåˆ†ä¸ºå®¹æ˜“å’Œéš¾çš„é—®é¢˜æ•°æ®é›†
  * å¼•å…¥ä¸¤ä¸ªVQAçš„æ¨¡å‹æ¥è¿›è¡Œå›ç­”ï¼Œéƒ½å›ç­”å¯¹çš„ä¸ºå®¹æ˜“ï¼Œéƒ½å›ç­”é”™è¯¯å°±æ˜¯éš¾çš„

* ![image-20220629231350190](https://s2.loli.net/2022/06/29/POftb69si7hnINX.png)
  * å…¶ä¸­Difficulty Variableå°±æ˜¯$\{0, 1\}$




### :video_camera: Video QG

**Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning**, in ICASSP 2020. [[pdf](https://ieeexplore.ieee.org/document/9053476)]

* ä½¿ç”¨äº†**[TVQA](https://paperswithcode.com/dataset/tvqa)**æ•°æ®é›†ï¼Œis based on 6 popular TV shows and consists of **152,545 QA pairs** from **21,793 clips**.
* æ€»ä½“æ²¡ä»€ä¹ˆåˆ›æ–°çš„

**Multi-Turn Video Question Generation via Reinforced Multi-Choice Attention Network**, in T-CSVT 2021.[[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9161024)]

* Multi-Turnï¼ˆM-VQGï¼‰ï¼šç»“åˆå¤šè½®å¯¹è¯+è§†é¢‘ä¿¡æ¯
* ä¼˜ç‚¹ï¼š åˆ©ç”¨åŠ¨æ€åœºæ™¯ä¿¡æ¯ï¼Œé—®é¢˜å¯å›ç­”æ€§ï¼Œå¯¹è¯è®°å½•ä¿¡æ¯æŠ½å–
* æ–¹æ³•ï¼šbaselineæ–¹æ³•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆçœ‹ä¸æ‡‚ï¼‰

**End-to-End Video Question-Answer Generation with Generator-Pretester Network**, in T-CSVT 2021. [[pdf](https://arxiv.org/pdf/2101.01447.pdf)]

* å¼•å…¥ä¸€é—®ä¸€ç­”çš„å½¢å¼ï¼Œç”Ÿæˆé—®é¢˜å’Œç­”æ¡ˆï¼Œç„¶åæµ‹è¯•ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
* ç¡¬ä»¶å¹³å°ï¼šNVIDIA DGX-1ï¼ˆ8 * V100ï¼‰

### :sun_with_face: QG examples

:white_check_mark:  :hammer_and_wrench:  **Mixture Content Selection for Diverse Sequence Generation**, in EMNLP 2019.[[pdf](https://arxiv.org/abs/1909.01953)] [[torch](https://github.com/clovaai/FocusSeq2Seq)]

:hammer_and_wrench: **Radial Graph Convolutional Network for Visual Question Generation**, in IEEE Transactions on Neural Networks and Learning Systems 2020. [[pdf](https://ieeexplore.ieee.org/document/9079208)] [[torch](https://github.com/Wangt-CN/VQG-GCN)]

## :bookmark_tabs: Question Answering

---

:fire:  :hammer_and_wrench: **[Question Answering] Commonsense for Generative Multi-Hop Question Answering Tasks**, in EMNLP 2018. [[pdf\]](https://arxiv.org/abs/1809.06309) [[code (tf)\]](https://github.com/yicheng-w/CommonSenseMultiHopQA)

:hammer_and_wrench: **[Dialogue System] Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering**, in AAAI 2020. [[pdf\]](https://arxiv.org/abs/1912.07491) [[code (torch)\]](https://github.com/siat-nlp/TransDG)

**[Question Answering] Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs**, in EMNLP 2019. [[pdf\]](https://arxiv.org/abs/1910.08435)

:fire: :hammer_and_wrench: **[Question Answering] ** **Improving Multi-hop Question Answering over Knowledge Graphs usingKnowledge Base Embeddings**, in ACL 2020. [[pdf](https://aclanthology.org/2020.acl-main.412/)] [[torch](https://github.com/malllabiisc/EmbedKGQA)]



## :book: Paraphrase

---



:hammer_and_wrench: **[Sentence Discrimination] Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator**,in COLING 2018. [[pdf](https://aclanthology.org/C18-1230/)] [[torch](https://github.com/badripatro/PQG)]

:hammer_and_wrench: **[Hierarchical Sketch&Paraphrase Generation] Hierarchical Sketch Induction for Paraphrase Generation**, in ACL 2022.[[pdf](https://aclanthology.org/2022.acl-long.178.pdf)] [[torch](https://github.com/tomhosking/hrq-vae)]

### :whale2: Related Big Model

:fire: :hammer_and_wrench: **[Cross-Modal&Contrastive Learning] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning**, in ACL(long paper) 2021. [[pdf](https://aclanthology.org/2021.acl-long.202/)] [[project from Baidu](https://unimo-ptm.github.io/)]

:hammer_and_wrench: **[MultiModal] UniT: Multimodal Multitask Learning with a Unified Transformer**, ICCV 2021. [[pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_UniT_Multimodal_Multitask_Learning_With_a_Unified_Transformer_ICCV_2021_paper.pdf)] [[project from Fair](https://mmf.sh/)]

## :framed_picture: Image Caption

---



:white_check_mark: :hammer_and_wrench: **[Image Caption] Generating Diverse and Descriptive Image Captions Using Visual Paraphrases**, in ICCV 2019. [[pdf](https://ieeexplore.ieee.org/document/9010984)] [[torch](https://github.com/pkuliu/visual-paraphrases-captioning)]

* è¯¥è®ºæ–‡ç ”ç©¶äº†ç›®å‰å›¾åƒçš„æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ç¼ºä¹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰å¤è¿°çš„ä¸¤é˜¶æ®µè§£ç çš„æ¨¡å‹ã€‚
  * ç»™å®šå›¾åƒè¾“å…¥ï¼Œè¯¥æ¨¡å‹é¦–å…ˆç”Ÿæˆåˆæ­¥çš„å¥å­ï¼Œå†å°†å…¶æ”¹å†™ä¸ºå†…å®¹æ›´åŠ å¤šæ ·å’Œä¸°å¯Œçš„æè¿°ã€‚åœ¨MS COCOå›¾åƒæè¿°æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ–‡æœ¬æè¿°çš„**å¤šæ ·æ€§**å’Œ**å…·ä½“æ€§**ã€‚

  * é‡ç‚¹æ¢ç´¢**visual paraphrases** è§’è‰² + **scoring function**
  
    * ```mermaid
      graph LR
      ä¸äººç±»ç›¸æ¯” --æ–‡ç« ä¸­æœ‰example--> ç¼ºå°‘å¤šæ ·æ€§å’Œå…·ä½“æ€§ --> ä¸¤é˜¶æ®µè§†è§‰å¤è¿°æ–¹æ³• --> MSCOCOæ•°æ®é›†
      ```
  
  
  * æ•…äº‹å±•å¼€:
  
    * ```mermaid
      graph LR
      æ ‡å‡† -->æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> å½¢å®¹è¯
      æµç•…+ç›¸å…³+å¤šæ ·+å…·ä½“ --å¤šæ ·æ€§--> ç»†èŠ‚,with
      å½¢å®¹è¯ --> Pa((Paraphrase))
      ç»†èŠ‚,with --> Pa
      Pa --> visual-paraphrase
      visual-paraphrase --> sentence_pairs --> ä¸¤é˜¶æ®µç¼–ç 
      ```
  
    * ```mermaid
      graph LR
      ç›¸å…³å·¥ä½œ --caption--> å¤šcaption.vs.å•caption --paraphrases--> æœªå¤„ç†ç‰¹å¾å’Œè§†è§‰ä¿¡æ¯ --ä¸¤é˜¶æ®µç¼–ç --> ä¸­é—´seq.vs.2captions 
      ```
  
  
  * æ¨¡å‹æ–¹æ³•ï¼š
  
    * ```mermaid
      graph LR
      é€‰æ‹©è§†è§‰å¤è¿°captionå¯¹ --> è¯„åˆ†å‡½æ•° --> è®¾è®¡ä¸‰ä¸ªAttentionæ“ä½œ,å­¦ä¹ åˆ°å¤šæ¨¡æ€çŸ¥è¯† --> æœ€åsoftmaxè¾“å‡º
      ```

* æ›´å¤šç»†èŠ‚å¯è§æˆ‘ä¸ªäººçš„[slide](https://kdocs.cn/l/conDzdschwAn)

:white_check_mark: ::fire: :hammer_and_wrench: **[Text Generation & Image Caption] Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions**, in CVPR 2019. [[pdf](https://openaccess.thecvf.com/content_CVPR_2019/html/Cornia_Show_Control_and_Tell_A_Framework_for_Generating_Controllable_and_CVPR_2019_paper.html)] [[torch](https://github.com/aimagelab/show-control-and-tell)]

![](https://s2.loli.net/2022/04/09/COnvomETrl6GRf2.png)

![](https://s2.loli.net/2022/04/09/7ASmXcCazOh9GsU.png)

```mermaid
  graph LR
  å¤–éƒ¨ä¿¡å·æ§åˆ¶ --> å›¾åƒä¸­çš„ä¸€ç»„åŒºåŸŸå— --> core((æ ¸å¿ƒ))
  core --> æ”¹å˜chunkçš„é¡ºåº
  core --> æ”¹å˜å›¾åƒçš„åŒºåŸŸ
  model((æ¨¡å‹)) --åŸºäºåŒºåŸŸçš„ç‰¹å¾ä¸çŠ¶æ€--> LSTM((LanguageModel,ä¸¤å±‚LSTM)) --ç¬¬ä¸€å±‚--> è®¡ç®—attention --æ³¨æ„--> æ‰€æœ‰åŒºåŸŸçš„ç‰¹å¾å‘é‡è¿›è¡Œmean-poolingä½œä¸ºå›¾åƒçš„æ€»ä½“ç‰¹å¾I
  LSTM --ç¬¬äºŒå±‚--> é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯
  model --ä½•æ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå›¾åƒåŒºåŸŸ--> å—è½¬ç§»é—¨ --è®¡ç®—gt--> åŸºäºç¬¬ä¸€å±‚LSTMçš„çŠ¶æ€è®¾ç«‹ä¸€ä¸ªchunk-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sc_t-rtçš„attention
  model --è§†è§‰è¯oræ–‡æœ¬æ¬¡--> AdaptiveAttention --> è®¾ç½®ä¸€ä¸ªvisual-sentinel --> ç±»ä¼¼è®¡ç®—htå¯¹sv_t-rtçš„attention --> attentionçš„ç»“æœ,å¯ä»¥è®¡ç®—å‡ºå½“å‰æ—¶åˆ»æ¨¡å‹æ­£åœ¨å…³æ³¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ct
  model --æ— åºé›†åˆæ’åº--> æ’åºç½‘ç»œ --> Rä¸­åŒ…å«Nä¸ªåŒºåŸŸé›† --å…¨è¿æ¥å±‚--> æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --Sinkhornç®—å­--> è½¯ç½®æ¢çŸ©é˜µ 
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --> æœ€å°åŒ–è½¯ç½®æ¢ä¸çœŸå®ç»“æœä¹‹é—´çš„å‡æ–¹è¯¯å·®
  æ¯ä¸ªåŒºåŸŸé›†çš„ç‰¹å¾æ˜ å°„ä¸ºNç»´å‘é‡,ç„¶åæ‹¼æ¥åœ¨ä¸€èµ· --æµ‹è¯•åŒˆç‰™åˆ©ç®—æ³•è¿›è¡ŒåŒ¹é…--> è½¯ç½®æ¢çŸ©é˜µè½¬åŒ–ä¸ºæœ€ç»ˆçš„ç½®æ¢,ä»¥æ­¤æ¥å¯¹Rè¿›è¡Œæ’åº
```

 [è¯¦ç»†è®²è§£](https://zhuanlan.zhihu.com/p/150667499)

## :sunglasses: Video Understanding

### :video_camera: Features Fusion

:white_check_mark: :fire: :hammer_and_wrench: **[TSN] Temporal Segment Networks: Towards Good Practices for Deep Action Recognition**, in ECCV 2016.  [[pdf](https://arxiv.org/abs/1608.00859)]  [[torch](https://github.com/yjxiong/temporal-segment-networks)]

* æŠ½å–æ‰€æœ‰å¸§æ˜¯ä¸ç°å®çš„ï¼ŒTSNå°†å…¶**ç­‰é—´éš”åˆ†**ä¸º$K$ä¸ªç‰‡æ®µï¼ˆi.e., $K=16$ï¼‰,åœ¨æ¯ä¸ªç‰‡æ®µä¸­è°å¯„æŠ½å–ä¸€å¸§ä½œä¸ºè¾“å…¥

* æä¾›äº†éå¸¸å¸¸ç”¨çš„æ•°æ®äº‰å¼ºæ–¹å¼å’Œä¸€äº›è®­ç»ƒæ—¶å€™çš„trickï¼ˆä¸»è¦åŒ…æ‹¬location jittering, horizontal flipping, corner cropping, and scale jitteringï¼‰

* ä»ç„¶åˆ©ç”¨åŒæµçš„æ€è·¯ï¼Œè®©æ¯ä¸ªç‰‡æ®µä¿¡æ¯æœ€åé€šè¿‡ä¸€ä¸ªå…±è¯†ç½‘ç»œå†Fusion

  ![](https://pic4.zhimg.com/80/v2-67b66b3618606af8b81d1f77b1f92a3b_1440w.jpg)

:white_check_mark: :fire: :hammer_and_wrench: **[TRN] Temporal Relation Reasoning in Videos**, in ECCV 2018.  [[pdf]()] [[torch](https://github.com/zhoubolei/TRN-pytorch)]

![img](https://pic4.zhimg.com/80/v2-86fa6c271c9d2dfad07d4603ed457a83_1440w.jpg)

* èåˆå°ºåº¦ç¡®å®š (éœ€è¦å¤šå°‘ä¸ªè§†é¢‘å¸§æ¥èåˆ)ã€å¦‚å›¾æ‰€ç¤ºã€‘æœ‰2ï¼Œ3ï¼Œ4è¿™ä¸‰ç§å°ºåº¦
* æ¯ä¸ªå°ºåº¦ä¸‹éœ€è¦å¤šå°‘ç»„è§†é¢‘å¸§
* åœ¨åº”ç”¨å¤šå°ºåº¦TRNçš„æ—¶å€™ï¼Œä¸€èˆ¬ä¼šé¢å¤–å¢åŠ ä¸€ä¸ªå…¨å¸§çš„å°ºåº¦ï¼Œå³12å¸§ç‰¹å¾å…¨éƒ¨concatåˆ°ä¸€èµ·ï¼Œä»¥å……åˆ†åˆ©ç”¨æœ‰æ•ˆä¿¡æ¯ã€‚
* **å¹³è¡¡**æ•ˆæœå’Œè®¡ç®—é€Ÿåº¦ï¼Œ**ç®€å•å¥½ç”¨**

:white_check_mark: :fire: :hammer_and_wrench: **[TSM] TSM: Temporal Shift Module for Efficient Video Understanding**, in ICCV 2019.  [[pdf](https://arxiv.org/abs/1811.08383)] [[torch](https://github.com/mit-han-lab/temporal-shift-module)]

* å¯¹æŸäº›é€šé“shiftï¼Œå¾—åˆ°å‰ä¸€å¸§æˆ–è€…åä¸€å¸§çš„ç‰¹å¾

  ![image-20220709174802714](https://s2.loli.net/2022/07/09/cb1f8LWpV9JotO3.png)

* ç”±äºshiftæ˜¯æœ‰æŸå¤±çš„ï¼Œä¸ºæ­¤è®¾è®¡æ®‹å·®æ¥è¿›è¡Œå¼¥è¡¥ï¼ˆåŸæ¥çš„ä¸æ®‹å·®çš„å¯¹æ¯”ï¼‰

![image-20220709174842935](https://s2.loli.net/2022/07/09/2dTjLBJwQ5FUber.png)

:white_check_mark: :fire: :hammer_and_wrench: **[LRCN] Long-term Recurrent Convolutional Networks for Visual Recognition and Description**, in CVPR 2015.  [[pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf)] [[torch](https://github.com/garythung/torch-lrcn)]

* CNNæŠ½å‡ºæ¥çš„å¸§ç‰¹å¾å†æ”¾è¿›å»`LSTM`å¾—åˆ°æ¯å¸§çš„æ—¶åºç‰¹å¾

> å…³äºè§†é¢‘ç‰¹å¾æŠ½å–ï¼Œä¸‹é¢è®²ä¸€ä¸‹`netvlad`ç³»åˆ—çš„ç»“æ„,NextVladå°±æ˜¯ä¸“é—¨é’ˆå¯¹è§†é¢‘å¸§èåˆæ¥åšçš„ä¼˜åŒ–ã€‚
>
> [ç›¸å…³åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/385512915)

:fire: :hammer_and_wrench: **[NetVLAD] NetVLAD: CNN architecture for weakly supervised place recognition**, in CVPR 2016.  [[pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)] [[torch (simple)](https://github.com/lyakaap/NetVLAD-pytorch)]

* VLADç®—æ³•ï¼ˆå®é™…ä¸Šå°±æ˜¯Kmeansï¼‰ï¼š
  $$
  V(j, k)=\sum_{i=1}^{N} a_{k}\left(x_{i}\right)\left(x_{i}(j)-c_{k}(j)\right), \quad k \in K, j \in D
  $$

* æœ¬æ–‡ä½¿ç”¨`CNN`æ¨¡æ‹Ÿè¯¥VLADç®—æ³•çš„è¿‡ç¨‹

  * å¹³æ»‘åŒ–$\alpha$ ä½¿å…¶å˜æˆä¸€ä¸ª0-1åˆ†å¸ƒçš„æƒé‡å‚æ•°ï¼Œä½¿ç”¨$1 \times 1$å·ç§¯+softmax è¿›è¡Œè¯¥è¿‡ç¨‹ï¼Œå¹³æ»‘æ¨å¯¼å…¬å¼$\bar{a}_{k}\left(\mathbf{x}_{i}\right)=\frac{e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k}\right\|^{2}}}{\sum_{k^{\prime}} e^{-\alpha\left\|\mathbf{x}_{i}-\mathbf{c}_{k^{\prime}}\right\|^{2}}}$

  

  ![image-20220712104829439](https://s2.loli.net/2022/07/12/aFlJSCw8dvBnzEH.png)

   

:fire: :hammer_and_wrench: **[NextVLAD] NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification**, in ECCV workshop 2018.  [[pdf](https://arxiv.org/pdf/1811.05014.pdf)] [[tensorflow](https://github.com/linrongc/youtube-8m)]

> åŒæ—¶ï¼Œè¿˜æœ‰[å…³äºå¤šæ¨¡æ€ï¼ˆè§†é¢‘-æ–‡æœ¬ï¼‰Transformeræ¨¡å‹çš„åšå®¢é“¾æ¥](https://zhuanlan.zhihu.com/p/388361095)

### :writing_hand: Video Caption

**[Video Caption] VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs**, in CVPR 2021. [[pdf](https://arxiv.org/abs/2101.12059)]

:hammer_and_wrench: :fire: **[Video Caption] Robust Change Captioning**, in ICCV 2019. [[pdf](https://arxiv.org/pdf/1901.02527.pdf)] [[torch](https://github.com/Seth-Park/RobustChangeCaptioning)]

* è¾“å…¥ä¸ºå‰åå›¾åƒå¯¹ï¼Œäº”ç§å˜åŒ–ç±»å‹ï¼ˆcolor/material change,adding/dropping/moving an objectï¼‰
* æå‡ºä¸€ä¸ªæœ‰è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[CLEVR-Change](https://cs.stanford.edu/people/jcjohns/clevr/)ï¼ˆ80Kå›¾ç‰‡å¯¹ï¼‰ï¼Œå¹¶åœ¨æ— è§†ç‚¹å˜åŒ–çš„æ•°æ®é›†[Spot-the-Diff](https://github.com/harsh19/spot-the-diff)å–å¾—SOTAæ•ˆæœã€‚
* æ¨¡å‹ï¼šDual æ³¨æ„åŠ›ï¼Œ åˆ†è¾¨**è§†ç‚¹å˜åŒ–**ï¼Œ å…¶å®æ˜¯é€šè¿‡è¾“å…¥ä¸¤å¼ å·®ä¸å¤šçš„å›¾ç‰‡ï¼Œæå‰æ ‡å®šå¥½æ•°æ®é›†è·å¾—çš„ï¼Œæœ‰ç‚¹è¢«å‘çš„æ„æ€![image-20220522213419579](https://s2.loli.net/2022/05/22/fiUArgZIjlzw4p1.png)

:hammer_and_wrench: :fire: **[Video Caption] Semantic Grouping Network for Video Captioning**, in AAAI 2021. [[pdf](https://arxiv.org/pdf/2102.00831.pdf)] [[torch](https://github.com/hobincar/SGN)]

* ![image-20220621204108736](https://s2.loli.net/2022/06/21/DMmzxs7dKwyU6BE.png)



```mermaid
  graph LR
  SG(Semantic-Grouping) --å»æ‰å†—ä½™phrase--> ç›¸ä¼¼åº¦è®¡ç®—
  SG --attentionæœºåˆ¶ --> å¯¹å…¶phraseå’Œframe --> åŠ å…¥å¯¹æ¯”æŸå¤±,è®¡ç®—æ²¡æœ‰åŒ…å«negativeçš„æ¦‚ç‡
```

**å¯¹æ¯”æŸå¤±**$\mathcal{L}_{c a}=\sum_{(V, Y) \in \mathcal{D}} \sum_{t} \sum_{i}^{M_{t}}\left(-\log p_{c a}\left(s_{i, t}\right)\right)$, $p_{c a}\left(s_{i, t}\right)=\sum_{j=1}^{N} \alpha_{i, j, t}^{p o s}$    ($\alpha^{pos}$ ä¸ºæ­£æ ·æœ¬æ—¶å€™å¯¹é½æ³¨æ„åŠ›çš„æƒé‡) 







## :abc: Scene Text Recognization

:hammer_and_wrench: **From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network**, in ICCV 2021. [[pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2108.09661)] [[torch](https://link.zhihu.com/?target=https%3A//github.com/wangyuxin87/VisionLAN)]

* è¿‡å»çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«éœ€è¦ï¼šè§†è§‰ç‰¹å¾æŠ½å–å™¨ + è¯­è¨€æ¨¡å‹

* æœ¬æ–‡ç›´æ¥åœ¨è§†è§‰ç©ºé—´è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼ˆç±»ä¼¼äººç±»ï¼Œè¯­è¨€ä¿¡æ¯æ˜¯å¯ä»¥å­¦ä¹ çš„ï¼‰
  * å¯¹å­—ç¬¦çº§åˆ«çš„Maskæ“ä½œ![image-20220701212346925](https://s2.loli.net/2022/07/01/ZLFUIkb41S782GD.png)
    * è®­ç»ƒè¿‡ç¨‹ï¼Œé‡‡ç”¨å¼±ç›‘ç£äº’è¡¥å­¦ä¹ ![image-20220701212430601](https://s2.loli.net/2022/07/01/kc3K7XxAN6SfRut.png)

:hammer_and_wrench: **Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition**, in AAAI 2022.  [[pdf]([https://arxiv.org/abs/2112.12916](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2112.12916))] [[torch]([https://github.com/adeline-cs/GTR](https://link.zhihu.com/?target=https%3A//github.com/adeline-cs/GTR))]
* åŠ å…¥ä¸€ä¸ªGCNå¼ºåŒ–äº†è§†è§‰å­¦ä¹ çš„è¿‡ç¨‹ï¼Œå¹¶ä¸”åšäº†ä¸€ä¸ªfusion







